

석 사 학 위 논 문


수학 학습에서 질문 명료화를 지원하는 AI에이전트 설계 및 개발:
 고등학교 2학년 수학적 귀납법 단원 중심으로

김규봉

부산대학교 교육대학원

AI융합교육전공

2026년 2월




수학 학습에서 질문 명료화를 지원하는 AI에이전트 설계 및 개발:
 고등학교 2학년 수학적 귀납법 단원 중심으로

김규봉

2026년 2월





수학 학습에서 질문 명료화를 지원하는 AI에이전트 설계 및 개발:
 고등학교 2학년 수학적 귀납법 단원 중심으로


이 논문을 교육학석사 학위논문으로 제출함

김규봉

부산대학교 교육대학원

AI융합교육전공

지 도 교 수   박성호

김규봉의 교육학석사 학위논문을 인준함

2025년  12월  27일

위원장
위  원
위  원
남윤경
송길태
박성호
인
인
인


차 례
I. 서론	1
  1. 연구의 필요성	1
  2. 연구 목적	2
  3. 연구 문제	4
  4. 용어의 정의	4

II. 이론적 배경	6
  1. 이론적 기반의 구조	6
  2. 블룸의 지식 분류: 4가지 지식 차원	8
  3. 듀이의 반성적 사고 이론: 명료화 프로세스의 철학적 기반	9
  4. 질문 생성 및 개선 이론: 질문 품질의 구조화	12
  5. AI 기반 피드백 시스템: 실시간 상호작용의 설계 원리	17
  6. 멀티 에이전트 시스템: 역할 분담과 협업	19
  7. 수학적 귀납법 단원 선정: 단원 맥락 반영 필요성	23
  8. 평가 루브릭의 이론적 기반	25

III. 연구 방법 및 설계 기반 개선 과정	31
  1. 연구 방법론: Design-Based Research	31
  2. 사이클 2: 베타테스트 및 반복적 개선 (2025년 9월)	33
  3. 사이클 3: 본 연구 A/B 테스트 (2025년 10-11월)	34
  4. 통제 변인	47
  5. 연구 도구	48
  6. 자료 수집 및 분석	50

IV. MAICE 교육 시스템 아키텍처	52
  1. 예비 연구: 문제 분석 및 설계 근거	52
  2. 설계 철학: "명료화 중심 학습"	55
  3. 전체 아키텍처 개요: 3계층 구조	56
  4. 5개 에이전트의 역할과 협업	60
V. 연구 결과	67
  1. 연구 실행 및 데이터 수집	67
  2. 명료화 효과: LLM-교사 이중 평가	68
  3. 학습자 자기 평가 및 증거의 수렴	77
  4. 피드백 내용의 질적 분석: Bloom-Dewey 이론 실증	80

VI. 논의 및 결론	93
  1. 명료화 프로세스의 작동 메커니즘	93
  2. 교육적 시사점	97
  3. 연구의 제한점	102
  4. 후속 연구 제언	106
  5. 결론	108


부록 차례

I. 부록 A: 기술 구현 상세	111
II. 부록 B: 에이전트 프롬프트 설계 및 구현 예시	118
III. 부록 C: 학생 사후설문지	130
IV. 부록 D: LLM 배치 채점 프롬프트 및 평가 방법	135

표 차례

그림 차례


수학 학습에서 질문 명료화를 지원하는 AI에이전트 설계 및 개발
: 고등학교 2학년 수학적 귀납법 단원 중심으로

김규봉

부산대학교 교육대학원 AI융합교육전공

요약


생성형 AI의 교육 현장 확산에도 불구하고 학생 질문의 낮은 품질이 효과적 학습을 저해하고 있다. 예비조사(n=385)에서 학생 질문의 72.3%가 학습 맥락을 제공하지 않았으며, 즉시 답변 방식은 학생의 사고 과정 확장을 지원하지 못하는 한계를 보였다.

본 연구는 Dewey의 반성적 사고 이론과 Bloom의 지식 분류를 기반으로 질문 명료화를 핵심으로 하는 AI 멀티 에이전트 시스템 MAICE를 설계·개발하였다. 고등학교 2학년 58명을 명료화 모드(n=28)와 즉시 답변 모드(n=30)에 무작위 배정하여 3주간 A/B 테스트를 수행하였다(284개 세션). 평가의 객관성과 타당성 확보를 위해 LLM-교사 이중 평가를 병행하였으며, 3개 독립 AI 모델(Gemini 2.5 Flash, Claude 4.5 Haiku, GPT-5-mini)이 전체 세션을 평가하고 외부 수학 교사 2명이 100개 표본의 교육적 타당성을 검증하였다. 평가 신뢰도는 LLM ICC=0.848(우수), 교사 ICC=0.707(양호), LLM-교사 상관 r=0.754* (p<0.001, 강한 일치)로 확보되었다.

주요 결과: (1) LLM 평가: 명료화 모드는 학습 지원(C2: p=0.004, d=0.353)과 학습 확장성(B3: p=0.041, d=0.245)에서 유의하게 우수하였으며, 특히 하위권 학생에게 큰 효과를 보였다(Q1 C2: d=0.855; Q1 전체: d=0.499). (2) 교사 평가: 하위권에서 매우 큰 효과를 보였으며(d=0.993), LLM-교사 강한 상관(r=0.754*)으로 평가 타당성을 확인하였다. (3) 질적 분석: 명료화 세션에서 Bloom 고차원 사고와 Dewey 반성적 사고 구현도가 높았으며, 하위권 학생의 세션에서 Bloom 4단계 상승과 Dewey 5/5 완전 구현 사례가 발견되었다. (4) 학생 평가**: 68.6%가 명료화 방식 선호("사고력 향상" 42%)하여 객관적 평가와 수렴하였다.
결론: 질문 명료화 프로세스는 학습 지원을 향상시키며 특히 하위권 학생에게 효과적이다. 네 가지 독립 증거(LLM, 교사, 학생, 질적)의 수렴으로 타당성을 확보하였다. 본 연구는 (1) Dewey의 문제 명료화 단계를 A/B 테스트로 검증하여 효과적 방법임을 입증하였고, (2) 하위권 학생 지원의 실천적 효과를 제시하였으며, (3) LLM-교사 이중 평가 모델(ICC=0.848, 0.707; r=0.754***)로 대규모 객관 평가와 전문가 타당성을 결합한 확장 가능한 연구 플랫폼을 제시하였다. 단, 참여자가 AI 사용 경험이 풍부한 소프트웨어 특화 고교 학생으로 일반화에 신중함이 필요하다.

주요어: 질문 명료화, AI 에이전트, 반성적 사고, 수학적 귀납법, 멀티에이전트 시스템, Dewey, 교육 격차 해소, 교사 주도 연구, 프롬프팅 설계
I. 서론
1. 연구의 필요성

가. 수학 교육에서 생성형 AI 활용의 확산과 한계
대규모 언어 모델(Large Language Model, LLM)의 비약적 발전에 따라 ChatGPT, Claude 등 생성형 AI 도구가 교육 현장에 빠르게 도입되고 있다. 특히 수학 교육 영역에서는 학생들이 개념 이해, 문제 풀이, 추가 학습 등의 목적으로 LLM을 활용하는 빈도가 급증하고 있으며, 일부 교사들은 이를 보조 학습 도구로 활용하려는 시도를 하고 있다.
그러나 현재 대부분의 LLM 기반 AI 도구는 학생의 질문에 대해 질문의 질이나 맥락과 무관하게 즉각적으로 답변을 제공한다. 이러한 즉시 답변 방식은 신속한 정보 제공이라는 장점이 있으나, 학습자가 스스로 질문을 구조화하고 사고를 확장하는 과정을 지원하지 못하여 깊이 있는 학습을 저해할 가능성이 제기되고 있다.

나. 예비 연구를 통한 문제 확인
본 연구는 에이전트 시스템 설계에 앞서, 2025년 5월 실제 고등학교 수학 수업에서 385건의 학생-AI 대화를 수집하여 현재 LLM 사용 방식의 교육적 문제점을 실증적으로 분석하였다 (상세 내용은 IV장 1절 참조).

핵심 발견:
◯ 학습 맥락 부재: 72.3%의 질문이 학습자 수준·목적 미제시
◯ 질문 구조 불명확: 45.8%가 질문 대상·범위 불분명
◯ 질문-답변 상관: 질문 품질 ↑ = AI 답변 품질 ↑ (r=0.691, p<0.001)

이러한 실증 분석은 질문 개선이 학습 효과 향상의 핵심 메커니즘임을 시사하였으며, 본 연구의 "질문 명료화" 접근법의 이론적 근거가 되었다.

2. 연구 목적
예비 조사에서 확인된 핵심 발견은 질문 품질과 답변 품질 간 강한 상관관계(r=0.691)로, 이는 질문 개선이 학습 효과 향상의 핵심 메커니즘임을 의미한다. 따라서 본 연구는 고등학교 2학년 수학적 귀납법 단원을 중심으로 질문 명료화와 지식 유형별 맞춤 답변을 통합한 멀티 에이전트 AI 학습 시스템을 설계·개발한다.

본 연구의 AI 에이전트 시스템은 다음 3가지 핵심 기능을 통합한다: 
● ① 질문 명료화(QuestionImprover: Dewey 5단계 기반 사고 구조화 지원)
● ② 지식 유형별 맞춤 답변(QuestionClassifier + AnswerGenerator: Bloom K1-K4 분류 및 차별화된 답변 전략)
● ③ 대화 맥락 관리(LearningObserver: 학습 흐름 유지 및 누적 지원).

시스템의 교육적 효과를 검증하기 위해, 본 연구는 무작위 배정 A/B 테스트를 설계하였다. 학생들을 두 그룹으로 나누어 한 그룹은 통합 학습 지원 기능을 제공받는 방식(본 연구에서 "Agent 모드"로 명명)을 사용하고, 다른 그룹은 일반적인 LLM처럼 즉시 답변만 제공하는 방식(본 연구에서 "Freepass 모드"로 명명)을 사용하도록 하여, 명료화 중심 멀티 에이전트 설계가 학습 지원에 미치는 효과를 비교 분석한다.

평가의 객관성과 신뢰성을 확보하기 위해, 본 연구는 LLM-교사 이중 평가 모델을 개발하였다. 3개 독립 AI 모델(Gemini 2.5 Flash, Claude 4.5 Haiku, GPT-5-mini)이 284개 전체 세션을 평가하여 대규모 패턴을 탐색하고, 외부 수학교사 2명이 100개 샘플을 독립 평가하여 교육적 타당성을 검증하였다. LLM 평가 신뢰도(ICC=0.848), 교사 평가 신뢰도(ICC=0.707), 두 평가 간 강한 상관(r=0.754)과 Bloom/Dewey 이론 정합성, 학생 자기 평가 수렴을 통해 삼각 검증을 완료하였다 (상세 내용은 V장, VI장 참조).

가. 학술적 연구 목적
◯ 질문 명료화 기반 멀티 에이전트 시스템 개발: 예비 조사에서 확인된 질문의 질적 문제(학습맥락 부재 72.3%, 질문구조 불명확 45.8%, 수학적전문성 결여 45.5%)를 개선하는 질문 명료화 프로세스를 핵심으로 하는 멀티 에이전트 시스템 개발
◯ 질문 품질 개선 효과 검증: 본 연구의 질문 명료화 프로세스가 학생의 질문 품질(학습맥락, 질문구조, 수학적전문성)을 일반적인 즉시 답변 방식 대비 실질적으로 개선함을 입증

◯ 학습 경험 향상 증명: 질문 명료화 프로세스를 통한 구조화된 학습 경험이 즉시 답변 방식보다 학습 만족도와 메타인지를 향상시킴을 입증

◯ 수학적 귀납법 단원 적용 검증: 본 연구의 AI 에이전트 시스템이 수학적 귀납법 단원(논리 구조 이해, 단계별 증명 필요)에서 효과적으로 작동하는지 검증

나. 교육적 실천 목적

본 연구는 학술적 검증과 함께, 교육 현장에서 활용 가능한 실용적 기여를 다음과 같이 목표한다:

◯ 메타인지 발달 지원 검증: 베타테스트(n=11)에서 학생들은 명료화 과정을 통해 "모호한 질문을 구체화하는 방법"(4.0/5점), "무엇을 모르는지 스스로 규정"(4.1/5점) 능력이 향상됨을 보고하였다. 본 실험에서는 더 큰 표본(N=280)을 통해 메타인지 발달 효과를 정량적으로 검증한다.

◯ 교사 맞춤형 프롬프팅 연구 환경: 본 연구의 에이전트 시스템은 5개 에이전트의 프롬프트를 모두 공개하고 수정 가능하도록 설계하였다. 교사는 자신의 교육 목표(예: 수학적 귀납법의 특정 단계 강조)에 맞춰 명료화 질문 템플릿, K1-K4 분류 기준, 답변 전략을 직접 수정하고 그 효과를 측정할 수 있다.

◯ 확장 가능한 연구 플랫폼 제공: 본 연구의 QAC 체크리스트, LLM-교사 이중 평가 모델, A/B 테스트 설계는 다른 교과·단원에 적용 가능하다. 시스템 코드와 평가 도구를 공개하여 후속 연구자와 교사가 자신의 맥락에서 재현·확장할 수 있도록 한다.
3. 연구 문제

예비 조사를 통해 확인된 일반적인 즉시 답변 제공 방식의 핵심 문제는 학생들의 질문 품질 부족(학습맥락 72.3% 최저점)이며, 질문 품질과 답변 품질 간 강한 상관관계(r=0.691)는 질문 개선이 학습 효과 향상의 핵심 메커니즘임을 시사한다.

앞서 I.2절에서 설명한 바와 같이, 본 연구는 에이전트 시스템의 두 가지 작동 방식인 질문 명료화를 제공하는 Agent 모드와 일반적인 LLM처럼 즉시 답변만 제공하는 Freepass 모드를 무작위 배정 A/B 테스트로 비교한다. 시스템의 효과성을 검증하기 위한 연구 문제를 다음과 같이 설정한다:

RQ1. 질문 품질 개선 검증 (핵심 메커니즘): 본 연구의 질문 명료화 프로세스(Agent 모드)가 학생의 질문 품질을 실질적으로 개선하는가? 특히 예비 조사에서 가장 심각한 문제로 확인된 학습 맥락 적용, 질문 구조화, 수학적 전문성 영역에서 일반적인 즉시 답변 방식(Freepass 모드) 대비 유의미한 개선이 나타나는가?

RQ2. 학습 경험 향상 검증 (교육적 가치): 질문 명료화 프로세스를 통한 구조화된 학습 경험(Agent 모드)이 즉시 답변 방식(Freepass 모드)보다 학생의 학습 만족도와 메타인지를 향상시키는가?



4. 용어의 정의

◯ 예비 조사(Pilot Study): 본 연구의 AI 에이전트 시스템 설계에 앞서, 2025년 5월 중순 실제 고등학교 수학 수업 환경에서 프리패스 방식 LLM의 교육적 문제점을 실증적으로 파악하기 위해 실시한 질문-답변 수집 및 교사 루브릭 평가 연구 (385건 질문, 1,012건 교사 평가)

◯ 에이전트 시스템 (Mathematical AI Chatbot for Education, MAICE): 예비 조사에서 확인된 일반적인 LLM 사용 방식의 한계를 극복하기 위해 개발된 질문 명료화 기반 AI 멀티 에이전트 시스템. 본 연구에서는 이 시스템을 두 가지 모드로 운영하여 비교 실험을 수행한다. 시스템의 상세 설계 및 구현은 IV장에서 다룬다.

◯ Agent 모드: 에이전트 시스템의 통합 학습 지원 기능을 제공하는 실험군 조건. 5개 에이전트가 협업하여 ① 질문을 K1-K4로 분류(QuestionClassifier)하고, ② 필요시 명료화 질문으로 사고를 구조화(QuestionImprover)하며, ③ 지식 유형별로 차별화된 답변을 제공(AnswerGenerator)하고, ④ 대화 맥락을 관리(LearningObserver)한다.

◯ Freepass 모드: 에이전트 시스템의 대조군 조건으로, 일반적인 LLM(ChatGPT, Claude 등)처럼 질문 명료화 과정 없이 학생의 질문에 즉시 답변만 제공하는 방식. "Freepass"는 "명료화 과정을 거치지 않고(free) 바로 통과(pass)하여 답변"이라는 의미로 본 연구에서 명명하였다.

◯ 질문 명료화: 모호하거나 불완전한 질문을 구체적이고 명확한 질문으로 개선하는 과정으로, 학습자가 스스로 질문을 구조화하고 사고 과정을 명료화하도록 지원하는 교육적 개입.

◯ AI 에이전트(AI Agent): 특정 목표를 달성하기 위해 환경을 인식하고, 자율적으로 의사결정을 내려 행동하는 소프트웨어 시스템. 본 연구에서는 LLM을 기반으로 특정 교육적 역할(질문 분류, 명료화 지원, 답변 생성 등)을 수행하도록 설계된 독립적 구성 요소를 의미함.

◯ 멀티 에이전트 시스템: 질문 분류, 명료화 지원, 답변 생성, 학습 관찰 등 각기 다른 역할을 수행하는 여러 개의 독립적인 AI agent가 협업하여 학습 과정을 지원하는 시스템

◯ 학습 효과: 수학적 이해도, 문제해결 능력, 질문 품질 개선, 메타인지 향상 등을 포함하는 학습 성과와 학습 경험의 종합적 측정
II. 이론적 배경

I장에서 확인된 프리패스 LLM의 문제점(학습 맥락 부재 72.3%, 질문-답변 상관 r=0.691)을 해결하기 위해, 본 장에서는 질문 명료화 기반 AI 멀티 에이전트 시스템의 이론적 기반을 제시한다.

본 연구는 다음 5개의 독립적 에이전트로 구성된 AI 멀티 에이전트 시스템을 개발하였다(시스템의 상세 명칭 및 구현은 IV장에서 다룬다):


약자
에이전트 명칭
핵심 역할
이론적 기반
QC
Question Classifier
질문 품질 진단 및 K1-K4 분류
Bloom 분류학 (2.1절)
QI
Question Improver
명료화 질문으로 문제 구체화
Dewey 반성적 사고 (2.2절)
AG
Answer Generator
K1-K4 유형별 맞춤 답변 생성
Bloom + AI 피드백 (2.4절)
LO
Learning Observer
대화 요약 및 컨텍스트 관리
세션 연속성 유지 (3.3.4절)
FT
Free Talker
대조군: 명료화 없는 즉시 답변
(비교 기준선)
[표Ⅱ-1] AI 멀티 에이전트 시스템 구성


1. 이론적 기반의 구조

본 연구 시스템의 핵심 설계 철학은 Dewey의 반성적 사고 5단계 중 2단계(문제 정의)를 명료화 프로세스로 구현하는 것이다. 다음 다이어그램은 프리패스 방식의 문제점과 본 연구의 해결 방안을 비교한다:







[그림Ⅱ-1] 프리패스 방식과 명료화 방식의 비교


핵심 통찰: 1장의 예비 조사에서 발견된 질문 품질과 답변 품질의 강한 상관관계(r=0.691)는 Dewey의 2단계(문제 정의)가 학습 효과의 핵심 메커니즘임을 실증적으로 뒷받침한다.

이론 영역
이론 (2장)
핵심 개념
1장 문제점 해결
AI 에이전트 시스템 구현
교육학적 토대
2.1 Bloom 지식 분류
K1→K4 4단계 분류
질문구조 불명확 45.8%
QC 질문 분류 → AG K1-K4 맞춤 답변
2.2 Dewey 반성적 사고
2단계 문제 정의
학습맥락 부재 72.3%
QI 명료화 프로세스 (핵심!)
2.3 질문 생성 이론
구조·완결·의도
수학적전문성 45.5%
QC 품질 진단 → QI 개선 유도
기술적 구현
2.4 AI 피드백 시스템
즉시성·맞춤성
일방향 답변
실시간 대화형 피드백 (4장)
2.5 멀티에이전트
역할 분담·협업
단일 AI의 한계
QC/QI/AG/LO/FT 5개 협업 (3.3)
2.6 수학적 귀납법
단원 특성 반영
일반적 설명
단원 맥락 적응형 명료화 (5장)
평가 방법론
2.7 루브릭 개발
질문-답변 평가
평가 기준 부재
QAC 체크리스트 (6.4)
[표Ⅱ-2] 이론적 기반과 시스템 구현의 연결

표Ⅱ-2와 그림Ⅱ-1에서 보듯이, 각 이론은 1장에서 발견된 문제점을 해결하기 위한 이론적 해결책을 제시한다. 이어지는 절에서는 각 이론의 핵심 개념과 본 연구에의 적용을 상세히 검토한다.
2. 블룸의 지식 분류: 4가지 지식 차원
블룸의 개정 분류(Anderson & Krathwohl, 2001; Krathwohl, 2002)에서 지식 차원(Knowledge Dimension)은 다음 네 범주로 구성된다. 본 연구에서는 질문 명료화의 목표를 학습자의 지식 차원에 정렬하여, 질문 품질과 학습 효과를 동시에 개선한다.


본 연구
표기
지식 차원
원문 표기
정의
수학적 귀납법 맥락의 예시 질문
K1
사실적 지식
Factual Knowledge
용어, 기본 사실, 기호·규칙의 기억
"귀납법의 기본 단계와 귀납 단계의 정의는 무엇인가요?"
K2
개념적 지식
Conceptual Knowledge
관계, 분류, 원리·법칙의 이해
"귀납 가정이 증명 논리에서 맡는 역할을 설명해 주세요."
K3
절차적 지식
Procedural Knowledge
방법·알고리즘·기법의 적용 절차
"부등식 증명에서 k→k+1 전개는 어떤 순서로 진행하나요?"
K4
메타인지적 지식
Metacognitive Knowledge
자기 인지·전략 선택·오류 진단
"내 전개에서 누락한 가정은 무엇이며, 어떤 전략을 선택해야 하나요?"
[표Ⅱ-3] Bloom의 지식 차원 분류 (K1~K4)

본 연구에서는 시스템 구현과 서술의 편의를 위해 4가지 지식 차원을 다음과 같이 약어로 표기한다: K1(사실적 지식), K2(개념적 지식), K3(절차적 지식), K4(메타인지적 지식). 이는 Anderson & Krathwohl (2001) 원문의 표기법은 아니며, 본 연구가 에이전트 명명 및 분류 체계를 위해 도입한 것이다. 이후 본문에서는 주로 K1-K4 표기를 사용한다.

이러한 지식 차원 분류는  AI 에이전트 시스템의 명료화 프로세스와 다음과 같이 정렬된다. 사실적·개념적 지식(K1-K2)의 경우 용어 혼동이나 개념적 오해를 명료화 질문으로 선제 교정하며, 절차적 지식(K3)은 절차적 막힘을 단계화된 프롬프트로 유도한다. 메타인지적 지식(K4)은 학습자가 스스로의 오류와 전략을 메타인지적으로 진단하도록 반문을 통해 유도한다.

본 연구의 QC(Question Classifier)는 질문 분류 단계에서 학습자의 발화를 이 4가지 지식 차원으로 분류하고, AG(Answer Generator)가 각 차원에 정렬된 맞춤 답변을 제공한다. 이를 통해 질문의 질적 향상과 문제 해결 과정을 지원한다.
3. 듀이의 반성적 사고 이론: 명료화 프로세스의 철학적 기반
가. 반성적 사고의 정의와 5단계
듀이(Dewey, 1910)는 반성적 사고(reflective thinking)를 "어떤 믿음이나 지식의 형태에 대한 능동적이고 지속적이며 신중한 고려로서, 그것을 뒷받침하는 근거와 그것이 이끄는 결론들을 검토하는 것"으로 정의하였다. 원문에서 Dewey는 "Active, persistent, and careful consideration of any belief or supposed form of knowledge in the light of the grounds that support it, and the further conclusions to which it tends"라고 표현하였다(p.6). 이는 단순한 정보 습득이나 수동적 수용을 넘어, 학습자가 능동적으로 믿음의 근거를 탐구하고 그 함의를 추론하는 과정을 강조한다.

Dewey(1910, p.13)는 "사고의 기원은 어떤 당혹감, 혼란, 또는 의심(the origin of thinking is some perplexity, confusion, or doubt)"이라고 강조하며, 반성적 사고가 단순한 제안의 수용이 아닌 추가 증거를 탐색하는 능동적 과정임을 명확히 하였다.

듀이가 제시한 반성적 사고의 5단계는 다음과 같다(Dewey, 1910, p.72):

단계
Dewey 원문 표현
본 연구의 한국어 명칭
수학적 귀납법 학습 예시
1단계
a felt difficulty
문제 상황 인식
"귀납법 문제를 풀려는데 어디서부터 시작해야 할지 모르겠다"
2단계
its location and definition
문제의 위치 파악 및 정의
"귀납 단계에서 n=k+1을 증명할 때 귀납 가정을 어떻게 사용하는지 모르겠다"
3단계
suggestion of possible solution
가능한 해결책의 제안
"귀납 가정 P(k)를 식에 대입해보면 될까?"
4단계
development by reasoning of the bearings of the suggestion
제안된 해결책의 추론적 전개
P(k)를 실제로 대입하여 P(k+1) 유도 시도
5단계
further observation and experiment leading to its acceptance or rejection
관찰과 실험을 통한 수용 또는 거부
"귀납 가정을 대입하고 정리하니 P(k+1)이 증명되었다"
[표Ⅱ-4] Dewey의 반성적 사고 5단계와 수학적 귀납법 학습 예시

위 표는 Dewey (1910)의 원문 표현을 정확히 제시하며, 한국어 명칭은 본 연구에서 교육적 맥락에 맞게 번역한 것이다.
나. 수학 학습에서 반성적 사고의 중요성
수학 학습에서 반성적 사고는 특히 중요하다. Schoenfeld(1985)는 수학적 문제해결 과정이 단순한 알고리즘 적용이 아니라, 문제 상황 분석, 전략 선택, 실행, 검증의 메타인지적 과정임을 강조하였다. 학생들이 수학적 문제에 직면했을 때, 자신의 기존 지식과 새로운 문제 상황 사이의 간극을 인식하고, 이를 해결하기 위해 체계적으로 탐구하는 과정이 바로 반성적 사고이다.

수학적 귀납법 학습에서는 다음과 같은 반성적 사고가 특히 중요하다:
● 문제 인식: " 단계가  단계와 어떻게 다른가?"
● 문제 정의: "귀납 가정을 '언제', '어디에' 사용하는가?"
● 가설 형성: "식을 전개하면 귀납 가정과 연결될까?"
● 검증: 실제로 전개하여 논리적 연결 확인
● 성찰: "이 방법을 다른 귀납법 문제에도 적용할 수 있을까?“

다. 기존 LLM의 한계: 2단계(문제 정의) 생략
I장의 예비 조사에서 확인된 프리패스 방식의 근본적 문제는 Dewey의 2단계(문제 정의)를 완전히 생략한다는 것이다(그림 2-1 상단 참조).

프리패스 방식에서 학생이 "귀납법 어려워요"라고 질문하면(1단계: 문제 상황 인식), AI는 2단계(문제 정의) 없이 즉시 3-5단계로 진행하여 "귀납법은 다음과 같이 증명합니다... 1. 기본 단계: n=1일 때... 2. 귀납 단계: n=k일 때 가정하면..."과 같은 일반적 설명을 제공한다. 그 결과 학생은 자신이 정확히 무엇을 모르는지 인식하지 못한 채 AI의 설명을 수동적으로 받아들이게 된다.

이는 I장에서 관찰된 학습맥락 부재 72.3%의 근본 원인이다. 학생이 자신의 어려움을 명확히 정의하지 못한 상태에서 AI가 일방적으로 답변하므로, 학습자 수준 파악 실패(27.6% 최저점)와 학습확장성 결여(48.9% 최저점)로 이어진다.
라. 본 연구의 해결 방안: 2단계 명료화 프로세스

본 연구의 에이전트 시스템은 Dewey의 2단계(문제 정의)를 명료화 프로세스로 구현하여, 학생이 스스로 질문을 구조화하도록 돕는다(그림Ⅱ-1 하단 참조).

본 연구의 명료화 방식에서는 학생이 "귀납법 어려워요"라고 질문하면(1단계: 문제 상황 인식), QI 에이전트가 2단계(문제 정의)를 위해 명료화 질문을 제시한다: "귀납법 중에서 어떤 부분이 가장 어렵거나 궁금하신가요? • 기본 단계 증명? • 귀납 가정 이해? • 귀납 단계 전개?" 학생이 "귀납 단계에서 식을 전개하는 과정이요"라고 응답하면 2단계가 완성되고, AG 에이전트는 학생이 정의한 문제에 맞춘 맞춤형 답변을 생성한다: " 대입 후 식 전개 과정을 단계별로 설명해드릴게요..."

그 결과 학생이 자신의 어려움을 명확히 인식하고 표현하게 되며, 이는 메타인지 능력 향상과 맞춤형 답변 수신으로 학습 효과 증대로 이어진다.

마. 본 연구에의 적용
듀이의 반성적 사고 이론은 본 연구 시스템의 명료화 프로세스 설계에 적용된다. Dewey 5단계는 시스템의 5개 에이전트 구조와 직접 매핑되며(상세 매핑은 II장 6절 참조), 특히 2단계(문제 정의)가 QI 에이전트의 명료화 프로세스로 구현된다(IV장 참조).

2단계(문제 정의)의 구현인 명료화 프로세스는 Dewey의 5단계를 실제 대화형 질문으로 변환한다:
◯ Dewey 5단계 기반 명료화 전략:
● 1단계 (문제 인식): "어떤 부분이 가장 어렵거나 궁금하셨나요? "
● 2단계 (문제 정의): "이해한 부분과 아직 헷갈리는 부분을 나누어볼까요?"
● 3단계 (연결 탐색): "알고 있는 개념과 비교하면 어떤 점이 비슷하거나 다른가요?"
● 4단계 (사고 전개): "왜 이 부분이 궁금하신지 조금 더 설명해주실 수 있나요?"
● 5단계 (이해 검증): "어디서부터 막히셨는지 말씀해주실 수 있나요?"
◯ 실제 구현 특징:
● K1-K4 질문 유형별로 맞춤형 명료화 질문 생성
● 매우 모호한 질문("수열 알려줘")에는 구체적인 선택지 제공
● 각 학생 응답마다 PASS/NEED_MORE 평가로 충분성 판단
● 최대 3회 시도 제한으로 과도한 명료화 방지

이는 단순히 정보를 전달하는 것이 아니라, 학생이 스스로 질문을 구조화하고 사고를 명료화하는 과정을 경험하도록 설계된 것이다. 이러한 명료화 프로세스는 Dewey의 반성적 사고 이론에 따르면 메타인지 발달과 학습 효과 향상을 가져올 것으로 기대된다. 본 연구에서는 6장에서 이를 실증적으로 검증한다.

4. 질문 생성 및 개선 이론: 질문 품질의 구조화
가. 질문 생성의 교육적 가치
질문 생성(question generation)은 학습자의 메타인지 및 비판적 사고를 유도하는 핵심적 활동으로, 학습자 주도적인 문제 해결과 이해 중심 수업을 가능하게 한다(홍경선, 김동익, 2011).

King(1994)은 초등학교 4-5학년 58명을 대상으로 한 실험 연구에서, 질문 생성 훈련을 받은 학생들이 대조군에 비해 이해도 검사(literal comprehension: F(2,21)=3.56, p<.05; inferential/integrative comprehension: F(2,21)=4.17, p<.05)와 7일 후 파지 검사(retention test: F(2,21)=16.34, p<.001)에서 유의미하게 높은 성취를 보였다. 특히 경험 기반 질문(experience-based questions)을 생성한 학생들이 수업 내 질문(lesson-based questions)만 한 학생들보다 장기 기억에서 더 우수한 성과를 나타냈다. 이는 학생 생성 질문이 단순 정보 습득을 넘어 복잡한 지식 구성(complex knowledge construction)을 유도하며, 이것이 학습 효과를 증진시킨다는 것을 실증적으로 보여준다.


특히 수학 학습에서 학생이 스스로 질문을 생성하는 것은 다음과 같은 교육적 효과를 갖는다:
◯ 능동적 학습: 교사의 일방적 설명을 수동적으로 받아들이는 것이 아니라, 학습자가 능동적으로 지식을 구성
◯ 메타인지 발달: 자신이 무엇을 아는지, 무엇을 모르는지 스스로 인식하는 능력 향상
◯ 개념 이해 심화: 질문을 구성하는 과정에서 개념 간 관계를 탐색하고 논리적 연결을 시도
◯ 학습 동기 증진: 자신의 궁금증에서 시작한 질문은 학습 몰입도를 높임

나. 효과적인 질문의 특징
King(1994)은 학생 생성 질문이 깊이 있는 이해와 장기 기억에 긍정적 영향을 미친다는 것을 실증하였으며, Graesser & Person(1994)은 튜터링 과정에서의 질문 유형을 체계적으로 분류하였다.[7] 본 연구는 이러한 선행 연구들과 예비 조사에서 관찰된 질문 품질 문제를 종합하여, 효과적인 학습 질문의 3가지 핵심 특징을 다음과 같이 도출하였다:
다음 제시하는 효과적인 질문의 3가지 특징은 King(1994)과 Graesser & Person(1994)의 이론적 틀을 바탕으로, 본 연구의 예비 조사에서 발견된 질문 품질 문제(I장 1.1.2절)를 해결하기 위해 구체화한 것이다.

1) 구조화 (Structuring)
질문이 명확한 구조를 가지고 있어야 한다:

질문 의도
기대 답변 유형
예시
정의 확인
간결한 정의
"귀납 가정이 정확히 뭐예요?"
개념 이해
관계 설명
"귀납 가정은 왜 필요한가요?"
절차 학습
단계별 안내
"귀납 가정을 어떻게 사용하나요?"
오류 진단
메타인지적 피드백
"제 풀이에서 뭐가 틀렸나요?"
[표Ⅱ-5] 효과적인 질문의 구조 요소

◯ 좋은 예시: "귀납 단계에서 을 증명할 때, 귀납 가정 를 어떻게 사용하나요?"
● 대상: 귀납 가정 
● 범위:  증명 단계
● 초점: 사용 방법

◯ 나쁜 예시: "귀납법 어려워요"
● 대상 불명확 (기본 단계? 귀납 단계? 개념?)
● 범위 불명확 (어떤 문제 유형?)
● 초점 부재 (무엇이 어려운지 불명확)

2) 완결성 (Completeness)
질문에 필요한 조건과 정보가 모두 제시되어야 한다:


조건 유형
정의
예시
학습 수준
현재 어디까지 배웠는가
"수열까지는 배웠어요"
선수 지식
어떤 배경 지식을 가지고 있는가
"등비수열은 알아요"
시도한 방법
무엇을 시도해봤는가
"을 대입했는데..."
막힌 지점
어디서 막혔는가
"식 정리 과정에서"
[표Ⅱ-6] 효과적인 질문의 완결성 조건

◯ 완결성 높은 질문 예시:
"고2 수학에서 수학적 귀납법을 배우고 있는데, 등비수열 합 공식을 귀납법으로 증명하는 문제에서  단계의 식을 전개했을 때 귀납 가정을 어떻게 대입해야 하는지 모르겠어요.“

이 질문은 학습 수준(고2), 학습 주제(귀납법), 구체적 문제(등비수열 합), 시도한 방법( 대입), 막힌 지점(귀납 가정 대입)을 모두 제시하여 AI가 맞춤형 답변을 생성할 수 있게 한다.
3) 의도의 명시성 (Explicitness)
질문의 목적과 기대하는 답변 유형이 명확해야 한다:


질문 의도
기대 답변 유형
예시
정의 확인
간결한 정의
"귀납 가정이 정확히 뭐예요?"
개념 이해
관계 설명
"귀납 가정은 왜 필요한가요?"
절차 학습
단계별 안내
"귀납 가정을 어떻게 사용하나요?"
오류 진단
메타인지적 피드백
"제 풀이에서 뭐가 틀렸나요?"
[표Ⅱ-7] 효과적인 질문의 의도 명시성

◯ 의도가 명확한 질문:
● "귀납 단계의 정의를 간단히 설명해주세요" → 간결한 정의 기대
● "귀납 가정을 단계별로 어떻게 사용하는지 예시를 들어 설명해주세요" →   상세한 절차 기대

다. 기존 연구의 한계: 피드백 시스템 부재
대부분의 선행연구들은 질문 생성 기법을 가르치는 데 초점을 맞추었으며, 질문의 질적 평가와 개선을 위한 체계적인 피드백 시스템에 대한 연구는 제한적이다. 교사가 좋은 질문 만드는 방법을 교육하더라도, 학생이 실제 질문을 생성할 때 질문 품질을 평가하고 개선 피드백을 제공하는 시스템이 없다면 다음과 같은 문제가 발생한다:

◯ 모호한 질문 → 모호한 답변: 질문 품질이 낮으면 AI 답변도 일반적이고 맥락 없는 설명에 그침
◯ 나쁜 습관 고착: 피드백 없이 반복되면 모호한 질문 습관이 고착됨
◯ 질문 개선 기회 상실: 질문을 구조화하고 명료화하는 메타인지 능력 발달 기회를 놓침

특히 AI 학습 환경에서는 즉각적인 질문 개선 피드백이 더욱 중요하다. 학생이 질문하는 순간이 바로 질문 품질을 개선할 수 있는 교육적 기회이기 때문이다.
라.  연구의 접근: 명료화 기반 질문 개선
본 연구 시스템은 질문 생성 이론을 실시간 피드백 시스템으로 구현하여, 학생이 질문하는 순간 질문 품질을 개선할 수 있도록 돕는다:

[그림Ⅱ-2] 에이전트 시스템의 질문 개선 메커니즘: 실시간 명료화


이는 단순히 질문 방법을 가르치는 것이 아니라, 질문하는 순간 실시간으로 개선 과정을 경험하게 하여 메타인지 능력을 체득하도록 한다.

마. 1장 문제점과의 연결
예비 조사에서 발견된 질문 품질 문제는 질문 생성 이론의 3가지 특징 결여와 정확히 일치한다. 질문구조 불명확(45.8%)은 구조화 부족, 학습맥락 부재(72.3%)는 완결성 부족, 수학적전문성 결여(45.5%)는 의도 명시성 부족에서 기인한다(상세 매핑은 표Ⅱ-2 참조). King(1994)의 이론에 따르면 좋은 질문이 좋은 학습을 만든다.

바. 본 연구에의 적용
질문 생성 및 개선 이론은 본 연구 시스템에 다음과 같이 적용된다:
◯ 질문 품질 평가 기준: 구조화·완결성·의도를 측정하는 QAC 체크리스트 A영역 설계
◯ 명료화 전략 (IV장): 3가지 특징을 유도하는 단계별 질문 설계
◯ 실시간 피드백 (IV장): QC 에이전트가 질문 품질을 진단하고 개선 방향 제시
◯ 효과성 검증 (V장, VI장): 명료화 프로세스가 질문 품질 향상과 메타인지 발달로 이어지는지 검증

5. AI 기반 피드백 시스템: 실시간 상호작용의 설계 원리
가. 피드백의 교육적 효과
피드백은 학습 과정에서 핵심적 역할을 하며, 특히 즉시성(immediacy)과 맞춤성(adaptiveness)이 학습 효과를 결정한다. Hattie와 Timperley(2007)의 메타분석 연구에 따르면, 효과적인 피드백은 학습자의 성취도에 평균 효과 크기 d=0.79의 큰 영향을 미친다.


요소
정의
수학 학습 예시
즉시성
학습자 행동 직후 제공
질문 직후 명료화 질문 제시
구체성
일반적이 아닌 구체적 지적
"귀납 가정 사용 부분에서..."
건설성
오류 지적 + 개선 방향
"n=k+1 대신 귀납 가정을 먼저 확인해보세요"
맞춤성
학습자 수준에 적합
하위권에게는 더 세분화된 단계 제공
[표Ⅱ-8] 효과적인 피드백의 핵심 요소


나. 질문 명료화를 통한 답변 범위 제한
본 연구 시스템의 핵심 전략은 질문을 구체화하여 답변 범위를 자동으로 제한하는 것이다:

◯ 기존 AI의 문제:
● 모호한 질문에 대해 모든 내용을 포괄적으로 설명
● 학생이 실제로 필요한 부분을 찾기 어려움
● 인지 과부하 발생 (정보량 과다)

본 연구 시스템의 접근:


단계
역할
활동
이론적 기반
1
QC
"어떤 부분이 어렵나요?"
Dewey 1단계: 문제 인식
2
학생
"귀납 단계요"
문제 구체화
3
QC
"귀납 단계 중에서?"
Dewey 2단계: 문제 정의
4
학생
"식 전개 과정"
최종 명료화
5
AG3
식 전개만 설명
Bloom K3: 절차적 지식
[표Ⅱ-9] 질문 명료화를 통한 답변 범위 제한 메커니즘


◯ 핵심 원리:
● 질문이 구체화되면 → 답변 범위가 자동으로 제한됨 → 인지 부담 최소화
● 학생은 "귀납법 전체"가 아니라 "식 전개 과정"만 학습하게 되어, 필요한 정보에 집중할 수 있다.




다. 실시간 상호작용의 기술적 구현
 AI 에이전트 시스템는 실시간 피드백의 즉시성을 보장하기 위해 다음 기술을 활용한다:
◯ 스트리밍 응답: AI 답변을 타이핑하듯 실시간으로 전송 (Server-Sent Events)
◯ 비동기 처리: FastAPI의 async/await로 여러 학생의 질문을 동시에 처리 
◯ 멀티프로세스 아키텍처: 5개 에이전트가 독립 프로세스로 실행되어 병렬 처리 
◯ 응답 시간 최적화: 평균 2초대 첫 응답 

라. 본 연구에의 적용
AI 피드백 이론은 AI 에이전트 시스템에 다음과 같이 적용된다:

◯ 즉시성: 학생 질문 직후 QC 에이전트가 품질 진단, QI 에이전트가 명료화 질문 제시
◯ 구체성: 질문의 어떤 부분(구조화/완결성/의도)이 부족한지 명시적 피드백
◯ 건설성: 단순 지적이 아닌 명료화 질문으로 개선 방향 제시
◯ 맞춤성: K1-K4 유형별 차별화된 답변 전략 

기술적 구현(스트리밍 응답, 비동기 처리)은 IV장 및 부록 A에서, 효과성 검증은 QAC 루브릭의 C2(학습 과정 지원) 항목으로 측정한다(V장, VI장).


6. 멀티 에이전트 시스템: 역할 분담과 협업
가. 멀티 에이전트 시스템의 정의
멀티 에이전트 시스템(Multi-Agent System, MAS)은 여러 자율적 agent가 협력하여 복잡한 문제를 해결하는 분산 시스템이다.[9] 교육 분야에서 멀티 에이전트 시스템은 학습자의 다양한 요구에 대응하는 개인화된 학습 환경을 구축하는 데 활용되고 있다.



Wooldridge와 Jennings(1995)는 agent의 주요 특성으로 다음을 제시했다

특성
정의
교육적 응용
자율성(Autonomy)
독립적으로 작동, 외부 개입 최소화
각 agent가 독자적 판단 및 실행
사회성(Social Ability)
다른 agent와 협력 및 정보 교환
agent 간 질문·답변 전달
반응성(Reactivity)
환경 변화를 감지하고 적절히 대응
학생 질문 패턴 변화 감지
능동성(Pro-activeness)
목표 지향적 행동, 선제적 조치
학습 어려움 조기 발견 및 개입
[표Ⅱ-10] Wooldridge & Jennings의 Agent 특성과 교육적 응용

나. 
[그림Ⅱ-3] 단일 Agent 시스템
단일 Agent vs 멀티 Agent의 차이


[그림Ⅱ-4] 멀티 Agent 시스템의 역할 분담과 협업
단일 Agent 시스템은 모든 역할을 하나의 Agent가 처리하므로 역할 간 충돌, 복잡도 증가, 전체 실패 위험, 유지보수 어려움 등의 문제가 발생한다.

멀티 Agent 시스템은 역할을 명확히 분담하여 각 agent를 독립적으로 최적화할 수 있고, 한 agent의 오류가 전체 시스템에 미치는 영향을 최소화하며, 새로운 agent 추가가 용이하다. 
다. 본 연구에의 적용
멀티 에이전트 이론은 본 연구 시스템에 다음과 같이 적용된다:

◯ 명확한 역할 분담: 5개 agent의 독립적 책임 정의 (IV장)
◯ agent 간 협업 프로토콜: 정보 교환 구조 설계 (IV장)
◯ 독립적 최적화: 각 agent의 프롬프트 독립 개선 가능
◯ 시스템 안정성: 한 agent 오류 시 fallback 전략
◯ 확장성: 새로운 agent 추가 용이한 아키텍처

특히, 본 연구 시스템은 교육적 프로세스를 agent 구조로 구현함으로써:
◯ Dewey 5단계 각각을 담당 agent가 지원 (QC → QI → AG → LO)
◯ 명료화(2단계)를 독립 agent(QI)로 분리하여 핵심 강조
◯ 대화 요약(LO)을 통해 장기 세션에서도 컨텍스트 관리 가능

이러한 멀티 에이전트 구조는 교육적 프로세스를 체계적으로 지원하며, 각 agent의 독립성과 협업을 통해 안정적인 시스템 운영이 가능할 것으로 기대된다. 본 연구에서는 V장과 VI장에서 실제 학생 세션을 통해 시스템의 안정성과 교육적 효과를 검증한다.



7. 수학적 귀납법 단원 선정: 단원 맥락 반영 필요성
가. 연구 대상 단원으로 수학적 귀납법을 선정한 이유
본 연구는 고등학교 2학년 수학Ⅰ 과정의 수학적 귀납법 단원을 AI 에이전트 시스템의 적용 대상으로 선정하였다. 이는 다음의 교육적 특성 때문이다:



1) 명료화가 특히 필요한 학습 내용

수학적 귀납법은 기본 단계()와 귀납 단계()라는 이중 구조를 가지며, 학생들은 자신의 어려움이 어느 단계에서 발생하는지 정확히 표현하기 어렵다.  "귀납법 어려워요"와 같은 막연한 질문이 빈번하게 발생하며, 이는 명료화 프로세스의 효과를 검증하기에 적합하다.

2) Dewey 반성적 사고의 적용 가능성

수학적 귀납법의 핵심인 귀납 가정은 "증명하려는 것을 먼저 가정한다"는 역설적 특성을 가진다. 이는 학습자에게 당혹감과 혼란을 야기하며(Dewey의 1단계), 이를 정확히 정의하고(2단계) 해결하는 반성적 사고 과정이 필수적이다.

3) 유형별 전략 차이

등식 증명과 부등식 증명은 전개 전략이 크게 다르다(등식: 대입 후 정리, 부등식: 보조 부등식 필요). 학생이 어떤 유형의 문제에서 어려움을 겪는지에 따라 다른 설명이 필요하므로, 질문 명료화의 중요성이 더욱 부각된다.

나. LLM의 맥락 적응 능력 활용
본 연구 시스템은 수학적 귀납법에 특화된 별도의 코드를 작성하지 않는다. 대신 LLM의 맥락 이해 능력을 활용하여, 학생 질문에 포함된 "귀납법", "귀납 가정" 등의 단어로부터 단원 맥락을 자동으로 인식하고 적응한다:
◯ QC 에이전트: 질문에서 "귀납 가정을 어떻게 써요?"를 보고 K3(절차적 지식)로 분류
◯ QI 에이전트: "귀납법"이라는 맥락을 파악하여 "어느 단계가 어려운가요? 기본 단계? 귀납 단계?"와 같은 명료화 질문 동적 생성
◯ AG 에이전트: K1-K4 분류에 따라 단원 맥락에 맞는 답변 생성
이러한 설계는 향후 다른 수학 단원(미적분, 삼각함수 등)으로 확장할 때도 시스템 구조 변경 없이 적용 가능함을 의미한다.
수학적 귀납법 단원의 구체적 특성과 시스템의 실제 적용 사례는 IV장과 V장에서 상세히 다룬다.

8. 평가 루브릭의 이론적 기반
본 연구는  AI 에이전트 시스템의 효과성을 측정하기 위해 질문-답변 품질을 평가하는 QAC(Question-Answer-Context) 체크리스트를 개발하였다. 본 절에서는 루브릭의 개발 동기, 이론적 기반, 그리고 체크리스트 구조를 제시한다.

가. 루브릭 개발의 필요성과 이론적 기반
1) 개발 동기: 예비 조사에서 발견된 문제
본 연구는 AI 에이전트 시스템의 개발에 앞서 실제 고등학생 385명이 AI와 나눈 수학 학습 대화를 분석하는 예비 조사를 수행하였다. 이 과정에서 학생 질문과 AI 답변에서 다음과 같은 체계적 문제가 발견되었다:

◯ 질문 영역의 문제:
● 학습 맥락 부재: "수학적 귀납법 어려워요"와 같이 현재 학습 수준, 어려움의 구체적 지점, 학습 목표를 제시하지 않음
● 질문 구조 불명확: 한 질문에 여러 의도가 섞이거나, 필요한 조건이 누락됨
● 수학적 표현 부정확: 수학 용어의 오용 또는 개념 혼동

◯ 답변 영역의 문제:
● 학습 확장성 결여: 단편적 답변 제공 후 심화 방향 제시 없음
● 학습자 맞춤도 부족: 학생의 수준이나 선수학습 상태를 고려하지 않은 일률적 설명
● 설명 체계성 부족: 논리적 흐름 없이 단편적 정보 나열

이러한 문제는 단순한 정답 제시를 넘어 교육적으로 효과적인 대화를 구성하는 요소가 무엇인지 명확한 기준이 필요함을 보여준다. 기존 AI 평가 연구들은 주로 답변의 정확성만을 평가하였으나, 본 연구는 질문 품질도 학습 효과의 핵심 요소로 보고 이를 독립적으로 평가하는 체계를 개발하였다.

2) 이론적 기반: 교육학 이론의 통합
루브릭의 6개 평가 영역은 전통적 교육학 이론에 기반하여 설계되었다:

(1) A영역: 질문 평가 (15점)
◯ A1. 수학적 전문성 (5점)
● 이론적 기반: 내용지식(content knowledge)의 중요성
● 측정 대상: 수학 개념의 정확성, 용어 사용의 적절성, 교과과정 내 위계 파악
● 교육적 의의: 학습자가 문제의 본질을 정확히 파악하고 있는지 평가

◯ A2. 질문 구조화 (5점)
● 이론적 기반: King(1994) 질문 생성 이론, Graesser & Person(1994) 튜터링 질문 분류
● 측정 대상: 질문의 단일성, 조건 완결성, 문장 논리성, 의도 명확성
● 교육적 의의: 명확한 질문은 명확한 답변을 유도하며, 이는 효과적 학습 대화의 출발점

◯ A3. 학습 맥락 적용 (5점)
● 이론적 기반: 상황학습이론(situated learning), 근접발달영역(ZPD) 개념
● 측정 대상: 현재 학습 단계, 선수학습 내용, 구체적 어려움, 학습 목표
● 교육적 의의: 맥락 정보는 AI가 학생에게 적합한 수준의 설명을 제공하는 데 필수적


(2) B영역: 답변 평가 (15점)
◯ B1. 학습자 적합성 (5점)
● 이론적 기반: 비계설정(scaffolding), 차별화 교수(differentiated instruction)
● 측정 대상: 수준별 접근, 선수지식 연계, 난이도 조절, 개인화 피드백
● 교육적 의의: 학생의 현재 수준에 맞는 설명이 학습 효과를 극대화

◯ B2. 설명의 체계성 (5점)
● 이론적 기반: 인지부하이론(cognitive load theory), 멀티미디어 학습 원리
● 측정 대상: 개념 위계화, 단계별 논리, 핵심 강조, 예시 적절성
● 교육적 의의: 체계적 설명은 인지 부하를 줄이고 이해를 촉진

◯ B3. 학습 내용 확장성 (5점)
● 이론적 기반: Bloom(1956, as cited in Anderson & Krathwohl, 2001) 교육목표분류학, Anderson & Krathwohl(2001) K1-K4 지식 차원
● 측정 대상: 심화 방향 제시, 응용 문제 연계, 오개념 교정, 자기주도 학습 유도
● 교육적 의의: 단편적 답변을 넘어 지속적 학습으로 연결

(3) C영역: 맥락 평가 (10점)
◯ C1. 대화 일관성 및 연속성 (5점)
● 이론적 기반: 대화 일관성 이론, 공통기반이론(common ground theory)
● 측정 대상: 학습 목표 중심성, 대화 이력 참조, 주제 연속성, 턴 간 유기적 연결
● 교육적 의의: 일관된 대화 흐름은 학습 몰입과 이해 누적을 지원

◯ C2. 학습 과정 지원성 (5점)
● 이론적 기반: Dewey(1910) 반성적 사고, 메타인지 이론
● 측정 대상: 사고 과정 유도, 이해도 확인, 메타인지 촉진, 깊이 있는 사고 유도
● 교육적 의의: 학습자가 자신의 사고 과정을 인식하고 조절하도록 지원
(4) 루브릭의 특징
본 루브릭은 기존 AI 평가 도구와 다음 점에서 차별화된다:
◯ 질문 품질 독립 평가: 기존 연구는 AI 답변만 평가했으나, 본 연구는 질문 품질이 학습 효과의 선행 조건임을 인식
◯ 이론-실증 통합: 교육학 이론을 실제 학생 데이터(385건)에서 발견된 문제와 연결
◯ 체크리스트 방식: 각 영역을 4개의 구체적 요소로 세분화하여 평가 일관성 확보 (체크리스트는 본 절 표Ⅱ-13 - 표Ⅱ-15 참조)
◯ 시스템 설계 근거 제공: 루브릭에서 발견된 A3(맥락 부재)와 B3(확장성 부족) 문제가 명료화 프로세스 설계 동기가 됨

나. 루브릭 구조: 체크리스트 기반 평가
예비 조사에서 발견된 문제 패턴과 교육 이론을 결합하여 8개 평가 항목, 32개 체크리스트 요소로 구성된 QAC 체크리스트를 개발하였다:

1) A영역: 질문 평가 (15점)

항목
체크리스트 요소
평가 기준 (질문 형태)
A1. 수학적 전문성(5점)
① 개념 정확성
수학 용어를 정확하게 사용했는가?
② 교과과정 위계
학년 수준에 맞는 개념인가?
③ 용어 적절성
전문 용어를 적절히 사용했는가?
④ 문제 방향 구체성
해결하려는 문제가 구체적인가?
A2.질문 구조화(5점)
① 질문 단일성
한 번에 하나의 명확한 질문을 하는가?
② 조건 완결성
필요한 조건/정보를 모두 제시했는가?
③ 문장 논리성
문장이 논리적으로 구성되었는가?
④ 의도 명확성
무엇을 알고 싶은지 명확한가?
A3. 학습 맥락(5점)
① 학습 단계 설명
학년/단원/진도를 언급했는가?
② 선수학습 언급
이전에 배운 내용을 언급했는가?
③ 어려움 명시
어디서 막혔는지 구체적으로 말했는가?
④ 학습 목표 제시
무엇을 배우고 싶은지 목표를 제시했는가?
[표Ⅱ-11] QAC 루브릭 A영역: 질문 평가 체크리스트


2) B영역: 답변 평가 (15점)

항목
체크리스트 요소
평가 기준 (질문 형태)
B1. 학습자 적합성(5점)
① 수준별 접근
학생 수준에 맞게 설명했는가?
② 선수지식 연계
이미 배운 내용과 연결했는가?
③ 난이도 조절
너무 어렵거나 쉽지 않은가?
④ 개인화 피드백
학생 상황을 고려한 피드백인가?
B2. 설명의 체계성(5점)
① 개념 위계화
쉬운 것부터 어려운 것으로 단계적 설명?
② 단계별 논리
각 단계가 논리적으로 연결되는가?
③ 핵심 강조
중요한 내용을 명확히 강조했는가?
④ 예시 적절성
이해를 돕는 적절한 예시 제공?
B3. 학습 확장성(5점)
① 심화 방향 제시
더 깊이 공부할 방향을 제시했는가?
② 응용 문제 연계
관련된 응용 문제를 연결했는가?
③ 오개념 교정
잘못된 이해를 바로잡았는가?
④ 자기주도 유도
스스로 탐구하도록 유도했는가?
[표Ⅱ-12] QAC 루브릭 B영역: 답변 평가 체크리스트



3) C영역: 맥락 평가 (10점)

항목
체크리스트 요소
평가 기준 (질문 형태)
C1. 대화 일관성(5점)
① 목표 중심성
학습 목표를 벗어나지 않고 진행?
② 맥락 참조
전체 대화 이력을 기억하고 참조?
③ 주제 연속성
주제가 자연스럽게 연결되는가?
④ 턴 간 연결
각 발화가 직전 턴과 유기적 연결?
C2. 학습 과정 지원(5점)
① 사고 과정 유도
학생의 사고 과정을 유도하는가?
② 이해도 확인
학생의 이해도를 확인하는가?
③ 메타인지 촉진
학생이 학습 과정을 돌아보게 하는가?
④ 깊이 있는 사고 유도
깊이 있는 사고를 유도하는가?
[표Ⅱ-13] QAC 루브릭 C영역: 맥락 평가 체크리스트



각 항목의 점수는 충족된 체크리스트 요소 개수로 자동 계산된다:
점수 = 충족 요소 개수 + 1
◯ 질문 영역 (A1+A2+A3): 최대 15점
◯ 답변 영역 (B1+B2+B3): 최대 15점
◯ 맥락 영역 (C1+C2): 최대 10점
◯ 전체 총점: 40점 만점


구분
낮은 품질 질문 예시
높은 품질 질문 예시
학생질문
"수학적 귀납법이 뭐야"
"모든 자연수 에 대해, 임을 수학적 귀납법을 이용하여 증명하시오. 기저단계, 귀납단계를 사용하여 수식으로 간단히 풀어내시오."
A1.수학적전문성
4점 (3개 충족)✅ 개념정확성, 용어적절성, 문제방향❌ 교과과정위계
5점 (4개 충족)✅ 개념정확성, 교과과정위계, 용어적절성, 문제방향
A2.질문구조화
4점 (3개 충족)✅ 질문단일성, 문장논리성, 의도명확성❌ 조건완결성
5점 (4개 충족)✅ 질문단일성, 조건완결성, 문장논리성, 의도명확성
A3.학습맥락
1점 (0개 충족)❌ 학습단계, 선수학습, 어려움, 목표 모두 부재
4점 (3개 충족)✅ 학습단계, 선수학습, 어려움❌ 학습목표
질문총점
9점/15점 (60%)
14점/15점 (93.3%)
[표Ⅱ-14] 체크리스트 기반 평가 예시 (실제 데이터)


체크리스트 방식은 이진 판단(충족/미충족)으로 평가자 간 일관성을 높이며, 어떤 요소가 부족한지 구체적으로 파악하여 개선 방향을 제시할 수 있다.

다. 본 연구에의 적용
본 루브릭은 기존의 검증된 측정 도구가 아닌, 본 연구가 개발하고 타당성을 탐색하는 평가 도구이다. 루브릭의 신뢰도 및 타당도 검증 과정, 교사 평가자 간 일치도, AI-교사 평가 일치도 등은 연구 결과 및 논의 장(V장, VI장)에서 상세히 다룬다.




III. 연구 방법 및 설계 기반 개선 과정
본 장에서는 Design-Based Research(DBR, 설계 기반 연구) 관점에서 AI 에이전트 시스템의 반복적 개선 과정과 본 연구의 A/B 테스트 방법론을 제시한다.

1. 연구 방법론: Design-Based Research
본 연구는 설계 기반 연구(Design-Based Research, DBR) 방법론을 채택하였다. DBR은 교육 이론을 실제 교육 맥락에서 검증하고, 실용적 산출물을 개발하며, 반복적 개선을 통해 설계 원리를 도출하는 연구 방법이다(Collins, Joseph, & Bielaczyc, 2004). Wang과 Hannafin(2005)은 DBR의 핵심 특징으로 실용성(pragmatic), 이론 기반(grounded), 상호작용성(interactive), 반복성(iterative), 유연성(flexible), 통합성(integrative)을 제시하였다.

본 연구의 DBR 수행 과정은 3차 반복 사이클로 진행되었다:

[그림Ⅲ-1] DBR 3차 사이클 및 반복적 개선 과정

◯ 사이클 1 (2025년 5월): 문제 분석 및 프로토타입 설계
● 예비조사 385건 질문-답변 분석 (I장, III-1절)
● 문제 발견: 학습 맥락 부재(72.3%), 질문-답변 상관(r=0.691)
● 이론 기반 설계: Dewey/Bloom →  AI 에이전트 시스템 5개 에이전트

◯ 사이클 2 (2025년 9월): 베타테스트 및 시스템 개선
● 베타테스트 10일간, 학생 11명 (V장 상세)
● 발견: 수식 입력 어려움, 이미지 질문 수요
● 개선: OCR 수식 인식 시스템 추가 (IV-7절)

◯ 사이클 3 (2025년 10-11월): 본 연구 A/B 테스트
● 무작위 배정 A/B 테스트 (N=58, 284개 세션)
● LLM-교사 이중 평가 + 질적 분석 (1,589건)
● 설계 원리 도출: 명료화 효과, 하위권 메커니즘 (VI-VII장)
DBR 반복성의 의의: 단순 일회성 실험이 아닌, 문제 발견 → 설계 → 검증 → 재설계의 체계적 과정을 거친 연구임을 보여준다.

2. 사이클 2: 베타테스트 및 반복적 개선 (2025년 9월)
본 절에서는 DBR 사이클 2에 해당하는 베타테스트 과정과 이를 통한 시스템 개선 사항을 기술한다 (V장 상세 내용 참조).

가. 베타테스트 개요
기간: 2025년 9월 15일 ~ 9월 25일 (10일)
참여자: 고2 학생 11명 (자발적)
단원: 수열 (등차수열, 등비수열)
목적: 시스템 안정성 + 교육적 효과 초기 검증
나. 주요 발견 및 개선
발견 1: 수식 입력 어려움
◯ 학생들이 LaTeX 수식 입력에 어려움 호소
● 개선: OCR 수식 인식 시스템 추가 (IV-7절)
      - 이미지 업로드 → Gemini Vision API → 수식 자동 인식
      - 입력 시간 단축 및 사용성 개선
발견 2: 메타인지 향상 징후
◯ 설문: "무엇을 모르는지 규정" 4.2/5점
◯ 설문: "사고 정교화" 4.1/5점
● 명료화 프로세스의 교육적 가능성 확인

발견 3: 시스템 안정성 개선 필요
◯ 간헐적 지연, 오류 보고
● 개선: FastAPI 워커 증설, Redis Pool 확대, 재시도 로직


다. 본 연구 설계 반영

베타테스트 발견사항은 다음과 같이 본 연구에 반영되었다:

◯ 무작위 대조 실험(RCT): 명료화 효과를 엄밀히 검증하기 위해 Agent vs Freepass 무작위 배정
◯ OCR 기능 탑재: 수식 입력 편의성 대폭 개선
◯ 시스템 안정화: 3주간 안정적 운영 가능한 인프라 구축
◯ QAC 체크리스트: 베타테스트 피드백을 반영한 객관적 평가 도구 개발

DBR 개선 사례: 베타테스트(n=11) → 문제 발견 → 시스템 개선 → 본 연구(N=58)

3. 사이클 3: 본 연구 A/B 테스트 (2025년 10-11월)
가. 표본 선정
본 연구는 부산광역시 소재 ○○고등학교 2학년 4개 학급을 대상으로 하였다. 표집 방법은 연구자의 접근 가능성을 고려한 편의 표집(convenience sampling)이었으며, 학교와 학급 선정 기준은 다음과 같다:
나. 참여자 특성

구분
내용
총 인원
58명
학년
고등학교 2학년
학교 유형
특수목적고등학교(공업계열)
지역
부산광역시
연구 기간
2025년 10월 20일 ~ 11월 1일 (약 2주)
[표Ⅲ-1] 연구 대상 개요


◯ 실험군과 대조군 구성:
● Agent 모드: 28명
● Freepass 모드: 30명

◯ 학생 사전 성적 분포 (중간고사 기준):
● 서술형 점수 (30점 만점): 범위 0~30점
● 객관식 점수 (70점 만점): 범위 14.6~61.7점
● 총점 (100점 만점): 범위 17.9~89.9점 (상세 기술통계는 표Ⅵ-2 참조)
다. 예비 조사
본 연구는 예비조사(N=385, 2025년 5월)를 통해 AI 에이전트 시스템 설계 필요성을 확인하였다 (상세: IV장 1절).

◯ 목적:  AI 에이전트 시스템의 필요성 검증
◯ 데이터: 고등학교 수학 수업에서 수집한 385건의 질문-답변 쌍
◯ 평가단: 현직 중등 수학교사 4명 
◯ 평가 방법: 교육학 이론 기반 평가 기준 (6개 영역, 5점 척도)
◯ 총 평가 건수: 1,012건 (교사 3-4명이 각 질문 평가)
예비 조사를 통해 학생 질문의 품질 문제와 질문-답변 품질 간 상관관계를 확인하여, 질문 명료화 기반 AI 에이전트 시스템의 필요성을 실증적으로 확인하였다. 

라. 무작위 배정
학급 내 학생들을 실험군과 대조군에 무작위 배정하기 위해  AI 에이전트 시스템의 UserModeService를 활용하였다. 이 서비스는 학생이 시스템에 가입할 때 Python의 random 모듈을 사용하여 "agent" 또는 "freepass" 모드를 50:50 비율로 자동 배정한다. 배정된 모드는 users 테이블의 assigned_mode 필드에 저장되며, 연구 기간 동안 고정된다.
무작위 배정 결과, Agent 모드 28명, Freepass 모드 30명이 배정되었다. 두 집단 간 사전 중간고사 성적에서 통계적으로 유의한 차이가 없어 동질성이 확보되었다.


변인
Agent 모드 (n=28)
Freepass 모드 (n=30)
t
p
해석

M(SD)
M(SD)



중간고사 총점 (100점)
56.8(16.7)
51.4(18.1)
1.18
.242
동질
- 서술형 점수 (30점)
15.6(8.5)
14.0(8.5)
0.74
.462
동질
- 객관식 점수 (70점)
41.2(9.4)
37.4(12.4)
1.30
.199
동질
[표Ⅲ-2] 실험군과 대조군의 사전 동질성 검증 (중간고사 기준)

해석: 모든 변인에서 p > .05로 두 집단 간 유의한 차이가 없어, 사전 동질성이 확보되었다. 이는 실험 처치 효과의 내적 타당도를 보장하는 중요한 근거가 된다.
마. 연구 윤리
1) 연구 참여 동의
◯ 모든 참여 학생과 보호자에게 연구 목적, 절차, 데이터 활용 방법을 설명하였다
◯ 학생 및 보호자로부터 서면 동의를 받았다
◯ 참여 거부 및 중도 철회 권리를 명시하였다

2) 개인정보 보호
◯ 수집된 데이터는 연구 목적으로만 사용되었다
◯ 데이터는 암호화된 서버에 저장하고, 연구자만 접근 가능하도록 제한하였다

3) 참여자 익명화
◯ 모든 학생 학번(24.xxx)을 익명 ID(S01~S58)로 변환하여 사용하였다
◯ 익명화 규칙:
● Agent 모드 학생: S01 ~ S28 (28명)
● Freepass 모드 학생: S29 ~ S58 (30명)
◯ 익명화 매핑 테이블은 별도 암호화 파일로 관리하며, Git 저장소에서 제외하였다
◯ 논문에는 익명 ID만 사용하여 개인 식별 불가능하도록 하였다


4) 참여자 보호
◯ 연구 참여로 인한 학업 불이익이 없음을 보장하였다
◯ 두 모드(Agent/Freepass) 모두 교육적 가치를 제공하도록 설계하였다
◯ 연구 종료 후 모든 참여자에게 두 모드의 사용 기회를 제공하였다


5) 데이터 보관 및 폐기
◯ 연구 데이터는 연구 종료 후 3년간 보관하며, 이후 안전하게 폐기한다
◯ 논문 출판 시 개인을 특정할 수 있는 정보는 일체 포함하지 않는다

6) 설문 식별자 수집
본 연구는 사후 설문조사에서 학생의 이메일 주소를 수집하였다. 이는 모드별 효과 비교를 위해 설문 응답과 객관적 데이터(QAC 점수, 교사 평가, 세션 사용 패턴)를 연계하기 위한 식별자로 활용되었다. 비익명 설문의 한계점 및 완화 전략은 9장 "연구의 한계" 섹션에서 논의한다.

바. 실험 설계 및 진행 절차
1) 1단계: 수학적 귀납법 개념 학습 (사전 교육)

A/B 테스트 시작 전, 모든 학생에게 수학적 귀납법에 대한 수업을 진행하였다. 수업은 학생 선행 학습 + 교사 해설 방식으로 진행되었으며, 매 수업 해설마다 핵심 개념을 반복적으로 강조하였다:


단계
내용
강조 개념
교수법
첫 수업
수학적 귀납법의 원리와 구조
① 템플릿과 도미노 모델
• 3단계 구조 (베이스, 가정, 결론)
• 도미노 비유: "첫 번째가 넘어지고, 하나가 넘어지면 다음도 넘어지면, 모두 넘어진다“
• 증명의 표준 형식 제공
강의 중심개념 도입
매 수업(과제 풀이)
학생들이 문제를 미리 풀고 제출
-
선행 학습시행착오 경험
매 수업(교사 해설)
제출된 문제를 해설하며핵심 포인트 반복 강조
② 귀납가정 → 귀납결론 유도
• k일 때 성립한 가정을 k+1 증명에 어떻게 활용하는가
• 논리적 연결고리 찾기

③ 등식 vs 부등식 전략
• 등식: "필요한 재료만 딱 맞게" (정확성 강조)
• 부등식: "스페어 부품도 있는" (부등호 여유 활용)
• 명제 유형별 차별적 접근
예제 중심반복 강조
[표Ⅲ-3] 수학적 귀납법 수업 구조 및 핵심 개념

◯ AI 학습과의 연결:
이러한 선행 학습 경험과 반복 강조된 개념들은 A/B 테스트 기간 동안 학생들이 AI와 대화할 때 사고의 틀과 공통 언어로 작용하였다. 특히 "도미노", "재료" 등의 비유가 학생들의 질문과 AI의 답변에서 자연스럽게 활용되었다.
2) 2단계: 수리논술 과제 단계적 부여 및 AI 학습 시스템 활용

본 연구는 2025학년도 2학기 수학 수리논술 수행평가의 일환으로 총 5개의 수학적 귀납법 증명 과제를 단계적으로 부여하였다. 학생들은 각 과제를 해결하는 과정에서 수업 시간(40분) 및 쉬는 시간(10-15분)을 활용하여 AI 학습 시스템 활용에 자유롭게 접근하였다.

가) 과제 구성 및 진행 방식:


과제
문제 1
문제 2
주요 개념
과제 1
등비급수 합 공식

팩토리얼 부등식
기본 급수, 팩토리얼
과제 2
피보나치 수열 합
지수 부등식
점화식, 
부등식
과제 3
팩토리얼 합 공식

로그 부등식

곱셈 전개, 
로그
과제 4
제곱수 합 공식

제곱 부등식
제곱수 
급수, 부등식
과제 5
하노이탑 일반항
, 
거듭제곱 부등식
점화식, 
거듭제곱
[표Ⅲ-4] 수리논술 과제 세부 내용

나) 학습 과정 구조:

[그림Ⅲ-2] 학습 과정 구조

학생들은 과제를 받고 개별 풀이를 시도하며 필요시 언제든지 AI 학습 시스템을 활용한다. 막히는 부분이 있을 때뿐만 아니라, 풀이 과정 확인이나 자신의 풀이 검토 시에도 자유롭게 사용한다. Agent 모드(28명)는 명료화 질문을 통해 문제를 구체화하는 과정을 거치고, Freepass 모드(30명)는 즉시 답변을 받는다. AI 답변을 참고하여 풀이를 완성한 후 교사와 함께 검토하며, 이 과정을 5개 과제에 걸쳐 반복한다.

AI 학습 시스템 활용 방식:

학생들은 다음과 같은 상황에서 AI 학습 시스템을 자유롭게 활용하였다:

(1) 수업 시간 활용 (주 활용 시간):
● 교사가 과제를 제시한 후 개별 풀이 시간 제공 (수업 40분 중 20-30분)
● 개인 노트북/태블릿으로 AI와 대화하며 필요시 언제든지 접속
● 막힐 때: "귀납 가정을 어디에 사용하나요?", "이 식을 어떻게 전개하죠?"
● 풀이 검토 시: "제 풀이 맞나요?", "이렇게 증명해도 되나요?"
● 과정 확인 시: "이 단계가 왜 필요한가요?", "다른 방법도 있나요?“
(2) 쉬는 시간 활용 (보조 활용):
● 수업 시간에 완전히 해결하지 못한 부분을 쉬는 시간에 추가 질문
● 완성한 풀이를 AI 학습 시스템에 입력하여 검토 요청
● 평균 2-3회의 짧은 대화 세션 (세션당 5-10분)
(3) 과제 해결 패턴:
● Agent 모드 학생: 명료화 질문을 통해 문제를 단계별로 구체화하며 해결
● Freepass 모드 학생: 즉시 제공되는 답변을 참고하여 풀이 작성
다) 교사 협력 학습:
● 각 과제 제출 후 교사가 학급 전체와 함께 표준 풀이 과정 검토
● 학생들이 를 통해 얻은 인사이트를 수업 중 공유
● 일반적인 오류 및 개선 방향 논의
라) 데이터 수집:
● 모든 학생-AI 대화 내용 자동 저장 (총 284개 세션)
● 세션별 질문 품질, 답변 품질, 학습 지원 수준 자동 평가
● 학생별 과제 완성도 및 제출 시간 기록
마) 실험 일정 및 기간:


회차
과제
활동 내용
수업 시간
1회차
-
수학적 귀납법 개념 학습 AI 에이전트 시스템 사용법 안내
1교시 (40분)
2회차
과제 1
등비급수, 팩토리얼 부등식+ 풀이 검토 및 피드백
1교시 (40분)
3회차
과제 2
피보나치 수열, 지수 부등식+ 풀이 검토 및 피드백
1교시 (40분)
4회차
과제 3
팩토리얼 곱셈, 로그 부등식+ 풀이 검토 및 피드백
1교시 (40분)
5회차
과제 4
제곱수 합, 제곱 부등식+ 풀이 검토 및 피드백
1교시 (40분)
6회차
과제 5
하노이탑, 거듭제곱 부등식+ 풀이 검토 및 피드백
1교시 (40분)
7회차
-
전체 과제 종합 리뷰 및 심화 문제 풀이
1교시 (40분)
8회차
-
사후 설문 및 연구 종료
1교시 (40분)
[표Ⅲ-5] 수리논술 과제 실시 일정




◯ 총 실험 기간: 약 2주 (2025년 10월 20일~11월 1일, 총 8회차 수업)
● 개념 학습: 1회차
● 과제 활동 및 풀이 검토: 2~6회차 (각 과제 수행 + 수업 말미 검토)
● 종합 리뷰: 7회차
● 사후 설문: 8회차
● AI 학습 시스템 활용 가능 시간: 수업 중 20-30분 + 쉬는 시간 10-15분
● 총 세션 수: 284개 (Agent 모드 118개, Freepass 모드 162개)
● 유효 세션 (메시지 ≥2): 280개 (Agent 118개, Freepass 162개)
● 평균 세션 길이: 약 15분 (최소 3분 ~ 최대 45분)

3) 3단계: 모드별 AI 활용 패턴 관찰
◯ Agent 모드: 명료화 질문을 통한 질문 구체화 과정 경험
◯ Freepass 모드: 즉시 답변 제공 방식으로 학습

4) 4단계: 데이터 수집 및 평가
본 연구는 수집된 세션 데이터를 다각도로 분석하여 신뢰성과 타당성을 확보하고자 하였다.
가)  다중 AI 모델 채점 시스템
평가자 편향(rater bias)을 최소화하고 채점 신뢰성을 높이기 위해 3개의 독립적인 대규모 언어 모델을 평가자로 활용하여 교차 검증을 실시하였다:

모델
개발사
선정 이유
Gemini 2.5 Flash
Google
긴 맥락 처리 능력, Batch API 지원, 한국어 성능 우수
Claude 4.5 Haiku
Anthropic
빠른 처리 속도, 일관성 있는 평가, Message Batches 지원
GPT-5 mini
OpenAI
범용적 평가 능력, 비용 효율성, Batch API 지원
[표Ⅲ-6] AI 모델 채점자 구성

채점 절차:
◯ 모든 세션 데이터를 JSON 형식으로 수집
◯ 각 모델에 동일한 QAC 체크리스트와 평가 프롬프트 제공
◯ 모델별로 독립적으로 채점 수행 (블라인드 평가)
◯ 3개 모델 점수의 평균(Ensemble)과 개별 모델 점수 모두 분석

평가 프롬프트: 3개 AI 모델에 동일한 QAC 체크리스트 프롬프트 제공 (32개 요소, 0/1 판단, JSON 응답, 상세: 부록 B).

나) 교사 평가를 통한 타당도 검증
AI 채점의 타당성을 검증하기 위해 현직 교사 2명이 독립적으로 평가하였다, 두 평가자는 동일한 100개 세션을 독립적으로 평가하여 완벽한 대응 평가(paired evaluation)를 수행하였다.

평가자 ID
교과
평가 세션 수
비고
평가자 96
수학
100개
외부 독립 평가자
평가자 97
수학
100개
외부 독립 평가자
합계
 2명
200개 평가
대응 평가 (동일 100개 세션)
[표Ⅲ-7] 교사 평가단 구성 (외부 평가자)


100개 세션 선별: 계층적 목적 표집
◯ LLM이 평가한 284개 세션 중 교사 검증용 100개를 4가지 전략으로 선별:
● AI 모델 간 불일치도 기반 (20개): 3개 모델 채점 차이가 큰 세션의 정답 기준 확립
● 성적 구간별 계층 표집 (64개): Quartile 분류(Q1~Q4) × Mode → 각 조합 8개씩 균등 표집
● 루브릭 패턴 특이 케이스 (10개): 특정 항목만 극단적 점수인 경우 타당성 검증
● 세션 길이 다양성 (6개): 짧은/중간/긴 세션별 AI 채점 일관성 검증


◯ 평가 절차:
● 동일한 100개 세션을 2명의 교사가 독립적으로 평가 (완벽한 대응 평가)
● AI 모델과 동일한 QAC 체크리스트 사용 (2.8 참조)
● 학생 모드 정보 블라인드 처리로 평가 편향 방지
● 교사 평가 완료 후 AI 채점 결과와 비교

◯ 검증 방법:
● 교사 간 신뢰도: Pearson 상관계수, Spearman 순위 상관
● AI-교사 일치도: AI 채점 평균 vs 교사 평균 간 상관분석
● 영역별 일치도: A, B, C 영역별 상관계수 계산
● 모드별 효과 수렴: 교사와 AI가 Agent vs Freepass 차이를 동일하게 감지하는지 확인

다) 다층적 분석 전략
수집된 데이터를 다음과 같은 다층적 기준으로 분석하여 종합적인 효과를 검증하고자 하였다:

(1) 항목별 분석:
● 8개 평가 영역별 점수 비교 (A1~A3, B1~B3, C1~C2)
● 질문 품질, 답변 품질, 학습 맥락 점수 독립 분석

(2) 사전 성취도 수준별 분석:
◯ Quartile 분석: 중간고사 점수 기준 4분위 (Q1 하위 25% ~ Q4 상위 25%)
● 각 구간에서 Agent vs Freepass 효과 크기 계산

(3) 종단적 학습 효과 분석:
● 복수 세션 참여 학생 대상 누적 변화 추적
● 각 학생의 첫 세션 vs 마지막 세션 점수 비교

(4) 질적 데이터 수집 및 분석:

◯ 사후 설문 조사:
실험 종료 후 학생들의 주관적 경험을 파악하기 위해 사후 설문조사를 실시한다:
● 블라인드 설계: 학생들은 자신이 Agent인지 Freepass인지 모르는 상태에서 응답
● 문항 구성:
● 리커트 척도 문항: 메타인지, 학습 효과, 시스템 만족도 등
● 서술형 문항 5개: 경험 서술, 변화, 선호도, 기억에 남는 순간, 개선 제안

◯ 질적 분석 절차:
● 서술형 응답을 Braun & Clarke(2006)의 주제 분석(Thematic Analysis) 6단계 절차로 분석
● 데이터 숙지 → 초기 코딩 → 테마 탐색 → 테마 검토 → 테마 정의 → 보고서 작성
● 귀납적 접근으로 모드별 경험 차이 분석

라) 삼각검증 전략 (Triangulation):
양적 데이터 + 설문 리커트+ 질적 데이터 세 가지 데이터를 통합하여 연구 결과의 신뢰성과 타당성 확보

◯ 통계 분석 방법:
● 독립표본 t-검정: Agent vs Freepass 집단 간 비교
● Cohen's d: 효과 크기 (small: 0.2, medium: 0.5, large: 0.8)
● Pearson 상관분석: 변인 간 관계
● 신뢰도 분석: ICC, Cronbach's Alpha
● 유의수준: α = .05 (양측검정)
4. 통제 변인
본 연구에서는 실험 처치(Agent 모드 vs Freepass 모드) 외에 학습 효과에 영향을 미칠 수 있는 변인들을 다음과 같이 통제하였다:

변인 범주
구체적 변인
통제 방법
학습 과제
과제 내용
동일한 수학적 귀납법 과제 5개 부여
과제 난이도
동일한 순서와 난이도로 제시
제출 기한
양 집단 동일한 제출 일정
교사 효과
수업 진행
동일 교사가 모든 학급 수업 진행
수업 내용
수학적 귀납법 개념 학습 내용 동일
풀이 검토
양 집단 동일한 방식으로 과제 검토
학습 자료
교과서
동일 교과서 사용 (수학Ⅰ)
기술 환경
과제 자료
동일한 수리논술 문제지 제공
접근 기기
개인 노트북 또는 태블릿 사용
네트워크
학교 Wi-Fi 환경
시스템
동일한 시스템 버전 및 UI
사전 지식
선수 학습
수열 단원 선수 학습 완료
기초 개념
1회차에 수학적 귀납법 개념 사전 교육 (양 집단 동일)
평가 방법
AI 채점
3개 AI 모델 + Ensemble 평균 (블라인드 채점)
채점 기준
동일 QAC 체크리스트 (40점 만점)
평가 시점
모든 세션 데이터 수집 후 일괄 채점
[표Ⅲ-8] 통제 변인 및 통제 방법


◯ 통제되지 않은 변인 (연구 제한점):
● AI 에이전트 시스템 활용 시간: 학생마다 수업 중/쉬는 시간 활용 정도 상이
● 세션 횟수: 학생의 자발적 선택에 따라 사용 빈도 다름 (1~13회)
● 학습 습관: 개인별 학습 전략 및 스타일 차이
● 가정 학습: 학교 밖 추가 학습 시간 및 자료 사용
● 동료 효과: 친구 간 정보 공유 및 상호작용
● 개인별 인지 능력 차이

이러한 통제되지 않은 변인들은 무작위 배정을 통해 두 집단에 균등하게 분산되도록 하였으며, 연구 결과 해석 시 한계점으로 고려되었다.
5. 연구 도구
가. 질문 품질 평가 도구
◯ QAC 체크리스트 (Question-Answer-Context Checklist)
본 연구는 학생-시스템 간 대화 세션의 질을 평가하기 위해 QAC 체크리스트를 개발하여 사용하였다:
● 구성: 3개 영역(질문, 답변, 맥락), 8개 항목, 32개 체크리스트 요소
● 배점: 40점 만점 (A영역 15점 + B영역 15점 + C영역 10점)
● 이론적 기반: Dewey의 반성적 사고 이론, 질문 생성 이론
● 평가 방식: 각 체크리스트 요소를 0(미충족) 또는 1(충족)로 이진 평가
● 평가자: AI 3개 모델(Gemini, Claude, GPT-5) + 수학교사 2명

◯ 타당도 및 신뢰도 확보 방법:
● 내용 타당도: 교육학 이론 기반 설계
● 전문가 타당도: 현직 수학교사 검토
● 평가자 간 신뢰도: 3개 AI 모델 교차 검증, ICC 및 Cronbach's α 계산
● 준거 타당도: 교사 평가와 AI 채점 간 상관분석
상세: QAC 체크리스트 전체 구조는 II-8절 참조. LLM 배치 채점 프롬프트는 부록 D 참조.

나. 학생 수준 분류 기준
◯ 중간고사 성적 활용 (수열 단원 선수 학습 수준)
본 연구는 학생들의 사전 학업 수준을 파악하고 수준별 효과를 분석하기 위해, 수학적 귀납법 단원 학습 이전에 실시된 중간고사 성적을 활용하였다. 이 중간고사는 수학적 귀납법의 선수 학습 내용인 수열 단원(등차수열, 등비수열, 여러 가지 수열의 합)에 대한 이해도를 평가한 시험이다.


◯ 성적 구성:
● 서술형 문항 (30점 만점): 수열의 합, 등차/등비수열 증명 과정 서술
● 객관식 문항 (70점 만점): 수열 개념 이해 및 적용
● 총점 (100점 만점): 서술형 + 객관식

◯ 평가 내용 (수학적 귀납법의 선수 학습):
● 등차수열의 일반항 및 합 공식
● 등비수열의 일반항 및 합 공식
● 여러 가지 수열의 합 (∑ 기호 활용)
● 수열의 귀납적 정의 (점화식)

◯ 활용 목적:
● 학생 수준 분류: 삼분위수로 하위 33%, 중위 33%, 상위 33% 구분
● 사전 동질성 검증: Agent vs Freepass 집단 간 선수 학습 수준 비교
● 조절 변수: 학업 수준별 차별적 효과 분석
● 선수 학습 지표: 수학적 귀납법 학습을 위한 기초 개념 이해도 대표


중요: 본 연구는 중간고사 성적을 사전-사후 비교에 사용한 것이 아니라, 수학적 귀납법 학습 전 선수 학습 수준을 나타내는 독립적 기준으로 활용하였다. 실제 학습 효과는 QAC 체크리스트 점수의 세션별 변화로 측정하였다.

다. 시스템 로그 데이터
세션별 대화 내용과 상호작용 패턴을 분석하기 위해 PostgreSQL 데이터베이스에서 자동 수집된 로그 데이터를 활용하였다.

◯ Agent 모드 로그:
● 명료화 대화 횟수 및 질문 개선 정도
● 에이전트별 응답 시간 및 처리 과정
● 학습자 질문 진화 추이
● 세션 단계별 전환 패턴

◯ Freepass 모드 로그:
● 즉시 답변 횟수 및 대화 길이
● 후속 질문 발생 패턴
● 메시지 유형별 분포

◯ 수집 데이터:
● 세션 메시지 수, 평균 메시지 길이
● 사용자-AI 상호작용 횟수

6. 자료 수집 및 분석
가. 정량적 분석
◯ 집단 간 비교 분석:
● 독립표본 t-검정: Agent vs Freepass 모드 간 QAC 점수 차이 검증
● Welch's t-test: 등분산 가정 위배 시 사용
● Mann-Whitney U test: 비모수 검정

◯ 세션 증가폭 분석:
● 대응표본 t-검정: 각 학생의 1회차 vs 최종회차 QAC 점수 변화
● 천장효과 보정: proportional improvement = (후반 평균 - 전반 평균) / (15 - 전반 평균)

◯ 효과 크기 계산:
● Cohen's d를 주 효과 크기 지표로 사용
● Hedge's g (소표본 보정)와 Cliff's delta (비모수 효과 크기)를 보조적으로 산출
● 평균 차이의 95% 신뢰구간은 Bootstrap 방법(재표본추출 1,000회)으로 추정
● 효과 크기 해석은 Cohen(1988)의 기준을 따름: d = 0.2 (작은 효과), 0.5 (중간 효과), 0.8 (큰 효과)
● Hattie (2009)는 교육 개입의 평균 효과크기가 d=0.4임을 제시하였으며, 이를 '힌지 포인트(hinge point)'로 명명하였다. 본 연구에서는 이를 참고하되, Cohen의 전통적 기준을 주요 해석 틀로 사용한다.

◯ 학업 수준별 차별적 효과 분석:
● 중간고사 성적 기준 사분위수(Quartile) 분류
● 각 수준별 Agent vs Freepass 효과 비교
● 상호작용 효과 검증

나. 질적 분석
◯ 대화 패턴 분석: 명료화 유형, 학생 응답 패턴, 학습 진화 추이
◯ 세션 사례 분석: 명료화 성공/실패 사례의 질적 코딩
◯ 로그 데이터 분석: 메시지 유형, 세션 단계, 상호작용 길이

IV. MAICE 교육 시스템 아키텍처
본 연구는 I장과 II장에서 확인된 문제점과 이론적 기반 위에 MAICE(Mathematical AI Chatbot for Education) 시스템을 설계·개발하였다. MAICE는 질문 명료화를 핵심으로 하는 AI 멀티 에이전트 시스템으로, Dewey의 반성적 사고와 Bloom의 지식 분류를 실제 학습 환경에 구현한 것이다.
1. 예비 연구: 문제 분석 및 설계 근거
I장에서 언급한 바와 같이, 본 연구는 MAICE 시스템 설계에 앞서 2025년 5월 실제 고등학교 수학 수업에서 385건의 학생-AI 대화를 수집하여 현재 LLM 사용 방식의 교육적 문제점을 실증적으로 분석하였다. 본 절에서는 그 상세 내용을 제시한다.
가. 예비 조사 절차
1단계: 데이터 수집 - 고등학교 수학 수업 환경에서 학생들에게 ChatGPT와 동일한 사용자 인터페이스를 제공하는 간단한 웹앱을 배포하였다. 수업 시간 동안 학생들이 자유롭게 질문하고 AI 답변을 받을 수 있도록 하여 총 385건의 실제 질문-답변 쌍을 수집하였다.

2단계: 루브릭 개발 - 수집된 데이터를 분석한 결과, 학생 질문과 AI 답변에서 반복적으로 나타나는 질적 문제 패턴들이 관찰되었다. 전통적 교육학 이론(Dewey, Bloom 등)과 최신 AI 교육 평가 연구를 종합하여 체계적인 분석적 채점 기준(analytic rubric)을 개발하였다.


구분
평가 영역
코드
평가 내용
질문 평가
수학적 전문성
A1
수학 개념의 정확성, 용어 사용
질문 구조화
A2
질문 대상·범위·초점의 명확성
학습 맥락 적용
A3
학습자 수준, 목적, 이해 상태
답변 평가
학습자 맞춤도
B1
학습자 수준 파악, 난이도 조절
설명의 체계성
B2
개념 위계, 단계별 논리 전개
학습 내용 확장성
B3
심화학습 유도, 자기주도 학습 촉진
[표Ⅳ-1] 예비조사 루브릭 6개 평가 영역

각 영역은 5점 리커트 척도로 평가 (총 30점 만점).
3단계: 교사 평가 - 중등 수학교사 4명이 독립적으로 385건을 평가하였다.
나. 예비 조사 핵심 발견

평가 영역
평균 (5점)
최저점 비율
문제
A3 학습 맥락
1.500
72.3%
가장 심각
A2 질문 구조
2.049
45.8%
심각
A1 수학 전문성
2.277
45.5%
심각
B3 학습 확장성
1.832
48.9%
가장 심각
B1 학습자 맞춤도
2.474
27.6%
중간
B2 설명 체계성
2.765
23.9%
중간
질문-답변 상관
-
-
r=0.691*
[표Ⅳ-2] 예비조사 평가 결과 (N=385)

주: ***p<0.001. 질문 품질이 높을수록 AI 답변 품질도 유의하게 향상.

핵심 발견:
◯ 학습 맥락 부재 (72.3%): 학생들이 자신의 수준·목적을 명시하지 않음
◯ 학습 확장성 결여 (48.9%): AI가 심화학습을 유도하지 못함
◯ 강한 질문-답변 상관 (r=0.691): 질문 개선이 학습 효과의 핵심

대표 사례:
◯ 불량 질문 (ID 294): "근데 어떻게 증명한거야?" → AI도 답변 불가
◯ 우수 질문 (ID 358): "소프트웨어마이스터고 2학년인데 0!이 1인 이유를..." → AI 맞춤 답변





다. MAICE 설계에의 반영

예비 조사 발견은 다음과 같이 MAICE 설계에 직접 반영되었다:

[그림Ⅳ-1] 예비조사 발견에서 MAICE 설계로의 반영



예비조사에서 발견된 4가지 핵심 문제가 MAICE 시스템의 설계 원칙으로 직접 반영되었다.
2. 설계 철학: "명료화 중심 학습"
가. 해결 아이디어: 교육 이론 기반 에이전트 시스템
본 연구는 II장에서 검토한 교육 이론을 실제 AI 시스템으로 구현하는 것을 목표로 한다:
◯ Bloom의 K1-K4 분류 → 질문 유형 자동 분류 및 맞춤형 답변 생성
● K1 (사실): 간결한 정의 중심 답변
● K2 (개념): 관계 설명 중심 답변
● K3 (절차): 단계별 안내 중심 답변
● K4 (메타인지): 메타인지 유도 중심 답변

◯ Dewey의 반성적 사고 5단계 → 명료화 프로세스 설계
● 1단계 (문제 인식): "무엇이 불확실한가요?"
● 2단계 (문제 정의): "정확히 무엇을 알고 싶은가요?"
● 3단계 (가설 설정): "어떤 방법을 시도해봤나요?"
● 4단계 (가설 검증): "논리적 연결을 어떻게 보나요?"
● 5단계 (결론 도출): "최종적으로 무엇을 얻고 싶나요?“

나. 핵심 아이디어: 질문 → 분류 → 명료화 → 답변 파이프라인
MAICE 시스템의 핵심은 학생의 질문을 즉시 답변하지 않고, 먼저 질문의 품질을 진단하고 필요시 명료화 과정을 거치는 것이다
[그림Ⅳ-2] MAICE 질문 처리 파이프라인

.
이러한 파이프라인은 단순히 정보를 전달하는 것이 아니라, 학생이 스스로 질문을 구조화하고 사고를 명료화하는 과정을 경험하도록 설계되었다.
다. 설계 대상: 일반 LLM보다 우수한 학습 효과

목표
Freepass 한계
MAICE 해결책
기대 효과
질문 품질 개선
맥락 72.3% 부재
명료화 프로세스
질문의 질 향상
답변 적합성 향상
맥락 오해, 수준 불일치
질문 유형 분류 (K1-K4)
맞춤형 답변
메타인지 향상
사고 과정 부재
Dewey 5단계 유도
자기 성찰 훈련
[표Ⅳ-3] MAICE 시스템의 설계 목표

MAICE 시스템은 위 3가지 측면에서 일반 Freepass 방식보다 우수한 학습 효과를 제공하는 것을 목표로 한다.
이러한 목표를 달성하기 위해, MAICE는 ChatGPT, Claude 등 상용 AI 대화 서비스와 유사한 UX를 제공하되, 수학 학습에 특화된 3계층 구조로 설계되었다.

3. 전체 아키텍처 개요: 3계층 구조
MAICE 시스템은 학생이 사용하는 대화 인터페이스, 대화를 관리하고 데이터를 저장하는 관리 시스템, 그리고 지능적으로 질문을 처리하는 에이전트 시스템의 3계층으로 구성된다.

가. 계층별 역할
1) 계층 1: 대화 인터페이스 (학생이 보는 화면)
역할: 학생이 수식을 쉽게 입력하고 AI 답변을 실시간으로 받을 수 있는 채팅 화면
● 주요 기능:
● 수식 입력 지원: 복잡한 수학 수식을 클릭 몇 번으로 입력 
● 실시간 답변: AI 답변이 타이핑하듯 실시간으로 표시
● 간편한 로그인: 학교 계정으로 바로 시작
● 모바일 지원: 핸드폰에서도 동일하게 사용 가능
● 학생 경험: ChatGPT, Claude와 동일한 UX로 별도 학습 없이 즉시 사용 가능

[그림Ⅳ-3] MAICE 대화 인터페이스 실제 화면



2) 계층 2: 관리 시스템 (대화 저장 및 분석)
역할: 모든 대화를 체계적으로 저장하고, 교사가 학생 학습 상황을 파악할 수 있도록 지원
◯ 주요 기능:
● 대화 기록 관리: 학생별 모든 대화 세션을 시간 순으로 저장
● 학습 데이터 분석: 학생의 학습 진도, 어려움 영역 자동 추출
● 교사 대시보드: 반 전체 학습 현황 및 개별 학생 상세 정보 제공
● 권한 관리: 학생은 본인 데이터만, 교사는 전체 데이터 접근 가능

가) 계층 3: 에이전트 시스템 (지능적 질문 처리)
역할: 학생 질문을 분석하고, 필요시 명료화하며, 맞춤형 답변을 생성하는 5개 AI 에이전트
◯ 5개 에이전트:
● Classifier (질문 분류): "이 질문은 어떤 유형인가?"
● Question Improvement (명료화): "질문을 더 명확하게"
● Answer Generator (답변 생성): "유형별 맞춤 답변"
● Observer (학습 관찰): "학생이 무엇을 배우고 있는가?"
● FreeTalker (대조군): "명료화 없이 즉시 답변“
나. 
[그림Ⅳ-4] MAICE 3계층 아키텍처
전체 시스템 구조도
다. 질문 처리 흐름
학생이 질문을 입력하면 다음과 같은 과정을 거친다:


[그림Ⅳ-5] 질문 처리 시퀀스




◯ 핵심 특징:
● 학생은 명료화 과정을 자연스러운 대화로 경험
● 모든 대화는 자동으로 저장되어 학습 분석에 활용
● 교사는 별도 대시보드에서 학생 학습 현황 확인
● 기술 구현 상세: 각 계층의 구체적인 기술 스택과 구현 방법은 4장 "시스템 구현"에서 다룬다.

4. 5개 에이전트의 역할과 협업
MAICE 시스템의 핵심은 5개의 독립적인 AI 에이전트가 협업하여 질문을 처리하는 것이다. 각 에이전트는 특정한 교육적 목적을 가지고 설계되었다.

가. Question Classifier (QC): "이 질문은 어떤 유형인가?"
1) 설계 목적
1장에서 확인했듯이, 학생 질문의 72.3%가 학습 맥락 정보 없이 제출되었다. 질문의 인지적 수준에 따라 답변 방식이 달라야 하므로(K1 사실형 vs K4 메타인지형), Classifier는 질문을 자동으로 분류하고 명료화 필요성을 판단한다.

2) 핵심 기능
가) 질문 유형 분류 (K1-K4)
● K1 (사실): "수학적 귀납법의 정의가 뭐에요?" → 간결한 정의 제공
● K2 (개념): "귀납 가정은 왜 필요한가요?" → 개념 간 관계 설명
● K3 (절차): "이 등식을 어떻게 증명하나요?" → 단계별 절차 안내
● K4 (메타인지): "제가 뭘 잘못 이해한 건가요?" → 메타인지 유도 답변



나) 명료화 필요성 판단 (3단계 게이팅)
● answerable: 질문이 명확함 → Answer Generator로 전달
● needs_clarify: 질문이 모호함 → Question Improvement로 전달
● unanswerable: 답변 불가능 → 정중한 안내

다) 명료화 질문 제안
● needs_clarify인 경우, Dewey 5단계 기반 구체적 질문 제안
● 예: "수열 알려줘" → "어떤 부분이 궁금한지 알려줄래? 

3) 
[그림Ⅳ-6] QC 3단계 게이팅
설계 흐름



나. Question Improver (QI): "질문을 더 명확하게"
1) 설계 목적
QC가 "명료화 필요" 판정을 내리면, QI는 학생이 스스로 질문을 구체화하도록 돕는다. Dewey의 반성적 사고 이론에 따르면, 학생이 자신의 어려움을 명확히 인식하고 표현하는 과정에서 메타인지 능력이 향상된다.

2) 핵심 설계: Dewey 5단계 기반 명료화
QI는 Dewey의 5단계를 대화형 질문으로 변환하여, 질문 유형(K1-K4)과 모호성 수준에 따라 1-3회 명료화를 수행한다:


Dewey 단계
명료화 질문 예시
학생 경험
1단계: 문제 인식
"무엇이 불확실한가요?"
막연한 어려움 → 구체적 문제 인식
2단계: 문제 정의
"정확히 무엇을 알고 싶은가요?"
"어려워요" → "귀납 단계가 어려워요"
3단계: 가설 설정
"어떤 방법을 시도해봤나요?"
학생의 시도와 이해 수준 파악
4단계: 가설 검증
"왜 그렇게 생각했나요?"
학생의 사고 과정 드러내기
5단계: 결론 도출
"최종적으로 무엇을 얻고 싶나요?"
학습 목표 명확화
[표Ⅳ-4] Dewey 5단계 기반 명료화 전략



3) 명료화 전략: 질문 유형별 차별화
질문 유형(K1-K4)과 모호성 수준에 따라 명료화 전략을 조절한다:
● K1 (즉답형): 선택지 제공으로 빠르게 범위 좁히기 (1회)
● K2 (설명형): 비교 대상이나 설명 깊이 확인 (1-2회)
● K3 (적용형): 구체적인 문제 상황이나 막힌 단계 파악 (1-2회)
● K4 (문제해결형): Dewey 5단계를 깊이 적용, 사고 과정 드러내기 (2-3회)


4) 명료화 완료 판단
Question Improvement는 학생의 각 응답을 평가하여, 충분한 정보가 모였는지 판단한다

[그림Ⅳ-7] 명료화 프로세스 흐름




◯ 판단 기준:
● PASS: 원본 질문의 의도가 명확해지고, 답변 생성에 필요한 정보 확보
● NEED_MORE: 원본 질문의 의도가 여전히 불분명하거나, 추가 정보 필요
● 최대 3회 제한: 3회 명료화 후에도 불충분하면 현재 정보로 답변 생성

5) 교육적 의도 명시화
기존 LLM은 "왜 명료화 질문을 하는지" 설명하지 않아 학생이 불편함을 느꼈다. Question Improvement는 명료화의 교육적 이유를 부드럽게 설명한다:

◯ 기본 프레이밍:
● 질문을 조금만 더 구체적으로 만들어주면, 딱 맞는 설명을 해드릴 수 있어요!

◯ 교육적 프레이밍 (K4 수준 학생):
● 함께 질문을 구체화해볼까요? 정확히 무엇을 모르는지 찾아가는 과정이 진짜 학습의 시작이에요!
6) 질문 유형 재분류
명료화 과정에서 학생의 실제 어려움이 드러나면, 질문 유형이 변경될 수 있다:

[그림Ⅳ-8] 질문 유형 재분류 예시





다. Answer Generator (AG): "유형별 맞춤 답변"
1) 설계 목적
1장에서 확인한 AI 답변의 문제는 "모든 학생에게 동일한 방식으로 설명"하여 인지 과부하를 유발했다. AG는 Bloom의 K1-K4 분류에 따라 답변 구조와 교수법을 차별화한다.
2) 핵심 설계: K1-K4별 답변 차별화

유형
답변 구조
교수법 특징
K1 (사실)
정의 → 핵심 예시 → 보충
간결함, 정확성 우선 (3-5문장)
K2 (개념)
개념 관계 → 비교/대조 → 시각화
"왜?" 중심 설명, 논리적 연결
K3 (절차)
전체 개요 → 단계별 안내 → 실수 방지
선택권 제공, 대화형 진행
K4 (메타인지)
문제 분석 → 자기 점검 → 대안 탐색
답 직접 제공 X, 사고 과정 유도
[표Ⅳ-5] K1-K4별 답변 구조 및 교수법

3) 주요 설계 특징
◯ 교육과정 표준 준수: 대한민국 교육과정 표준 용어 사용 
◯ 실시간 스트리밍: 답변을 타이핑하듯 실시간 전송하여 학생이 자연스러운 대화 경험
◯ LaTeX 수식 렌더링: 인라인($수식$) 및 블록($$수식$$) 수식 지원


라. Learning Observer (LO): "대화 요약 및 컨텍스트 관리"
1) 설계 목적
긴 대화 세션에서 컨텍스트 길이 증가와 맥락 손실을 방지하기 위해, Observer는 대화를 주기적으로 요약하여 핵심 내용만 유지한다.
2) 핵심 기능
◯ 대화 요약: 15회 턴 이상 시 자동 요약 (핵심 주제, 학습 진행, 질문 유형, 현재 상태)
◯ 맥락 유지: 토큰 효율성 확보 + 학습 연속성 지원


유형
답변 구조
K1 (사실)
정의 → 핵심 예시 → 보충
K2 (개념)
개념 관계 → 비교/대조 → 시각화
K3 (절차)
전체 개요 → 단계별 안내 → 실수 방지
K4 (메타인지)
문제 분석 → 자기 점검 → 대안 탐색
[표Ⅳ-6] LO 관찰 및 추출 정보


마. Free Talker (FT): "대조군 (Freepass 모드)"
1) 설계 목적
명료화 프로세스의 효과를 검증하기 위한 A/B 테스트 대조군. 일반 ChatGPT, Claude처럼 명료화 없이 즉시 답변한다.
2) 핵심 특징
◯ 즉시 답변: 질문 분류, 명료화 없이 바로 답변 생성
◯ A/B 테스트: 학생 58명을 무작위로 Agent(28명) / Freepass(30명) 배정





항목
Agent 모드
Freepass 모드
질문 분류
✓ K1-K4 분류
✗ 분류 없음
명료화
✓ needs_clarify 시 명료화
✗ 명료화 없음
답변 전략
✓ K1-K4별 차별화
✗ 동일한 방식
[표Ⅳ-7] Agent vs Freepass 모드 기능 비교


◯ 대조군의 중요성:
● Agent 모드의 효과를 인과적으로 검증 가능
● 선택 편향(selection bias) 제거
● "명료화가 정말 도움이 되는가?" 실증적 답변 제공

3) 검증 계획
FT를 활용한 A/B 테스트를 통해 다음을 비교 검증할 예정이다:
◯ 비교 차원:
● 즉시 효과: 단일 세션에서 질문-답변 품질 비교
● 누적 효과: 다회 세션 시 학습 진행 패턴 비교
● 학습자 수준별 효과: 상위권/하위권 학생에 대한 차별적 효과 분석
◯ 기대 가설:
● 명료화 프로세스는 단일 세션에서는 시간이 더 소요되나, 장기적으로 질문 능력과 메타인지 향상에 기여할 것
● 특히 절차적 지식이 부족한 하위권 학생들에게 더 큰 학습 효과가 나타날 것

검증 결과는 V장에서 상세히 분석한다.
V. 연구 결과
본 연구는 고등학교 2학년 수학적 귀납법 단원을 대상으로 질문 명료화를 지원하는 AI 에이전트 시스템(MAICE, IV장 참조)을 설계·개발하여 실제 교육 현장에 배포하였다. 3주간 A/B 테스트를 통해 284개 유효 세션을 수집하였으며, 방법론적 한계를 상호 보완하기 위해 LLM 평가(3개 모델)와 교사 평가를 병행하여 명료화 효과를 검증하였다.

1. 연구 실행 및 데이터 수집
가. 시스템 배포
MAICE 시스템을 Docker 기반으로 구축하여 실제 고등학교 환경에 성공적으로 배포하였다.
◯ 배포 환경:
● 기간: 2025년 10월 20일 ~ 11월 8일 (3주)
● 대상: 고등학교 2학년 58명 (Agent 28명, Freepass 30명)
● 플랫폼: Docker Compose 기반 웹 애플리케이션
● LLM: Gemini 2.5 Flash (Google)
● 시스템 가동률: 99.2%

나. 데이터 수집 현황


구분
Agent
Freepass
전체
세션 수
115
169
284
학생 수
28
30
58
1인당 평균
4.1
5.6
4.9
[표Ⅴ-1] 수집 데이터 현황



다. 사전 동질성 검증
실험 처치 효과의 내적 타당도를 확보하기 위해, 두 집단 간 사전 중간고사 성적을 독립표본 t-검정으로 비교하였다(표Ⅵ-2 참조).

결과: 모든 변인에서 p > .05로 두 집단 간 통계적으로 유의한 차이가 없어, 사전 동질성이 확보되었다(총점: t=1.18, p=.242; 서술형: t=0.74, p=.462; 객관식: t=1.30, p=.199). 이는 실험 처치 효과의 내적 타당도를 보장하는 중요한 근거가 된다.

라. 명료화 프로세스 작동 확인
Agent 모드 118개 세션 중 98개(83.1%)에서 명료화 질문이 수행되었다.

구분
세션 수
비율
평균 메시지 수
명료화 수행
98
83.1%
9.8개
명료화 미수행
20
16.9%
4.1개
[표Ⅴ-2] 명료화 수행 현황

명료화가 수행된 세션은 평균 9.8개의 메시지로 구성되어, 미수행 세션(4.1개)보다 2.4배 많은 상호작용이 발생하였다.

2. 명료화 효과: LLM-교사 이중 평가
가. 이중 평가 설계의 논리
본 연구는 평가 방법의 한계를 상호 보완하기 위해 LLM 평가와 교사 평가를 병행하였다.


평가 방법
역할
표본
평가자
강점
한계
LLM 평가
패턴 탐색
N=284
3개 모델
대규모, 객관적
교육적 타당성 확인 필요
교사 평가
타당성 검증
N=100
2명
골드 스탠다드
표본 작아 재현 필요
상호 검증
신뢰성 확보
-
-
서로 약점 보완
r=0.754*
[표Ⅴ-3] LLM-교사 이중 평가 설계

◯ 평가 전략:
● LLM으로 전체 284개 세션에서 효과 패턴 탐색
● 교사가 100개 세션에서 교육적 타당성 검증
● 두 평가의 일치도 확인하여 상호 검증
나. LLM 평가 결과 (N=284)
1) 평가 도구 및 신뢰도
◯ QAC(Question-Answer-Context) 체크리스트 (40점 만점):
● 8개 항목 (A1-A3 질문, B1-B3 응답, C1-C2 맥락)
● 32개 체크리스트 요소 (항목당 4개, 0/1 판단)
● 충족 개수에 따라 1~5점 자동 산정
● 평가 완료 후 교사들로부터 사용 소감 수집 (후술 2.다.(4))
◯ 평가자: 3개 독립 AI 모델
● Gemini 2.5 Flash (Google): 284개 세션
● Claude 4.5 Haiku (Anthropic): 284개 세션
● GPT-5 mini (OpenAI): 284개 세션
◯ 신뢰도 (284개 공통 세션):
● Cronbach's α = 0.868 (우수한 내적 일관성)
● ICC(2,k) = 0.848* (p<0.001)**: 3개 모델 간 평가자 일치도, 우수한 수준 (>0.75)

모델 쌍
Pearson r
해석
Anthropic ↔ OpenAI
0.826*
매우 높음
Gemini ↔ Anthropic
0.707***
높음
Gemini ↔ OpenAI
0.622***
중간-높음
평균
0.718*
높음
[표Ⅴ-4] LLM 모델 간 쌍별 상관계수 (N=284)

주: ***p<0.001. 전체 점수(40점 만점) 기준. 3개 모델 간 높은 일치도로 평가의 객관성 확보. 이후 모든 LLM 평가 결과는 3개 모델 평균값.
2) 전체 모드 효과

항목
Agent
Freepass
차이
t
p
d
C2 학습 지원
2.32
2.05
+0.28
2.86
0.004*
0.353
A3 학습 맥락
1.26
1.47
-0.21
-3.41
0.001*
-0.425
B3 학습 확장성
1.97
1.74
+0.22
2.05
0.041*
0.245
A1 수학 전문성
3.80
3.70
+0.10
0.98
0.330
0.120
A2 질문 구조화
4.50
4.57
-0.06
-0.66
0.511
-0.078
B1 학습자 맞춤도
3.66
3.52
+0.14
1.19
0.233
0.145
B2 설명 체계성
4.56
4.61
-0.05
-0.40
0.692
-0.047
C1 대화 일관성
4.41
4.47
-0.06
-0.64
0.524
-0.076
[표Ⅴ-5] 세부 항목별 모드 비교 (LLM 평가, N=280)

주: *p<0.05, **p<0.01. 
LLM 3개 모델(Gemini 2.5 Flash, Claude 4.5 Haiku, GPT-5 mini) 평균값.

◯ 핵심 발견:
● C2(학습 지원): Agent 우수 (p=0.001, d=0.415)
   - 사고 과정 유도, 이해도 확인에서 강점
● A3(학습 맥락): Freepass 우수 (p=0.001, d=-0.393)
   - 학습 목표, 수준 반영
● B3(학습 확장성): Agent 우수 (p=0.041, d=0.245) 
   - 심화 학습 방향 제시

명료화 모드는 학습 과정 지원(C2)과 학습 확장성(B3)에서 유의한 강점을 가지나, 학습 맥락 파악(A3)에서는 즉시 답변 모드가 우수하였다.




3) 성적 수준별 차별적 효과
가) 중간고사 성적 기준 Quartile별로 C2(학습 지원) 효과를 분석하였다.

Quartile
n
Agent
Freepass
차이
p
Cohen's d
Q1 (하위)
85
2.24
1.73
+0.51
<0.001*
0.855
Q2 (중하위)
61
2.30
2.25
+0.05
0.839
0.052
Q3 (중상위)
52
2.31
2.01
+0.30
0.238
0.369
Q4 (상위)
86
2.42
2.29
+0.13
0.471
0.160
[표Ⅴ-6] Quartile별 C2(학습 지원) 비교 (LLM 평가)

주: ***p<0.001, LLM 3개 모델(Gemini, Claude, GPT-5) 평균값
◯ 핵심 발견: Q1 하위권에서 통계적으로 매우 유의하며 큰 효과 크기 (p<0.001, d=0.855). 명료화 프로세스는 학습에 어려움을 겪는 학생에게 특히 효과적.

나) 전체 점수 기준:

Quartile (n)
Agent
Freepass
차이
p
d
Q1 (85)
26.52
24.26
+2.26
0.032*
0.499
Q2 (61)
27.08
27.07
+0.01
0.989
0.004
Q3 (52)
25.50
26.88
-1.38
0.346
-0.251
Q4 (86)
26.30
27.00
-0.70
0.531
-0.133
[표Ⅴ-7] Quartile별 전체 점수 (LLM 평가)

주: *p<0.05, 40점 만점, LLM 3개 모델 평균값

하위권 학생은 명료화 모드에서 2.03점 더 높은 평가 (40점 만점 중 5.1% 차이, p=0.038). Q2, Q3, Q4는 통계적으로 유의한 차이 없음.
4) 반복 사용 효과

항목
Agent 첫
Agent 마지막
Agent 변화
Agent p
Freepass 첫
Freepass 마지막
Freepass 변화
Freepass p
A1 수학전문성
3.49
4.06
+0.57
0.006**
3.49
3.85
+0.36
0.180
A2 질문구조화
4.23
4.94
+0.71
0.003**
4.21
4.74
+0.53
0.023*
A3 학습맥락
1.19
1.22
+0.03
0.799
1.36
1.48
+0.12
0.454
B1 학습자맞춤
3.17
4.10
+0.93
0.001***
3.23
3.60
+0.37
0.106
B2 설명체계성
4.07
5.00
+0.93
0.015*
4.48
4.81
+0.33
0.250
B3 학습확장성
2.32
1.78
-0.54
0.074
1.88
1.57
-0.31
0.139
C1 대화일관성
4.13
4.77
+0.64
0.010**
4.20
4.62
+0.42
0.031*
C2 학습지원
2.14
2.33
+0.19
0.396
2.02
2.00
-0.02
0.911
전체
24.75
28.20
+3.45
0.016*
24.88
26.68
+1.80
0.154
[표Ⅴ-8] 세션 증가에 따른 C2 점수 변화 (LLM 평가)

주: 복수 세션 참여 학생 (Agent n=23, Freepass n=27), paired t-test, *p<0.05, **p<0.01
명료화 모드는 반복 사용 시 점수가 증가하는 반면, 즉시 답변 모드는 감소하여 대조적 패턴을 보임.


[그림Ⅴ-1] 세션 순서에 따른 항목별 점수 변화 추이 

주: 오차막대는 표준오차(SE)를 나타냄. Agent 모드(파란색)는 대부분의 항목에서 세션이 증가할수록 점수가 상승하는 반면, Freepass 모드(주황색)는 변화가 적거나 없음. 특히 B1(학습자 맞춤도), B2(설명 체계성), A2(질문 구조화), C1(대화 일관성)에서 Agent의 뚜렷한 상승 추세 관찰.
명료화 모드는 9개 항목 중 6개(66.7%)에서 유의한 변화를 보였다: B1 학습자 맞춤도(+0.93, p=0.001), B2 설명 체계성(+0.93, p=0.015), A2 질문 구조화(+0.71, p=0.003), C1 대화 일관성(+0.64, p=0.010), A1 수학 전문성(+0.57, p=0.006), 전체 점수(+3.45, p=0.016). 반면 C2는 유의하지 않았다(+0.19, p=0.396).
즉시 답변 모드는 9개 항목 중 2개(22.2%)에서만 유의하였다: A2(+0.53, p=0.023), C1(+0.42, p=0.031). 전체 점수는 1.80점 증가하였으나 유의하지 않았으며(p=0.154), C2는 거의 변화가 없었다(-0.02, p=0.911).

그림Ⅴ-1에서 Agent 모드는 대부분의 항목에서 세션이 증가할수록 지속적 우상향 추세를 보인다. 특히 B1(3.2→4.1), B2(4.1→5.0), A2(4.2→4.9), 전체(24.8→28.2)는 명확한 선형 증가 패턴을 보인다. 반면 Freepass 모드는 대부분 평평하거나 불규칙한 변동을 보이며, C2는 모든 세션에서 약 2.0점을 유지한다.

5) LLM 평가 소결
◯ 발견된 패턴 (N=284):
● C2(학습 지원)에서 명료화 우수 (전체: p=0.004, d=0.353; Q1: p<0.001, d=0.855)
● Q1 하위권에서 큰 효과 (C2: +0.51, 전체: +2.26, p<0.05)
● B3(학습 확장성)에서도 유의한 차이 (p=0.041, d=0.245)

◯ 한계: AI가 AI를 평가 → 교육적 타당성 확인 필요
● 순환 논리 우려: AI가 AI를 평가 → 교육적 타당성 확인 필요 
● 학습 맥락 파악: A3 항목에서 Freepass 우수 (p=0.001, d=-0.425) → 명료화 질문 과정에서 학습자 정보(학년, 수준, 목표) 수집이 부족할 가능성
다. 교사 평가 (N=100)
1) 평가 설계
연구 객관성 확보를 위해 연구자를 제외하고, 외부 수학 교사 2명이 100개 세션을 독립 평가하였다.

구분
내용
평가자
외부 수학 교사 2명 (평가자 96, 97)
평가 세션
100개 (Agent 50, Freepass 50)
평가 방식
동일 세션 독립 평가 (완전한 대응 설계)
평가 도구
QAC 체크리스트 (LLM과 동일)
총 레코드
200개 (100×2)
표집 방법
계층적 목적 표집 (Stratified Purposive Sampling)
[표Ⅴ-9] 교사 평가 설계

100개 세션 선별: 계층적 목적 표집 (Stratified Purposive Sampling)

LLM이 평가한 284개 세션 중 교사 검증용 100개를 다음 4가지 전략으로 선별하였다:
◯ 전략 1. AI 모델 간 불일치도 기반 (20개)
● 검증 목적: 3개 모델 간 채점 차이가 큰 세션의 정답 기준 확립, 평가자 간 신뢰도(Inter-rater Reliability) 검증
● 선별 방법: Gemini, Claude, GPT-5 총점의 표준편차 계산 → 상위 30개 중 Agent/Freepass 균형 유지하며 우선 선택
● 결과 (100개 전체 기준): 평균 불일치도 2.47점(40점 만점 대비 6.2%), 최대 9.67점(24%), 불일치도 >5점 세션 4개

◯ 전략 2. 성적 구간별 계층 표집 (64개)
● 검증 목적 학습자 수준별 AI 채점 정확도 편향 검증 (하위권/상위권 공정성 확인)
● 선별 방법: 중간고사 총점으로 Quartile 분류(Q1~Q4) → 각 Quartile × Mode 조합에서 균등 표집 시도
● 결과: Q1(하위) 26개, Q2 26개, Q3 24개, Q4(상위) 24개 - 전 성적대 거의 균등 분포
◯ 전략 3. 루브릭 패턴 특이 케이스 (10개)
● 검증 목적: 루브릭 항목별 편향성 검증 (특정 항목만 극단적 점수인 경우의 타당성 확인)
● 선별 방법: 8개 항목(A1~C2) 점수의 표준편차 계산 → 패턴 분산이 큰 세션 우선 선별
● 결과: Agent/Freepass 균형 유지

◯ 전략 4. 세션 길이 다양성 (6개)
● 검증 목적: 대화 길이에 따른 AI 채점 일관성 검증
   - 짧은 세션: 정보 부족 → 과소평가 가능성
   - 긴 세션: 맥락 추적 오류 가능성
● 선별 방법: 짧은(≤5턴), 중간(6-15턴), 긴(>15턴) 세션 포함
● 결과: 실제 분포 반영 (짧은 64%, 중간 30%, 긴 6%)

최종 균형 조정: 4가지 전략을 우선순위 순으로 적용한 후, Agent/Freepass 모드 균형을 위해 세션 일부 교체 → 최종 Agent 50개, Freepass 50개 (50:50) 완벽한 균형 달성

검증 항목
전체 집단 (N=284)
표본 (N=100)
검증 결과
모드별 균형
Agent 115 (40.5%)
Freepass 169 (59.5%)
Agent 50 (50%)
Freepass 50 (50%)
의도적 균형 표집
중간고사 평균
52.4점 (SD=15.2)
53.1점 (SD=14.8)
t=0.31, p=0.758
유의한 차이 없음
Quartile 분포
Q1~Q4 각 25%
Q1 26%, Q2 26%
Q3 24%, Q4 24%
균등 분포
세션 길이
짧은 68%
중간 27%
긴 5%
짧은 64%
중간 30%
긴 6%
실제 분포 반영
[표Ⅴ-10] 표본 100개의 대표성 검증

주: 중간고사 점수는 100점 만점 기준. 모드별 균형은 표집 전략의 목적상 의도적으로 50:50으로 조정.


측정 방법
전체 점수
질문 영역
응답 영역
맥락 영역
ICC(2,k)
0.707*
0.719*
0.655*
0.285**
Pearson r
0.644***
0.578***
0.573***
0.392***
Spearman ρ
0.571***
0.629***
0.466***
0.417***
일치도 수준
양호-높음
양호-높음
양호
낮음-중간
[표Ⅴ-11] 교사 평가자 간 신뢰도 (N=100)

주: ***p<0.001. 외부 교사 2명(A, B) 독립 평가 결과. ICC(2,k)는 평균 측정값 기준 급내상관계수. 평균은 질문/응답/맥락 3개 영역의 산술평균.

해석:
● ICC(2,k) = 0.707* (p<0.001)로 양호한 신뢰도** (Koo & Li, 2016 기준: >0.60 양호)
● 전체 점수 기준 Pearson r=0.644 (p<0.001), Spearman ρ=0.571 (p<0.001)로 중간-높은 일치도
● 모든 영역에서 통계적으로 매우 유의한 상관관계
● QAC 체크리스트가 일관된 평가 도구로 기능함을 확인

2) 전체 모드 효과

영역
Agent (n=50)
Freepass (n=50)
차이
t
p
d
전체
21.73 (5.86)
19.48 (7.00)
+2.25
1.76
0.085
0.349
질문
8.02 (2.52)
7.54 (2.90)
+0.48
0.89
0.379
0.177
응답
8.50 (2.49)
7.22 (2.75)
+1.28
2.47
0.017*
0.488
맥락
5.21 (1.67)
4.72 (1.88)
+0.49
1.38
0.172
0.275
[표Ⅴ-12] 모드별 점수 비교 (교사 평가, N=100)

주: 평균(표준편차). *p<0.05
교사 평가에서 명료화 모드가 전체적으로 높은 경향을 보였으나 통계적으로 유의하지 않았으며 (p=0.085), 응답 영역에서만 유의한 차이 (p=0.017, d=0.488).
3) 하위권 효과 (교사 평가)

Quartile (n)
Agent
Freepass
차이
p
d
Q1 (29)
20.79 (6.52)
14.47 (6.22)
+6.32
0.013*
0.993
Q2 (23)
22.12 (5.20)
21.80 (5.60)
+0.32
0.890
0.059
Q3 (22)
21.89 (5.66)
20.58 (6.94)
+1.31
0.644
0.203
Q4 (26)
22.21 (6.40)
22.62 (6.29)
-0.41
0.871
-0.065
[표Ⅴ-13] Quartile별 전체 점수 (교사 평가, N=100)

주: 평균(표준편차). *p<0.05
◯ 핵심 발견: Q1 하위권에서 유의한 효과 (p=0.013, d=0.993). LLM 평가 결과(p=0.038, d=0.471)와 방향성 및 유의성 일치. Q2, Q3, Q4는 통계적으로 유의하지 않음.
◯ 한계: Q1 표본 작음 (n=29) → 해석 신중 필요


4) 연구의 제한점

본 연구의 교사 평가는 다음과 같은 제한점을 가진다:

◯ 소규모 예비 연구: 평가자 2명, 표본 100개 → 본격적 연구를 위한 예비 연구 수준
◯ 일반화 제약: 계층적 목적 표집(Stratified Purposive Sampling)으로 인해 무작위 표본이 아님 → 전체 모집단으로의 직접적 일반화 제한
◯ 향후 필요성: 더 많은 평가자, 더 큰 표본, 무작위 표집을 통한 후속 연구 필요
라. LLM-교사 평가 일치도
1) 전체 점수 상관관계

비교
Pearson r
p-value
Spearman ρ
해석
3모델 평균 ↔ 교사 평균
0.754*
<0.001
0.622***
강한 상관
[표Ⅴ-14] LLM-교사 평가 상관관계 (N=100)

주: *p<0.001, p<0.01. N=100. 전체 점수(40점 만점) 기준.
LLM 3개 모델(Gemini 2.5 Flash, Claude 4.5 Haiku, GPT-5-mini) 평균과 교사 2명 평균 간 상관계수는 r=0.754* (p<0.001)로 강한 양의 상관관계**를 보였다. 이는 LLM 평가가 전문가(교사) 평가와 높은 일치도를 가지며, 대규모 평가 도구로서의 타당성을 확보하였음을 의미한다.

항목
LLM 3모델
교사 2명
차이
r
비고
A1 수학전문성
3.63 (0.86)
3.00 (1.07)
+0.64
0.672***
5점
A2 질문구조화
4.35 (0.90)
3.27 (1.36)
+1.08
0.581***
5점
A3 학습맥락
1.40 (0.52)
1.52 (0.56)
-0.12
0.529***
5점
B1 학습자맞춤
3.44 (1.01)
3.18 (1.20)
+0.26
0.699***
5점
B2 설명체계성
4.46 (1.14)
3.13 (1.18)
+1.33
0.674***
5점
B3 학습확장성
2.00 (0.98)
1.55 (0.66)
+0.45
0.455***
5점
C1 대화일관성
4.28 (0.89)
3.18 (1.23)
+1.10
0.514***
5점
C2 학습지원
2.25 (0.82)
1.78 (0.82)
+0.47
0.420***
5점
전체
25.81 (5.09)
20.60 (6.52)
+5.20
0.706*
40점
[표Ⅴ-15] LLM-교사 평가 항목별 점수 비교 (N=100)

해석: LLM이 전체적으로 교사보다 5.20점 높게 평가(과대평가 경향). 특히 B2 설명 체계성(+1.33), C1 대화 일관성(+1.10), A2 질문 구조화(+1.08)에서 큰 차이. 항목별 상관계수는 B1 학습자 맞춤(r=0.699)이 가장 높고, C2 학습 지원(r=0.420)이 가장 낮음. A3 학습 맥락은 유일하게 교사가 더 높게 평가(-0.12).

중분류
LLM 평균
교사 평균
차이
Pearson r
해석
A. 질문 (/15)
9.54 (1.90)
7.78 (2.71)
+1.76
0.682*
강한 상관
B. 답변 (/15)
9.93 (2.73)
7.86 (2.69)
+2.07
0.735*
강한 상관
C. 맥락 (/10)
6.60 (1.40)
4.96 (1.79)
+1.63
0.653*
강한 상관
전체 (/40)
26.06 (5.40)
20.61 (6.52)
+5.46
0.754*
강한 상관
[표Ⅴ-16] LLM-교사 평가 중분류별 상관관계 (N=100)

주: 평균(표준편차). ***p<0.001. LLM은 Gemini, Claude, GPT-5 평균, 교사는 A, B 평균. N=100 공통 세션.
중분류별로도 모두 강한 양의 상관관계를 보였다. 특히 B. 답변 영역에서 가장 높은 상관(r=0.735*)을, C. 맥락 영역에서 가장 낮은 상관(r=0.653*)을 보였다. LLM은 모든 영역에서 교사보다 높게 평가하는 경향을 보였으며, 특히 답변 영역에서 과대평가 경향(+2.07점)이 가장 컸다.

2) Q1 하위권 효과의 수렴

평가자
Agent
Freepass
차이
일치도
교사(A, B 평균)
20.79
14.47
+6.32
기준
LLM(3모델 평균)
25.24
22.60
+2.64
방향 일치
[표Ⅴ-17] Q1(하위권) Agent 우위 폭 비교

주: 교사 N=29 (Q1), LLM N=29 (Q1, 교사 평가 100개와 겹치는 세션)
◯ 핵심 발견:
● 모든 평가자가 Q1에서 Agent 우위 방향성 일치
● 교사가 더 큰 효과 감지 (+6.32 vs +2.64)
● LLM 평가 패턴의 교육적 타당성 확인
마. 상호 검증된 핵심 발견
LLM 평가와 교사 평가의 일치 분석 결과, 다음의 핵심 발견이 상호 검증되었다.

핵심 발견
LLM (N=284)
교사 (N=100)
일치도
전체 Agent 우위
경향성
+2.25점
방향 일치
하위권(Q1) 효과
+2.26점 (d=0.499)
+6.32점 (d=0.993)
방향 일치
학습 지원(C2)
+0.28점 (p=0.004)
응답 유의 (p=0.017)
영역 일치
상관계수
-
r=0.754*
강한 상관
[표Ⅴ-18] LLM-교사 평가 수렴 요약

상호 검증의 의미:
◯ LLM → 교사 검증:
● LLM이 발견한 패턴 (C2 효과, Q1 큰 효과)
● 교사 평가에서도 동일 패턴 관찰
◯ 교사 → LLM 확장:
● 교사가 100개에서 발견한 효과
● LLM이 284개에서 재현
◯ 상호 보완:
● LLM의 순환 논리 우려 → 교사가 검증
● 교사의 표본 부족 → LLM이 확장
명료화 프로세스는 학습 지원을 향상시키며(LLM p=0.001), 특히 학습에 어려움을 겪는 하위권 학생에게 교육적 효과를 보인다(LLM d=0.471-0.840, 교사 d=0.993 방향 일치). LLM 평가와 교사 평가는 강한 상관관계(r=0.754***)를 보여 평가 도구의 타당성이 확보되었다.

3. 학습자 자기 평가 및 증거의 수렴
가. 학습자 자기 평가 (N=40)
본 실험 후 수행한 사후 설문조사에서 학생들의 학습 효과 자기 평가를 수집하였다. 설문은 20개 문항(리커트 5점 척도 15문항, 개방형 5문항)으로 구성되었으며, 학습 효과와 시스템 만족도를 측정하였다.

카테고리
문항 수
평균
SD
주요 문항 및 점수
B. AI 상호작용 품질
5
4.38
0.45
뭘 모르는지 알게 됨(4.38), AI 도움 충분(4.53), 
다음 질문 알게 됨(4.38)
C. 질문 능력
4
4.13
0.57
분명하게 말함(4.23), 상황 설명(4.10)
D. 개념 이해
3
4.36
0.52
귀납 가정 이해(4.58), 귀납법 구조(4.48)
E. 시스템 만족도
3
4.63
0.41
사용 쉬움(4.85), 도움됨(4.55), 계속 사용(4.48)
[표Ⅴ-19] 학습자 자기 평가 결과 (N=40)

주: 5점 리커트 척도 (1=전혀 그렇지 않다, 5=매우 그렇다). 설문지 전문은 부록 B 참조.
◯ 해석:
● 학생들은 AI 상호작용 품질(4.38), 개념 이해(4.36), 질문 능력(4.13) 모두에서 높은 자기 평가
● 특히 "뭘 모르는지 알게 됨"(4.38), "다음 질문 알게 됨"(4.38)은 메타인지 발달을 직접 체감
● 시스템 만족도(4.63)가 가장 높아 사용성과 학습 효과 모두 긍정적
● 귀납 가정 이해(4.58)가 최고점으로 수학적 귀납법 학습 목표 달성

나. 모드 선호도 및 이유
본 연구는 서술형 응답 17명과 전체 세션 데이터(346개 세션, 1,689개 메시지)를 Braun & Clarke(2006)의 주제 분석 6단계로 분석하였다:

◯ 데이터 숙지 (Familiarizing with the data): 전체 세션 데이터를 반복적으로 읽으며 Agent 모드와 Freepass 모드의 대화 구조 비교

◯ 초기 코딩 (Generating initial codes): '사고력', '효율성', '이해', '명료화 패턴', '질문 유형', '메타인지 마커' 등 의미 단위 추출

◯ 테마 탐색 (Searching for themes): 관련 코드를 '학습 효과', '효율성', '명료화 효과', '질문 진화' 테마로 그룹화
◯ 테마 검토 (Reviewing themes): 테마와 원 데이터 일치도 확인, Agent vs Freepass 모드 간 양적 차이로 테마 타당성 검증

◯ 테마 정의 (Defining and naming themes): '사고력 향상', '깊은 이해', '즉시성 선호', '효율성' 등으로 명명

◯ 보고서 작성 (Producing the report): 표Ⅴ-19와 학생 응답 예시로 제시


선호 방식
응답
비율
주요 테마 (Braun & Clarke 분석)
B 방식 (질문 유도형)
24명
68.6%
테마1: 사고력 향상(42%)테마2: 깊은 이해(25%)테마3: 자기주도(17%)
A 방식 (즉시 답변형)
11명
31.4%
테마4: 즉시성(44%)테마5: 효율성(31%)
[표Ⅴ-20] 명료화 방식 선호도 (N=35, 유효 응답)

주: 전체 40명 중 35명이 명확한 선호도 표시 (5명 불명확 제외)


서술형 응답 (설문 Part 3: 선호 방식 + 이유):
◯ B 방식 선호 이유 (n=24, 대표 사례):
● "AI가 질문을 함으로써 본인이 모르는 부분을 생각해보는 시간을 가질 수 있다" (학생 ID 40)
● "생각하는 힘이 길러진다. 사고력이 부족하면 도태되기 때문에" (학생 ID 23)
● "내 머릿속에 남는 학습이 된다. 이해가 깊어진다" (학생 ID 15)
● "질문/답변을 여러 번 다듬으며 사고가 정교화되었다" (학생 ID 32)

◯ A 방식 선호 이유 (n=11, 대표 사례):
● "내가 원하는 건 답이다. 고민해도 답이 안 나와서 물어보는 거" (학생 ID 18)
● "바로바로 답을 알려줘서 좋다. 시간을 절약할 수 있다" (학생 ID 27)
● "빠른 답변이 효율적이다. AI는 최후의 수단" (학생 ID 09)

◯ 질적 분석 결과:
● 과반수(68.6%)가 Agent 선호: "사고력 향상"(42%), "깊은 이해"(25%), "자기주도"(17%)
● 소수(31.4%)는 Freepass 선호: "효율성"(44%), "즉시성"(31%)

◯ 학습자 변화 인식 (설문 Part 2: 학습 방식 변화):
● "질문의 질이 처음에는 뭉툭했는데, 이제는 명확하게 표현하면 더 좋은 답변이 온다는 걸 깨달았다" (학생 ID 09)
● "질문 방식이 구체적으로 바뀌었다" (학생 ID 40)
● "모호한 질문을 더 구체적으로 바꾸는 방법을 배웠다" (평균 4.23/5.0)

◯ 한계: 교사-학생 관계로 인한 사회적 바람직성 편향 가능성. 따라서 주요 주장은 객관적 평가(LLM·교사)에 기반하고, 학생 자기 평가는 보조 증거로 활용.

다. 수렴적 증거: 다중 관점의 일치

증거 유형
방법
표본
핵심 발견
효과 크기
객관적 평가
LLM (QAC)
N=284
Agent 우수C2: +0.28점, p=0.004B3: +0.22점, p=0.041
C2: d=0.353*B3: d=0.245*ICC=0.848***
전문가 평가
교사 (QAC)
N=100
Agent 경향응답: +1.28점, p=0.017
d=0.488*ICC=0.707***
상호 검증
LLM↔교사
N=100
강한 일치
r=0.754*
학습자 평가
설문 (자기평가)
N=40
AI 상호작용 4.38개념 이해 4.36질문 능력 4.13
평균 4.32/5.0(높은 만족도)
[표Ⅴ-21] 네 가지 독립 증거의 수렴


주: *p<0.001, p<0.01, *p<0.05. d는 Cohen's 효과 크기, ICC는 평가자 간 급내상관계수, r은 LLM-교사 평가 상관. LLM은 3개 모델(Gemini 2.5 Flash, Claude 4.5 Haiku, GPT-5-mini) 평균 기준. 학생 설문 전문은 부록 C 참조.

평가 방법
Agent
Freepass
차이
p
d
일치도
LLM 평가
26.52
24.26
+2.26
0.032*
0.499
방향 일치
교사 평가
20.79
14.47
+6.32
0.013*
0.993
더 큰 효과 
[표Ⅴ-22] 하위권(Q1) 효과 비교 (LLM vs 교사)

주: *p<0.05, **p<0.01. LLM은 3개 모델 평균(40점 만점), 교사는 2명 평균.

수렴 패턴:
◯ 정량적 수렴:
● LLM과 교사 평가 모두 Agent 모드 우수 (r=0.627~0.670)
● 학생 자기 평가에서도 높은 학습 효과 체감 (4.13~4.38/5.0)
◯ 정성적 수렴:
● 학생 서술형 응답에서 "사고력 향상"(42%), "깊은 이해"(25%), "오래 남음"(25%) 반복 언급
● 68.6%가 질문 유도형 방식 선호 (이유: 학습 효과)
◯ 하위권 효과 수렴:
● 객관적 평가(LLM d=0.471, 교사 d=0.993) 모두 하위권 학생에게 더 큰 효과
● 학생 응답에서도 "혼자 풀 수 있게 됨"(4.23) 높은 점수
◯ 메타인지 발달 수렴:
● 객관적 평가: C2(학습 지원) +1.55점, p=0.002
● 학습자 평가: "뭘 모르는지 알게 됨" 4.38/5.0
● 질적 증거: "질문 방식이 구체적으로 바뀌었다" (ID 40)

종합 해석:
◯ 객관적 측정(LLM·교사 QAC)과 주관적 체감(학생 자기 평가, 설문 부록 C)이 일치
◯ 양적 증거(QAC 점수, 설문 점수)와 질적 증거(서술형 응답)가 같은 방향 지지
◯ 명료화 모드가 대화 품질뿐 아니라 학습자가 체감하는 실제 학습 효과도 향상
◯ 특히 하위권 학생에 대한 효과가 모든 증거에서 일관되게 확인
◯ 학생들 스스로 "사고력", "메타인지", "깊은 학습"의 가치를 인식



4. 피드백 내용의 질적 분석: Bloom-Dewey 이론 실증
본 절에서는 2절의 정량적 발견을 질적으로 심화하기 위해, 실제 학생-MAICE 대화 로그 1,589건을 분석하였다. 특히 LLM 평가점수와 연계하여 '왜 점수가 높은가/낮은가'를 Bloom 교육 목표 분류와 Dewey 반성적 사고 이론으로 해석하였다.
가. 분석 방법론
1) 데이터 출처 및 규모
◯ DB 대화 로그 (maice_agent 데이터베이스):
● 총 프롬프트-응답 로그: 1,589건
● 분석 기간: 2025-10-27 ~ 2025-11-11 (16일)
● 고유 세션: 229개
● 에이전트별 분류:
      - answer_generator_llm: 237건 (교육적 답변 생성)
      - classifier_llm: 278건 (질문 분류 K1~K4)
      - observer_llm: 255건 (학습 과정 요약)
      - freetalker_llm: 628건 (자유 대화)

통합 데이터셋:
◯ LLM 평가점수: 283개 세션
◯ 세션 ID 매칭: 9개 샘플 세션
◯ 점수 구간별: 우수(30-34점) 4개, 중간(20-29점) 3개, 하위(<20점) 2개

2) 분석 틀

가) Bloom (1956) 교육 목표 분류: 
Remember → Understand → Apply → Analyze → Evaluate → Create

나) Dewey (1933) 반성적 사고 5단계: 
문제 인식 → 문제 정의 → 가설 형성 → 추론 전개 → 검증

3) 코딩 절차
◯ Bloom 단계 코딩 (237건 답변)
◯ Dewey 단계 추적 (대화 흐름)
◯ 점수 연계 분석








나. Bloom 교육 목표 단계별 실증 사례

1) 평가(Evaluate) 단계 도달: 하위권 학생의 고차원 사고 촉진

항목
값
LLM 평가점수
31.67점 (Gemini: 30, Claude: 34, GPT-5: 31) - 284개 중 12등, 상위 4.2%
성적 분위
Q1 (하위 25%, Quartile 기준)
대화 턴수
3턴 (총 6개 메시지)
Bloom 진행
Understand → Apply → Analyze → Evaluate (4단계 상승)
Dewey 완성도
5/5 단계 (100%)
[표Ⅴ-23] 대화1 (세션 414) 평가 정보



턴
발화자
대화 내용
Bloom 단계
교육적 기능
1-2
학생
"일 때의 과정에서 이 되는게 맞음?"
-
막연한 질문

MAICE
"수학적 귀납법으로 명제를 증명할 때는 기저 단계와 귀납 단계를 거치게 됩니다. 을 증명하려면 부터 시작해야 합니다..."
Understand
전체 구조 설명
3
학생
"서술형 평가 느낌으로 더 자세히 설명해줘"
-
학습 깊이 요구

MAICE
[명료화] "서술형 평가처럼 '왜 그렇게 되는지'를 좀 더 자세히 설명해달라는 말씀이신가요?"
-
의도 명확화
4
MAICE
"귀납 단계에서 보이고 싶은 것은 입니다. 이것을 증명하는 데 두 가지 방법이 있습니다:

방법 1: 로 전개...
방법 2: 귀납 가정에서 양변에 을 더하면..."
Apply
증명 절차 제시
5-6
MAICE
"방법 1과 방법 2를 비교하면, 각 방법의 논리적 흐름은..."
Analyze
방법 비교 분석
7-9
MAICE
"직접적으로 과 의 관계를 보일 때는 방법 1이 좀 더 일반적입니다. 왜냐하면 최종 목표에 직접적으로 접근하기 때문이죠."
Evaluate
전략 효율성 평가
[표Ⅴ-24] 대화1 (세션 414) 흐름 및 Bloom 단계 진행

Bloom 단계 진행: Understand → Apply → Analyze → Evaluate (4단계 상승)
교육적 의의:
● 성적 하위권 학생(Q1)도 명료화+비계를 통해 Bloom 최고 단계(Evaluate) 도달 가능
● 막연한 질문 → 명료화 → 두 가지 방법 제시 → 전략 평가의 체계적 과정
● 이는 "Q1 하위권 +6.91점 효과"의 질적 메커니즘을 실증

2) 분석(Analyze) 단계: 오류 교정


항목
값
LLM 평가점수
 31.33점 (Gemini: 28, Claude: 35, GPT-5: 31)
교사 평가점수
20.5/40점
성적 분위
Q2 (중하위 25%)
대화 턴수
6턴
학습 주제
2의 거듭제곱 합
Bloom 진행
K2(Understand) → K3(Apply/Analyze)
[표Ⅴ-25] 대화2 (세션 150) 평가 정보



턴
발화자
대화 내용
Bloom/기능
Hattie 피드백 수준
1
학생
"기저단계, 귀납단계가 뭐야?"
-
-

MAICE
"기저단계: 시작점()에서 명제가 참임을 보이는 단계
귀납단계: 가 참이라 가정 →  증명
예시: ..."
K2 Understand
-
2
학생
""
-
❌ 틀린 공식 제시
3
MAICE
"주어진 식은 일반적으로 틀립니다.
올바른 공식: "
K3 Analyze
✅ 오류 발견
4
MAICE
"[Task 수준] 오른쪽에 이 빠져 있는 것이 흔한 실수"
-
Back (무엇이 틀렸나)
5
MAICE
"[Process 수준] 시작 인덱스가 0인지 1인지 문제마다 다를 수 있으니 확인하세요. 예: 이면 "
-
Up (왜 틀렸나)
6
MAICE
"[Self-regulation 수준] 증명할 때는 기저단계를 반드시 쓰고, 귀납가정에서 증명하려는 것을 암묵적으로 쓰지 않도록 주의"
-
Forward (어떻게 할 것인가)
[표Ⅴ-26] 대화2 (세션 150) 흐름 및 오류 교정 과정


Hattie & Timperley(2007) 3수준 피드백: Task(Back) → Process(Up) → Self-regulation(Forward)

교육적 의의:
● 오류 기반 학습(Productive Failure) - 학생의 오류를 학습 기회로 전환
◯ Hattie 3수준 피드백 구현: Task(틀림) → Process($-1$ 누락) → Self-regulation(시작 인덱스 확인)
● K2(개념 이해) → K3(절차 적용/오류 분석)으로 즉각 전환
● 단순 정답 제시가 아닌 "왜 틀렸는가"와 "앞으로 어떻게 할 것인가"까지 제공

3) K4 메타인지적 문제해결


항목
값
LLM 평가점수
32.33점 (Gemini: 31, Claude: 34, GPT-5: 32)
교사 평가점수
21.0/40점
성적 분위
Q3 (중상위 25%)
대화 턴수
3턴
질문 유형
K4 (메타인지) - 최고 수준
학습 주제
 부등식 증명 (학생 주도)
[표Ⅴ-27] 대화3 (세션 352) 평가 정보



턴
발화자
대화 내용
K4 메타인지 요소
Bloom 수준
1
학생
(수학적 귀납법을 이용한  증명 과정을 스스로 작성하여 제시)

- 기저 단계: 일 때 확인
- 귀납 가정:  가정
- 귀납 증명:  유도
문제 해결 전략 수립
계획 및 실행
자기 검증
Create
Evaluate

MAICE
[분류] "문제 해결 전략, 계획, 실행, 결과 검증이라는 *메타인지적 지식(K4)*에 해당"
K4 인식
-
2
MAICE
"보여주신 증명 과정이 아주 꼼꼼하고 논리적으로 잘 구성되어 있어서 감탄했습니다! 각 단계별로 무엇을 보여야 하는지 명확하게 파악하고 계신 점이 돋보여요."
긍정적 검증
메타인지 강화
Apply
3
MAICE
[명료화] "증명 과정 중 다른 방식으로 접근해보고 싶은 부분이 있을까요?"

대안 제시:
- 귀납 가정 활용법 정밀화
-  증명 상세화
- 로그 미분 대안적 접근법
학습 확장
다중 전략 제시
Analyze
Evaluate

[표Ⅴ-28] 대화3 (세션 352) 흐름 및 K4 메타인지 과정

K4 메타인지 순환: 문제 해결 전략 → 실행 → 자기 검증 → AI 피드백 → 대안 탐색
교육적 의의:
◯ Schoenfeld(1985) 메타 수준 통제: 학생이 자신의 문제 해결 과정을 모니터링하고 평가
◯ 학생 주도성 존중: MAICE가 먼저 답을 주지 않고, 학생의 증명을 검증하고 개선하는 역할
◯ K4 메타인지: 문제 해결 전략 수립 → 실행 → 자기 검증 → 피드백 요청의 완전한 메타인지 순환
● 상위권 학생에게도 명료화가 효과적 - 단순 답변이 아닌 사고 확장과 대안 탐색 지원

4) 명료화 성공 사례: 막연함에서 구체화로


항목
값
LLM 평가점수
30.33점 (Gemini: 데이터 확인 필요)
교사 평가점수
평가됨 (100개 샘플 포함)
성적 분위
Q4 (상위 25%)
대화 턴수
9턴
명료화 횟수
1회 (효과적)
[표Ⅴ-29] 대화4 (세션 311) 평가 정보



턴
발화자
대화 내용
명료화 효과
5
학생
"이해 안되는데"
막연한 표현
7
MAICE
[명료화] "이해 안 되는 부분이 구체적으로 어떤 단계인가요? 예를 들어, 기저 단계, 귀납 가정, 아니면 마지막 결론 부분일까요? 😊"
선택지 제공
8
학생
"귀납가정"
명확한 특정
9
MAICE
"귀납 가정 단계에서 어려움을 느끼고 계시는군요... 귀납 가정은 '증명할 내용'이 아니라, '증명을 위해 빌려오는 도구'라고 생각하면 좋아요..."
맞춤 설명 제공
[표Ⅴ-30] 세션 311 명료화 과정

명료화 효과: "이해 안됨" (전체 막연) → "귀납 가정" (특정 단계) → 정확한 맞춤 설명

[사례 B-2] 세션 321: "뭐로 변환?" → "로그의 곱 성질" 구체화


항목
값
LLM 평가점수
30.00점 (Gemini: 데이터 확인 필요)
교사 평가점수
평가됨 (100개 샘플 포함)
성적 분위
Q4 (상위 25%)
대화 턴수
7턴
명료화 횟수
1회 (효과적)
[표Ⅴ-31] 세션 321 평가 정보



턴
발화자
대화 내용
명료화 효과
1
학생
"을 뭐로 변환 가능해?"
막연한 질문
4
MAICE
[명료화] "어떤 맥락에서 ''을 변환하고 싶으신가요? 예를 들어, 특정한 방정식을 풀거나, 그래프를 그리거나, 아니면 다른 형태의 식을 만들기 위해서인가요? 😊"
맥락 파악 시도
5
학생
"로그의 곱 성질에 따라서 나눠봐"
명확한 방법 제시
7
MAICE
"... 로그의 곱셈 성질: "
정확한 답변 제공
[표Ⅴ-32] 세션 321 명료화 과정


명료화 효과: "뭐로 변환?" (방법 불명확) → "로그의 곱 성질" (명확한 방법) → 정확한 수학적 변환
사례 B-1, B-2의 의의:
● 짧은 대화(7-9턴)에서도 명료화의 효과 확인
● 막연한 표현("이해 안됨", "뭐로 변환?") → 구체적 특정("귀납가정", "로그 곱 성질")
● 명료화 1회만으로도 학생의 질문 초점을 명확히 하는 효과
● 학생이 스스로 문제를 특정하면 MAICE가 정확한 맞춤 답변 제공

5) 통합 분석: Bloom 단계와 점수의 관계


점수 구간
N
평균 점수
Evaluate
Analyze 이상
Apply 이상
30-34점
4
31.75
75%
100%
100%
20-29점
3
27.56
0%
67%
100%
<20점
2
10.00
0%
0%
0%
[표Ⅴ-33] 점수 구간별 Bloom 교육 목표 달성도

결론: LLM 평가점수는 Bloom 교육 목표 달성도를 반영하는 교육적으로 타당한 지표.


다. Dewey 반성적 사고 5단계 구현 실증

1) 세션 75: Dewey 5/5 단계 완전 구현 사례


항목
값
LLM 평가점수
34.00점 (Gemini: 32, Claude: 35, GPT-5: 35)
교사 평가점수
23.50점 (교사96: 25, 교사97: 22)
성적 분위
Q1 (하위 25%)
모드
agent
대화 턴수
25턴 (평균 3턴의 8배 이상)
Dewey 완성도
5/5 단계 (100%)
명료화 횟수
3회
Bloom 단계
Remember, Understand, Apply
[표Ⅴ-34] 세션 75 평가 정보




Dewey 단계
턴
발화자
대화 내용
교육적 기능
1. 문제 인식
1
학생
" 을 다르게 나타낼 방법이 있어?"
막연한 의문 제기
2. 문제 정의
3
MAICE
[명료화 1회] "'다르게 나타낸다'는 게 곱셈식으로 전개한다는 뜻인가요, 아니면  을 이나 다른 수식으로 표현하고 싶으신 건가요? 🤔"
의도 구체화 유도
3. 가설 형성
5-10
학생
MAICE
학생: 을 로 표현하는 방법 탐색
MAICE: 귀납법 논리 구조 설명
접근법 모색
4. 추론 전개
15-20
학생
" 는 로 나타낼 수 있는거야?"
부등식 관계 추론


MAICE
"두 식을 비교하면 이고, 이므로 서로 다릅니다. 지수법칙을 정확히 구분..."
추론 과정 지원
5. 검증
25
학생
"가 어떻게 이 되는거야"
논리 관계 검증


MAICE
"양변을 로 나누면 가 되므로 일 때 항상 성립합니다"
검증 지원
[표Ⅴ-35] 세션 75 Dewey 5단계 대화 흐름

Dewey 완성도: 5/5 단계 (100%)

질문 진화: " 표현" (막연) → "부등식 전개 관계 검증" (구체적)
교육적 의의:
● 25턴의 긴 대화를 통해 Dewey 5단계 완전 구현
◯ 질문 진화 패턴: "(k+1)! 표현" (막연) → "부등식 전개 관계 검증" (구체적)
● 명료화 3회가 문제 정의 → 가설 형성 → 추론 → 검증 과정을 체계적으로 촉진
● Dewey가 강조한 "문제의 점진적 명료화(progressive clarification)" 과정을 실증
● Dewey 5/5 단계 완전 구현 사례
● 25턴의 긴 대화를 통한 깊은 학습 과정 확인
● 명료화 3회가 Dewey의 "문제 정의" → "검증" 전 과정을 촉진
◯ 질문 진화 패턴: "(k+1)! 표현" (막연) → "부등식 전개 관계" (구체적 검증)



2)  명료화 질문의 Dewey 분류 (278건)


Dewey 단계
빈도
비율
1-2. 문제 인식/정의
231건
83%
3-5. 가설/추론/검증
47건
17%
[표Ⅴ-36] 명료화 질문의 Dewey 단계 분포


분석: 명료화의 주요 역할은 학생이 자신의 의문을 구체화하도록 돕는 것 (Dewey "문제 정의")

3) Dewey 완성도와 점수


점수 구간
평균 Dewey 완성도
5단계 완성
30-34점
4.0/5 (80%)
25% (세션 414)
20-29점
2.3/5 (46%)
0%
<20점
0.7/5 (14%)
0%
[표Ⅴ-37] Dewey 완성도와 LLM 평가점수


상관: Dewey 완성도 ↑ = 점수 ↑ (r≈0.82)



라. LLM 평가점수의 교육적 타당성 검증
1) 점수와 교육적 특성의 강한 연관
◯ Bloom 고차원: 30점 이상 100% Analyze 이상
◯ 대화 깊이: 30점 이상 평균 5.25턴
◯ Dewey 완성도: 30점 이상 평균 4.0/5

2) "AI가 AI 평가" 순환 논리의 해소
LLM 평가의 타당성은 3가지 독립 증거로 검증:
◯ 교사 평가 (r=0.743)
◯ Bloom/Dewey 이론 정합성
◯ 학생 자기 평가 수렴

본 장에서는 MAICE 명료화 프로세스의 효과를 4가지 증거를 통해 다각도로 검증하였다.

1. 객관적 평가 (LLM, N=284):
◯ 전체 효과: C2(학습 지원) Agent 우수 (+0.28점, p=0.004, d=0.353)
◯ 하위권(Q1) 효과: +2.26점, p=0.032, d=0.499 (중간 효과)
◯ 3개 모델 평균, 높은 신뢰도 (ICC=0.848)

2. 전문가 평가 (교사, N=100):
◯ 전체 효과: Agent 경향 (+2.25점, p=0.085, n.s.), 응답 영역 유의 (p=0.017, d=0.488)
◯ 하위권(Q1) 효과: +6.32점, p=0.009, d=0.993 (매우 큰 효과)
◯ LLM과 강한 상관 (r=0.754*), 교사 간 ICC=0.707

3. 학습자 자기 평가 (설문, N=40):
◯ AI 상호작용 품질: 4.38/5.0, 개념 이해: 4.36/5.0
◯ 68.6%가 질문 유도형(Agent) 선호 (이유: "사고력 향상"(42%), "깊은 이해"(25%))
◯ 객관적 평가와 주관적 체감의 일치
4. 질적 분석 (DB 로그 1,589건):
◯ Bloom 고차원 사고: 30점 이상 세션 100% Analyze 도달, 75% Evaluate 도달 (세션 414, 150, 352)
◯ Dewey 반성적 사고: 세션 414는 5/5 단계 완전 구현
◯ 하위권 메커니즘: 명료화 → 질문 구체화 → 맞춤 비계 → Bloom 4단계 상승 → 고득점
◯ LLM 평가 타당성: 교사 평가(r=0.754*) + Bloom/Dewey 이론 + 학생 평가 → 삼각 검증 완료

수렴 패턴:
● 양적 증거(LLM·교사 점수)와 질적 증거(Bloom/Dewey 분석)가 일관
● 객관적 평가와 주관적 체감(학생)이 수렴
● 특히 하위권 효과가 모든 증거에서 일관 (+2.26~+6.32점, Bloom 4단계 상승)
● LLM 평가점수는 단순 수치가 아닌 Bloom 목표 달성도 + Dewey 사고 구현도를 반영하는 교육적 지표임이 입증됨

다음 VI장에서는 이러한 결과의 교육적 의미, 시사점, 연구의 제한점을 논의한다.
VI. 논의 및 결론
본 장에서는 V장에서 확인된 연구 결과를 바탕으로, 그 교육적 의미와 시사점을 논의하고, 연구의 제한점과 후속 연구 방향을 제시한 후, 연구 전체의 기여와 결론을 제시한다.

1. 명료화 프로세스의 작동 메커니즘
가. 질적-양적 증거의 수렴 (Triangulation)
V장의 정량적 발견과 질적 사례 분석은 다음과 같이 수렴한다:


발견 항목
정량적 증거 (V-1~3)
질적 증거 (V-4)
학습 지원
C2 Agent 우수 (p=0.004, d=0.353)
대화1, 2: 명료화로 메타인지 촉진
하위권 효과
Q1 +6.32점 (교사 d=0.993)
대화1 (Q1): 9턴 대화로 31.33점 달성
하위권 C2
Q1 C2 +0.51점 (d=0.855)
Q1 학생의 깊은 사고 과정 촉진
Bloom 고차원 사고
30점 이상 고득점
대화1: Evaluate 단계 도달 (4단계 상승)
Dewey 반성적 사고
명료화 83.1% 작동
대화1, 2: Dewey 5/5 단계 완전 구현
질문 진화
평균 대화 길이 9.8턴 (Agent)
대화2: 막연함→구체적 관계 검증 (25턴)
학습 확장
B3 Agent 우수 (p=0.041, d=0.245)
대화3, 4: 명료화로 질문 구체화
메타인지 (K4)
상위권도 Agent 효과
대화5: K4 질문, 학생 주도 증명
LLM 평가 타당성
교사 상관 r=0.754*(강함)
Bloom/Dewey 이론 정합성 
+ 학생 평가 수렴
[표Ⅵ-1] 질적-양적 증거의 삼각검증

삼각검증의 의미:
◯ 정량 데이터가 "무엇이" 일어났는지 보여주었다면, 질적 분석(1,589건 DB 로그)은 "어떻게, 왜" 일어났는지 설명한다.
◯ 두 증거가 수렴함으로써 연구 결과의 타당성(Validity)과 신뢰성(Reliability)을 확보한다.
◯ 특히 Bloom 교육 목표와 Dewey 반성적 사고 이론이 실제 AI 시스템에서 구현됨을 실증하였다.

나. 명료화 프로세스의 교육적 메커니즘

V장의 양적-질적 증거를 통합하면, 명료화 프로세스는 다음의 교육적 메커니즘으로 작동한다:

1) 메커니즘 1: Dewey "문제 정의" 단계의 구현
세션 414는 Dewey(1933)가 강조한 "문제를 명확히 정의하는 과정"이 AI-학생 대화에서 구현됨을 보여준다. 학생의 막연한 질문("일 때 어떻게?")에 대해 명료화가 3회 작동하면서, 학생은 자신의 의문을 점차 구체화하였고(Turn 3: "서술형 평가 느낌으로"), 최종적으로 Dewey 5단계를 완전히 구현하였다.

이는 단순한 "정보 수집"이 아니라, Dewey가 제시한 "반성적 사고를 촉진하는 교육적 개입"이다. 명료화 질문 278건 중 83%가 Dewey "문제 정의" 단계를 촉진한 것(표Ⅵ-21)은 이를 뒷받침한다.

가) 메커니즘 2: Bloom 단계의 점진적 상승
고득점 세션(30점 이상)은 100% Analyze 이상 도달하고 75%가 Evaluate 단계까지 도달하였다(표Ⅵ-19). 이는 명료화가 단순히 질문을 구체화하는 것을 넘어, 학생의 인지 수준을 점진적으로 상승시키는 교육적 기능을 수행함을 의미한다.

특히 세션 414(Q1 하위권)는 Understand → Apply → Analyze → Evaluate의 4단계 상승을 보여, Anderson & Krathwohl(2001)이 제시한 "고차원 사고(Higher-Order Thinking)"에 도달하였다. 이는 적절한 비계(scaffolding)를 받으면 하위권 학생도 고차원 사고가 가능함을 실증한다.

나) 메커니즘 3: 피드백 루프와 누적 효과

명료화 → 맞춤 답변 → 이해 성공 → 추가 질문 → 재명료화의 순환 구조는 학생의 학습을 점진적으로 심화시킨다. 세션 414의 9턴 대화(평균 3턴의 3배)는 이러한 피드백 루프가 학습 깊이를 만드는 과정을 보여준다.

메커니즘 4: 질문 진화의 가시화

세션 75는 학생의 질문이 어떻게 진화하는지를 명확히 보여준다.

◯ 세션 75 질문 진화 (25턴, 명료화 3회):
  첫 질문: "을 다르게 나타낼 방법이 있어?"
  마지막 질문: "가 어떻게 이 되는거야?"
  → 막연한 표현 질문에서 구체적 부등식 관계 검증으로 진화

이러한 질문 진화는 Dewey가 강조한 "문제의 점진적 명료화(progressive clarification)" 과정을 실증하며, 명료화 질문이 학생의 사고를 단계적으로 심화시키는 교육적 도구로 작동함을 보여준다. 세션 75는 Dewey 5/5 단계를 완전히 구현하였으며, 25턴의 긴 대화를 통해 깊은 학습 과정이 확인되었다.
다. 일반 LLM 대비 차별성

본 연구에서 확인된 명료화 프로세스의 교육적 효과는 다음과 같은 차별성을 갖는다:

차별점 1: 학생 주도성 존중
◯ 즉시 답변 방식: AI가 완성된 답변 제시 → 학생은 수동 수용
◯ 명료화 프로세스: 학생이 문제를 특정 → AI는 맞춤 지원

차별점 2: 메타인지 촉진
◯ 즉시 답변 방식: 답만 얻고 끝
◯ 명료화 프로세스: "무엇을 모르는가" 언어화 과정 경험 → 메타인지 발달 (C2 효과 p=0.004)

차별점 3: 고차원 사고 촉진
◯ 명료화를 통한 깊이 있는 대화(평균 9.8턴)에서 Analyze/Evaluate 단계 도달 사례 확인 (표Ⅴ-23)
◯ 특히 고득점 세션(30점 이상)의 100%가 Analyze 이상, 75%가 Evaluate 단계 도달
◯ 하위권 학생(Q1)도 명료화를 통해 Bloom 4단계 상승 사례 확인 (세션 414)


2. 교육적 시사점

V장에서 확인된 명료화 프로세스의 효과는 다음과 같은 교육적 의미를 갖는다.

가. Bloom-Dewey 교육 이론의 AI 구현 실증

V장 질적 분석(1,589건 DB 로그)에서 확인된 Bloom 교육 목표와 Dewey 반성적 사고의 실제 구현은 고전 교육 이론이 현대 AI 기술로 구현 가능함을 시사한다.

Bloom (1956) 교육 목표의 AI 구현:
◯ 세션 414: Understand → Apply → Analyze → Evaluate (4단계 상승)
◯ 고득점 세션(30점 이상): 100% Analyze 이상 도달, 75% Evaluate 도달
● AI 답변 설계가 의도적으로 고차원 사고를 촉진할 수 있음을 실증

Dewey (1933) 반성적 사고의 디지털 구현:
◯ 세션 414, 75: Dewey 5단계 완전 구현 (100%)
  - 세션 414: 9턴, 명료화 3회
  - 세션 75: 25턴, 명료화 3회, 질문 진화 "(k+1)!" → "부등식 관계 검증"
  - 세션 61: 20턴, 명료화 5회, 학습 확장 "귀납법 개념" → "삼각함수 응용"
● 명료화 질문 278건 중 83%가 "문제 정의" 단계 촉진
● 전통적 교사-학생 대화의 AI 재현 가능성

이론적 시사점:
● 단순 기술 구현이 아닌, 교육 이론 기반 설계의 실증적 효과
● Bloom/Dewey 이론이 LLM 프롬프트 설계로 구현 가능
● 고전 이론의 현대적 재해석 및 확장 가능성

실천적 시사점:
● AI 교육 도구 설계 시 Bloom 단계 명시적 설계 필요
● 명료화 = 정보 수집 아닌 Dewey 문제 정의 도구로 활용

나. 학생 수준별 차별적 효과

V장에서 확인된 Q1 하위권 효과(교사 평가 +6.91점, d=1.117)는 세션 414의 질적 분석을 통해 그 메커니즘이 규명되었다.

세션 414 (Q1 하위권, LLM 31.33점)의 교육적 메커니즘:

V-4절 표Ⅴ-27, 표Ⅴ-28에서 상세히 분석한 바와 같이, 세션 414는 하위권 학생이 명료화 프로세스를 통해 고차원 사고에 도달하는 과정을 보여준다:

하위권 효과 메커니즘:
● 근접발달영역(ZPD) 및 비계(Scaffolding): 명료화가 비계 역할, 학생을 잠재 수준으로 끌어올림
◯ Bloom 단계 점진적 상승: Understand → Apply → Analyze → Evaluate (하위권도 고차원 사고 가능)
◯ 안전한 학습 환경: AI는 잘못된 질문을 비판하지 않음
◯ 성공 경험 누적: 명료화 → 맞춤 답변 → 이해 성공의 선순환

즉시 답변 방식의 한계:
● Freepass는 질문 능력이 있는 중상위권에 효율적
● 하위권은 "무엇을 질문할지" 자체를 모름 → 즉시 답변은 효과 제한적
다. 상호보완적 교육 모델

V장의 결과는 Agent 모드와 Freepass 모드가 서로 다른 교육적 강점을 가지며, 경쟁 관계가 아닌 상호보완적 관계임을 시사한다.

차별적 강점의 의미:
◯ Agent 모드: C2(학습 지원)에서 우세 → 사고 유도와 이해도 확인에 효과적 → 하위권 학생과 개념 학습 단계에 적합
◯ Freepass 모드: A3(학습 맥락)에서 우세 → 메타인지적 질문 표현 능력이 있는 학생에게 효율적 → 중상위권 학생과 빠른 정보 탐색에 적합

교육적 시사점:
● AI 교육 도구 설계 시 학습자 특성과 학습 상황을 고려한 선택적 적용 필요
● 명료화 프로세스와 즉시 답변 방식은 각각 다른 교육적 목적에 부합할 수 있음
● 실제 적용 시 학생 수준에 따라 모드를 선택하거나 전환하는 방식을 고려할 수 있음

라. 방법론적 기여: LLM 평가의 교육적 타당성 입증

V장에서 LLM 평가점수의 교육적 타당성을 삼각 검증으로 입증한 것은 중요한 방법론적 기여이다.
"AI가 AI 평가" 순환 논리의 해소:
본 연구의 LLM 평가는 삼각 검증(Triangulation)을 통해 타당성이 입증되었다:

[그림Ⅵ-1] LLM 평가 타당성 삼각 검증





결론: LLM 평가점수는 세 가지 독립 증거(교사 평가, Bloom/Dewey 이론, 학생 자기 평가)와의 수렴을 통해, 단순 자동 채점이 아닌 Bloom 목표 달성도 + Dewey 사고 구현도를 반영하는 교육적으로 타당한 지표임이 입증되었다.

방법론적 시사점:

◯ 질적-양적 혼합 연구의 완성:
   - 양적: 283개 세션 통계 분석
   - 질적: 1,589건 DB 로그 Bloom/Dewey 분석
   - 통합: 점수의 교육적 의미 해석

◯ 대규모 질적 분석의 가능성:
   - 전통적 질적 연구: 소규모(10~20건) 수동 코딩
   - 본 연구: DB 로그 1,589건 + 자동/수동 혼합 코딩
   - AI 시대의 새로운 질적 연구 방법론 제시

◯ 이론 실증 연구의 모델:
   - 이론(Bloom/Dewey) → 설계(프롬프트) → 구현(AI) → 검증(로그 분석)
   - 교육 이론의 AI 구현 연구에 활용 가능한 방법론
3. 연구의 제한점
V장의 결과는 다음과 같은 제한점을 가지며, 이에 대한 신중한 해석이 필요하다.

가. 연구 범위의 제한
맥락적 제한:
● 고등학교 2학년, 수학적 귀납법 단원이라는 특정 맥락에 한정
● 특수목적고 1개교, 3주간(N=58)의 소규모 단기 연구
● 학교급, 교과목, 학교 유형, 학생 특성 등에 따라 효과가 달라질 가능성

학생 특성의 특수성:
● 본 연구의 참여자는 소프트웨어 개발 특화 고등학교 학생들로, 이미 ChatGPT 등 LLM 도구 사용 경험이 풍부함
● 사전 조사에서 대다수 학생이 AI 학습 도구를 자주 사용(주 3회 이상)하는 것으로 확인됨
● 이러한 학생들은 AI와의 대화 방식, 프롬프트 작성 등에 이미 익숙하여, 일반 고등학생보다 명료화 프로세스에 빠르게 적응했을 가능성
● 따라서 본 연구의 결과를 AI 사용 경험이 적은 학생들에게 일반화하는 데 신중함이 필요
● 후속 연구에서는 AI 사용 경험이 다양한 학교급과 학생군을 포함한 검증이 필요함

명료화 프로세스의 한계:
◯ 학습 맥락 파악 부족: LLM 평가에서 A3(학습 맥락) 항목은 Freepass가 우수 (p=0.001, d=-0.411)
● 명료화 질문 중에 학습자의 학년, 수준, 목표 등을 충분히 수집하지 못했을 가능성
◯ 향후 시스템 개선: 명료화 질문에 학습자 정보 수집 단계 추가 필요

수동적 반응의 한계 (실패 사례):

케이스: 세션 73에서 학생이 " 부터 모르겠어"라고 질문
◯ QI 명료화 시도: "식 전개가 어려운가요? 아니면 가정 부분이 어려운가요?"
◯ 학생 응답: "둘 다" (수동적 선택)
◯ 결과: 학생이 자신의 어려움을 구체화하지 못함
문제점:
● QI가 선택지를 제시했으나, 학생이 능동적으로 문제를 특정하지 못함
● "둘 다", "전부 다", "모르겠어" 같은 애매한 응답만 반복
● 이런 경우 명료화 프로세스가 Dewey의 "문제 정의" 단계로 이어지지 못함
교육적 해석:
● 명료화는 학생의 메타인지 능력이 어느 정도 전제되어야 작동
● 자신의 어려움을 인식하고 언어화할 능력이 부족한 학생에게는 한계
● V-4절 사례(세션 311)처럼 학생이 능동적으로 "기저 단계", "귀납 가정"을 특정한 경우에만 명료화가 효과적
시스템 개선 방향:
● 수동적 응답 감지 시 더 구체적인 보조 질문 추가
◯ 예: "기저 단계와 귀납 단계 중 어디서 막혔나요?"처럼 더 세분화
● 학생이 끝까지 특정하지 못하면, Freepass 모드로 전환하는 하이브리드 전략 고려
일반화 가능성에 대한 해석:
● 본 연구의 결과는 "명료화 프로세스의 잠재력"을 보여주는 탐색적 증거로 해석되어야 함
● 결과의 일반화를 위해서는 다양한 맥락(학년, 단원, 학교 유형)에서의 재현 연구가 필수적
● 현재 결과는 proof-of-concept 수준이며, 추가 검증이 필요

나. 평가의 제한
AI 평가의 방법론적 딜레마:
● AI가 AI를 평가하는 순환성 문제는 근본적으로 완전히 해소될 수 없음
● 교육 현장의 미묘한 맥락(학생 감정, 동기, 학급 분위기 등)을 완전히 반영하기 어려움
● 체크리스트 기반 평가는 정량화 가능한 측면만 포착
교사 평가의 표본 한계:
● 평가자 2명(교사 96, 97), 표본 100개는 통계적 검정력이 제한적
● 특히 하위집단 분석(Q1, Q2 등)에서 표본 크기가 매우 작아 결과 해석에 신중함 필요
● 평가자가 2명으로 제한되어 평가 신뢰성 확보에 한계
LLM-교사 평가 불일치의 교육적 의미:
V-2-라(표Ⅴ-11)에서 LLM과 교사 평가의 항목별 상관계수는 흥미로운 패턴을 보였다:
◯ B1(학습자 맞춤도): r=0.758 (매우 높은 일치)
◯ C2(학습 지원): r=0.416 (중간 수준 일치)
◯ 
이러한 차이는 단순한 "제한점"이 아니라, 인간 교사와 LLM의 평가 관점 차이를 보여주는 중요한 발견이다.

해석 1: 평가 난이도의 차이
● B1(맞춤도)은 "학생 수준 언급", "선수지식 연계" 등 텍스트 기반으로 판단 가능한 명시적 요소
● C2(학습 지원)는 "사고 과정 유도", "메타인지 촉진" 등 교육적 의도를 파악해야 하는 고차원 요소
● LLM은 명시적 요소는 잘 포착하나, 교육적 의도 같은 암묵적 요소는 교사만큼 정확히 판단하기 어려움

해석 2: 교육적 판단 기준의 차이
● 교사는 "이 답변이 학생의 사고를 어떻게 변화시킬까?"라는 잠재적 학습 효과 중심 평가
● LLM은 "이 답변에 어떤 요소가 포함되었는가?"라는 텍스트 표면 특징 중심 평가
● C2는 표면 특징보다 교육적 효과를 평가해야 하므로 차이 발생

연구적 가치:
● 이 불일치는 "AI 평가의 한계"이자 동시에 "인간 교사의 고유한 전문성이 무엇인가"를 규명하는 단서
● 향후 연구에서 이 불일치 지점을 심층 분석하면, AI가 포착하기 어려운 "교육적 판단"의 본질을 밝힐 수 있음

상호 보완을 통한 완화:
● LLM-교사 평가의 높은 전체 일치도(r=0.743)는 각 평가 방법의 약점을 상당 부분 보완
● 항목별 차이(B1=0.758 vs C2=0.416)는 제한점이자 새로운 연구 질문을 제공
● 결과는 "강한 증거"라기보다 "수렴하는 패턴"으로 해석되어야 함

다. 응답 편향 가능성
학생 설문의 잠재적 편향:
● 교사-연구자 이중 역할로 인한 권위 관계가 학생 응답에 영향을 미쳤을 가능성
● 비익명 응답 구조(모드 매칭을 위한 개인 식별 필요)로 인한 사회적 바람직성 편향
● 실험 참여에 대한 호손 효과(Hawthorne effect) 가능성

완화 전략 및 해석:
● 본 연구는 주요 증거를 객관적 QAC 점수(N=280)에 두고, 학생 설문은 보조 자료로만 활용
● 설문에서 반대 의견(27.5%)이 존재한 것은 어느 정도의 솔직성을 시사
● 설문 결과와 QAC 패턴이 수렴한 것은 방향성의 신뢰성을 지지하나, 절대적 수치는 신중하게 해석되어야 함
● 향후 연구에서는 외부 연구자에 의한 익명 설문이 필요

4. 후속 연구 제언
본 연구의 제한점과 발견을 바탕으로 다음과 같은 후속 연구를 제안한다.

가. 교사 평가 확대 및 검증
본 연구의 교사 평가는 예비적 수준(N=100, 평가자 2명)이므로, 다음과 같은 대규모 검증 연구가 필요하다:

◯ 표본 확대: 평가자 10명 이상, 표본 300개 이상으로 확대하여 통계적 검정력 확보
◯ 독립 검증: 새로운 학생 집단에서 AI-교사 일치도 재검증으로 일반화 가능성 확인
◯ 타당도 연구: QAC 체크리스트의 내용 타당도와 교사 간 일치도에 대한 심층 연구

나. 맥락 확장 연구
본 연구는 단일 맥락(고2, 수학적 귀납법, 특목고)에 한정되어 일반화 가능성이 제한적이므로, 다음과 같은 확장 연구가 필요하다:

◯ 단원 확장: 수학Ⅰ의 다른 단원(수열, 극한), 수학Ⅱ, 미적분 등으로 확장하여 명료화 프로세스가 수학의 다른 영역에서도 효과적인지 검증
◯ 학년 확장: 고1, 고3 학생 대상 효과 검증으로 학년별 차별적 효과 탐색
◯ 학교 유형 다양화: 일반고, 자사고 등 다양한 학교 유형에서의 효과 비교
◯ 장기 추적 연구: 1학기 이상 장기 사용 효과 및 학습 패턴의 변화 관찰
◯ 교과 확장: 과학, 사회, 언어 등 다른 교과로 확장하여 범용성 검증
다. LLM-교사 평가 불일치 심층 연구

V-2-라에서 확인한 B1(맞춤도, r=0.758) vs C2(학습 지원, r=0.416) 불일치는 중요한 후속 연구 주제이다.

연구 질문:
◯ 왜 LLM은 "맞춤도"는 교사와 비슷하게 판단하면서, "학습 지원"은 다르게 판단하는가?
◯ C2에서 교사가 높게 평가하고 LLM이 낮게 평가한 답변의 특징은 무엇인가?
◯ 반대로, LLM이 높게 평가하고 교사가 낮게 평가한 답변은 어떤 차이가 있는가?
연구 방법:
◯ 불일치가 큰 사례 50개를 질적 분석 (grounded theory)
◯ 교사 심층 인터뷰: "어떤 기준으로 C2를 평가했는가?"
◯ LLM 평가 근거 추출: Chain-of-Thought 프롬프팅으로 평가 이유 명시화
◯ 인간-AI 평가 기준의 차이를 체계적으로 규명
기대 효과:
◯ AI가 포착하기 어려운 "교육적 판단"의 본질 규명
◯ 인간 교사의 고유한 전문성 이론화
◯ AI 평가 도구의 한계와 개선 방향 제시

라. 실제 학업 성취도 효과 검증
본 연구는 QAC 점수(학습 과정 품질)와 학생 인식(주관적 효과)을 검증하였으나, 실제 학업 성취도로의 전이는 미검증 상태이다.

후속 연구에서는 다음과 같은 학업 성취도 측정이 필요하다:
● 정기고사, 수행평가 점수 변화
● 사전-사후 개념 이해도 검사
● 장기적 학업 성취도 추이 분석
● 명료화 경험이 실제 시험 성적 향상으로 이어지는지 인과관계 검증
마. 교사 주도 연구 플랫폼으로의 확장

본 연구는 교사-연구자가 직접 시스템을 설계하고 배포한 사례로서, 교사 전문성 중심 AI 교육 연구의 가능성을 보여주었다.

이를 확장하여 다음과 같은 플랫폼 연구가 필요하다:
◯ 커스터마이징 기능: 교사가 자신의 맥락에 맞게 프롬프트와 QAC 체크리스트를 수정할 수 있는 도구 제공
◯ 최적 전략 탐색: 단원별, 학생 수준별로 효과적인 명료화 전략을 교사들이 함께 탐색하는 실행 연구(Action Research)
◯ 교사 커뮤니티: 성공 사례와 실패 사례를 공유하며 AI 교육 방법론을 공동으로 발전시키는 협력적 연구 생태계 구축
◯ DBR 접근: 설계기반연구(Design-Based Research) 방법론을 적용하여 현장 교사들이 지속적으로 시스템을 개선하고 연구하는 순환 모델

5. 결론
본 연구는 질문 명료화를 지원하는 AI 에이전트 시스템 MAICE를 개발하고, 고등학교 2학년 58명 대상 A/B 테스트(284개 세션)를 통해 다음의 발견을 확인하였다.

가. 주요 연구 결과
1. 명료화 프로세스의 교육적 효과:

이중 평가(LLM N=284, 교사 N=100)를 통해 명료화 프로세스가 학습 지원(C2)을 통계적으로 유의하게 향상시킴을 관찰(p=0.002, d=0.391). 특히 하위권 학생에 대한 큰 효과(d=0.840)는 교육 격차 해소에 기여할 가능성을 시사.
2. Dewey 반성적 사고 이론의 적용 시도:

Dewey의 교육 이론(문제의 명료화)을 AI 시스템으로 구현하고 A/B 테스트로 효과를 검증한 결과, 고전 교육 이론이 현대 기술과 결합하여 측정 가능한 학습 효과를 보일 수 있음을 확인.

3. 상호보완적 교육 모델 가능성:

명료화(Agent)와 즉시 답변(Freepass) 방식이 각각 다른 교육적 강점을 가지며, 학습자 특성에 따른 선택적 적용이 필요함을 확인.

나. 이론적 의의

1. 교육 이론 기반 AI 시스템 설계: Dewey 반성적 사고와 Bloom 지식 분류를 AI 시스템 설계에 적용하고, A/B 테스트를 통해 효과를 검증한 사례 제공
2. QAC 평가 도구 개발: 질문-답변-맥락 3개 영역, 8개 항목으로 구성된 체크리스트를 개발하여 AI 교육 도구의 교육적 효과를 평가하는 방법 제안
3. LLM-교사 이중 평가 시도: 대규모 AI 평가(N=284)와 소규모 교사 평가(N=100)를 결합한 평가 방법을 시도하여, 각 방법의 한계를 보완하는 전략 탐색

다. 실천적 의의

1. 하위권 학생 지원 가능성: 하위권 학생에 대한 효과(d=0.840)를 확인하여, AI 도구가 학습 지원에 기여할 가능성 확인
2. AI 튜터 설계 시 고려사항:
● 즉시 답변 제공 방식의 한계
● 명료화 과정의 교육적 가치
● 학생 수준별 맞춤형 지원의 필요성
3. 교사 주도 연구 사례:
● 교사가 직접 AI 교육 도구를 설계하고 연구할 수 있는 사례 제공
● 교육 현장에서 실행 가능한 연구 모델 제안

라. 연구의 의의

본 연구는 AI 교육 도구가 단순히 정답을 제공하는 것보다 학생의 사고 과정을 자극하는 방향으로 설계될 때 학습 지원 측면에서 차별적 효과를 보일 수 있음을 확인하였다.

특히, 명료화 프로세스가 하위권 학생에게 통계적으로 유의한 효과를 보인 것은 AI 기술이 학습 지원에 기여할 수 있는 하나의 방향을 제안한다.

다만, 본 연구는 소규모 단기 연구로 일반화에 한계가 있으며, 결과는 명료화 프로세스의 가능성을 탐색한 예비적 증거로 해석되어야 한다. 향후 다양한 맥락에서의 재현 연구와 장기적 효과 검증을 통해 본 연구의 발견이 확장될 수 있기를 기대한다.
I. 부록 A: 기술 구현 상세


본 부록에서는 MAICE 시스템의 기술적 구현 내역을 요약한다. 본문 III장과 IV장에서 다룬 교육적 설계와 프롬프트 관리를 보완하는 공학적 세부사항을 제시한다.

1. 
[그림 A-1] MAICE 시스템 3계층 아키텍처
 시스템 아키텍처 및 기술 스택
가. 3계층 마이크로서비스 구조

계층
핵심 기술
통신 방식
역할
프론트엔드
SvelteKit 2.0, MathLive
HTTP/SSE
학생 UI, 수식 입력
백엔드
FastAPI 0.104, Python 3.11
REST API, Redis
API 제공, 에이전트 조율
에이전트
Gemini 2.5 Flash, asyncio
Redis pub/sub
질문 분류, 명료화, 답변
데이터
PostgreSQL 15, Redis 7
ORM, Streams
영구 저장, 메시지 큐


나. Agent vs Freepass 모드 비교

항목
Agent 모드
Freepass 모드
에이전트 수
4개 (QC→QI→AG→LO)
2개 (FT→LO)
명료화 과정
✓ Dewey 5단계 기반
✗ 생략
질문 분류
✓ K1-K4 분류
✗ 생략
교육적 개입
✓ 메타인지 유도
✗ 즉시 답변



2. A.2. 에이전트 구현 핵심
가. 5개 에이전트 역할

에이전트
역할
입력
출력
모드
QC
질문 분류
학생 질문
K1-K4 유형, 품질 평가
Agent
QI
명료화 평가
학생 답변
PASS/NEED_MORE
Agent
AG
답변 생성
최종 질문
교육적 답변
Agent/Freepass
LO
학습 요약
전체 대화
세션 제목, 요약
Agent/Freepass
FT
즉시 답변
학생 질문
일반 답변
Freepass


나. 에이전트 간 협업 흐름


[그림 A-2] 에이전트 협업 메커니즘 (Redis pub/sub 기반)



Agent 모드: 학생 질문 → QC(분류) → QI(명료화) → AG(답변) → LO(요약)
Freepass 모드: 학생 질문 → FT(즉시 답변) → LO(요약)

다. Redis Streams 메시지 실제 예시

[표 A-1] Redis Streams 메시지 구조 실제 예시 (세션 13 기반)

필드
실제 값 (예시)
설명
session_id
13
세션 고유 ID
user_id
24
학생 ID (24.019@bssm.hs.kr)
question
"수열의 합을 통한 일반항 구하기"
학생 질문 내용
mode
"agent"
Agent 또는 Freepass
conversation_history
`[{"role": "user", "content": "하이"}, {"role": "user", "content": "수열의 합을 통한 일반항 구하기"}]`
이전 대화 히스토리
timestamp
"2025-09-30T08:38:03.117789"
메시지 생성 시각 (ISO8601)

3. 배포 및 인프라
가. 
[그림 A-3] Docker Compose 컨테이너 구조
Docker Compose 서비스 구성


서비스
기술
포트
역할
nginx
Nginx 1.25
80, 443
정적 파일 서빙, 리버스 프록시
maice-back
FastAPI + Python 3.11
8000
REST API 제공
maice-agent
Python 3.11 + Gemini API
-
5개 에이전트 AI 처리
postgres
PostgreSQL 15
5432
데이터 영구 저장
redis
Redis 7
6379
메시지 큐, 세션 캐시


나. 성능 최적화 및 안정성

영역
전략
세부 내용
Connection Pool
Redis
최대 50개 연결 재사용
PostgreSQL
20개 기본 + 10개 overflow
Rate Limiting
Gemini API
분당 15 requests, 자동 큐잉
에러 처리
LLM API
3회 재시도, 지수 백오프
Redis/DB
자동 재연결
비동기 I/O
uvloop
asyncio 대비 2-4배 성능



4. 이미지 OCR 수식 인식 시스템

가. 가. 처리 흐름

[그림 A-4] 이미지 OCR 처리 파이프라인





단계
처리 내용
기술
1. 이미지 업로드
사용자가 수식 사진 업로드
JPG/PNG/WebP
2. 파일 검증
10MB 이하, 형식 확인
ImageToLatexService
3. 전처리
RGB 변환, 1536×1536 리사이즈
PIL
4. OCR 처리
이미지 → LaTeX 변환
Gemini Vision API
5. 정제
MathLive 호환성 변환
`\dots` → `\ldots` 등
6. 에디터 삽입
커서 위치에 삽입, 실시간 렌더링
MathLive


나. 교육적 차별점

특징
일반 LLM
MAICE OCR
처리 방식
이미지를 LLM에 직접 전달
이미지 → LaTeX 텍스트 변환
편집 가능
❌ 이미지로만 인식
✅ 텍스트로 편집 가능
오인식 수정
❌ 재업로드 필요
✅ 입력창에서 즉시 수정
통합성
❌ 이미지와 텍스트 분리
✅ 하나의 텍스트로 통합

교육적 가치: OCR 결과 확인 → 자신이 쓴 수식 점검 → 질문 명료화


II. 부록 B: 에이전트 프롬프트 설계 및 구현 예시
본 부록에서는 MAICE 시스템의 5개 에이전트에서 사용된 프롬프트 설계 원리와 핵심 구현 전략을 제시한다. 모든 내용은 2025년 10월 20일~11월 7일 실제 운영 환경에서 수집된 284개 세션(58명 학생)을 기반으로 하며, 실제 대화 예시와 함께 제공하여 연구의 재현 가능성을 확보한다.

재현성 관련 안내: 본 부록은 에이전트의 교육적 설계 원리, 핵심 프롬프트 전략, 실제 세션 예시를 포함하여 연구의 이론적·실천적 재현을 지원한다. 시스템 전체 구현 코드 및 상세 프롬프트는 논문 분량과 가독성을 고려하여 핵심 부분을 발췌하였으며, 전체 구현에 관심 있는 연구자는 저자에게 연락하거나 향후 공개될 오픈소스 저장소를 통해 접근할 수 있다.

1. 에이전트별 프롬프트 개요

Agent
역할
모델
입력
출력
모드
classifier_llm (QC)
질문 분류
Gemini 2.5 Flash Lite
학생 질문, 이전 대화
K1-K4 유형, answerable/needs_clarify, 명료화 질문
Agent
question_improvement_llm (QI)
명료화 평가
학생 응답, 원본 질문
PASS/NEED_MORE, 최종 질문
Agent
answer_generator_llm (AG)
답변 생성
최종 질문, K1-K4 유형
교육적 답변 (마크다운)
Agent/Freepass
observer_llm (LO)
학습 요약
전체 대화 내역
세션 제목 (15자), 학습 요약
Agent/Freepass
freetalker_llm (FT)
즉시 답변
학생 질문
일반 답변
Freepass


◯ `max_tokens`: 3,000~5,000 (에이전트별 상이)
◯ `temperature`: null (기본값 사용)
◯ `stream`: QC/QI는 false, AG/FT는 true


2.  QC (classifier_llm) 프롬프트: 질문 분류 및 명료화 질문 생성
가. System Prompt (핵심 발췌)

구분
내용
역할
당신은 대한민국 고등학교 수학 교육과정 전문 분류기입니다.
질문을 정확히 분석하여 4가지 유형과 3단계 품질로 분류하고, 필요한 경우 학생에게 직접 묻는 명료화 질문까지 생성하세요.

🚨 명료화 질문은 학생이 직접 읽고 답변할 수 있는 자연스러운 질문이어야 합니다!
❌ 시스템 분석: "'나'라는 답변이 구체적으로 무엇을 의미하는지 확인 필요"
✅ 학생 질문: "어떤 부분이 더 궁금하신가요? 😊"
질문 유형
(Anderson & Krathwohl, 2001)
K1 (즉답형) - 사실적 지식 - 정의, 용어, 기호, 공식, 값, 단위 등 기본 사실
K2 (설명형) - 개념적 지식 - 개념 간 관계, 분류, 원리, 이론, 비교/대조
K3 (적용형) - 절차적 지식 - 수행 방법, 기술, 알고리즘, 절차, 단계별 과정
K4 (문제해결형) - 메타인지적 지식 - 전략적 사고, 문제 접근법, 계획, 반성
품질 평가
answerable - 교과(수학), 단원·수준 지정, 목표 동사 명확, 충분한 정보 제공
needs_clarify - 범위 과대/목표 불명/수준 불명/용어 혼동, 추가 정보 필요
unanswerable - 수학 외 영역, 평가윤리 위배, 교과 불일치 심각
명료화 질문
생성 규칙
(Dewey 반성적 사고)
기본 원칙:
- 학생이 스스로 생각하고 성찰하도록 유도하는 질문
- 단순 정보 요청(❌) → 사고 과정 촉진(✅)
- 닫힌 질문(❌) → 열린 탐색 질문(✅)

5단계 질문 전략:
문제 인식 - "이 개념을 공부하면서 어떤 부분이 가장 어렵거나 궁금하셨나요? 🤔"
문제 정의 - "지금까지 이해한 부분과 아직 헷갈리는 부분을 나누어볼까요?"
 연결 탐색 - "이미 알고 있는 개념과 비교하면 어떤 점이 비슷하거나 다른가요?"
사고 전개 - "왜 이 부분이 궁금하신지 조금 더 설명해주실 수 있나요?"
 이해 검증 - "어디까지 이해했고, 어디서부터 막히셨는지 말씀해주실 수 있나요?"

나. 출력 형식 (JSON)

필드
설명
knowledge_code
K1/K2/K3/K4 중 하나
quality
answerable/needs_clarify/unanswerable 중 하나
reasoning
분류 근거
missing_fields
부족한 정보 목록 (배열)
clarification_questions
학생에게 묻는 자연스러운 질문 (배열)
unanswerable_response
unanswerable인 경우 안내 메시지


다. 실제 적용 예시 (DB에서 추출)

세션 예시: "근의 공식 싫다고 쉽게 설명해줘"
이전 맥락: 학생이 수학적 귀납법 증명 중 근의 공식을 사용하는 과정에서 어려움 표현

QC 분류 결과:
◯ `knowledge_code`: "K2" (근의 공식과 대수적 방법의 관계 비교)
◯ `quality`: "needs_clarify" (어떤 방법으로 쉽게 설명할지 불명확)
◯ `clarification_questions`: ["근의 공식을 사용하지 않고 다른 방법(대수적 방법, 그래프 등)으로 설명해드릴까요? 😊"]
3. QI (question_improvement_llm) 프롬프트: 명료화 평가
가. System Prompt (핵심 발췌)

구분
내용
역할
당신은 명료화 과정을 통해 원본 질문이 명료해졌는지 평가하고, 필요시 질문 유형을 재분류하며, 명료화가 완료된 경우 최종 질문을 생성하는 전문가입니다.
평가 목적
1. 명료화 질문-답변 과정을 통해 원본 질문의 의도가 명확해졌는지 판단
2. 학생의 답변에 따라 질문 유형이 변경되었는지 재분류
3. 명료화가 완료된 경우 상황을 요약한 최종 질문 생성
평가 기준
PASS (충분히 명료): 원본 질문의 의도가 명확해짐, 답변 생성이 가능한 수준의 정보 확보
NEED_MORE (추가 필요): 원본 질문의 의도가 여전히 불분명함, 답변 생성에 필요한 정보가 부족함
명료화 질문 생성
(Dewey 반성적 사고)
역할: 존 듀이의 반성적 사고 이론 기반
- 학생이 스스로 사고하고 성찰하도록 유도하는 명료화 질문 생성
- 단순 정보 수집이 아닌, 메타인지적 사고 촉진
- 친근하고 지지적인 학습 환경 조성

5단계 질문 전략:
문제 인식 - "이 개념을 공부하면서 어떤 부분이 가장 어렵거나 궁금하셨나요? 🤔"
 문제 정의 - "지금까지 이해한 부분과 아직 헷갈리는 부분을 나누어볼까요?"
연결 탐색 - "이미 알고 있는 개념과 비교하면 어떤 점이 비슷하거나 다른가요?"
사고 전개 - "왜 이 부분이 궁금하신지 조금 더 설명해주실 수 있나요?"
이해 검증 - "어디까지 이해했고, 어디서부터 막히셨는지 말씀해주실 수 있나요?"
재분류 기준
- 학생의 답변에서 드러난 구체적인 관심사나 목표 분석
- 답변 내용의 성격 (사실/개념/절차/메타인지) 파악
- 원본 질문과 학생 답변을 종합한 최종 요구사항 판단
- 기존 유형과의 일치성 및 적절성 검토


나. 평가 로직


평가 결과
조건
다음 단계
PASS
질문 의도 명확, 답변 생성 가능
최종 질문 생성 → AG로 전달
NEED_MORE
의도 불명확, 정보 부족
추가 명료화 질문 (최대 3회)


다. 실제 적용 예시

입력:
◯ 원본 질문: "" (막연)
◯ 기존 유형: K2
◯ 학생 답변: "맞맞맞아"

QI 평가:
◯ `evaluation`: "NEED_MORE" (너무 짧고 구체적이지 않음)
◯ `reasoning`: "학생이 무엇에 동의하는지 불명확"
◯ `next_clarification`: "어떤 부분이 가장 궁금하신가요?"


4. AG (answer_generator_llm) 프롬프트: 교육적 답변 생성

가. System Prompt (핵심 발췌)


구분
내용
역할
당신은 대한민국 고등학교 수학 교육과정 전문가입니다.
학생의 질문에 대해 체계적이고 교육적인 답변을 생성해주세요.
기본 원칙
- 대상: 고등학교 2학년 학생
- 언어: 한국어, 존댓말 필수 (해요체/하세요체), 친근하지만 정중한 톤
- 용어: 2015 개정 교육과정 표준 용어 준수 (✅ 부등식 / ❌ 불등식)
- 수식: LaTeX 형식 ($수식$), 한글과 수식 분리 필수
- 인지부하: 복잡한 내용은 핵심 흐름 먼저, 세부사항은 단계적으로
- 점진적 학습: 이해 가능한 수준부터 시작하여 점진적으로 심화
수식 작성 규칙
(매우 중요!)
올바른 예시:
✅ $P(k)$가 참이면 $P(k+1)$도 참이라는 것을 보일 수 있다면, 모든 자연수 $n$에 대해 $P(n)$이 참입니다.
✅ 즉, $P(k) \Rightarrow P(k+1)$이라면 모든 자연수 $n$에 대해 $P(n)$이 참이라는 논리가 완성됩니다.

잘못된 예시:
❌ $$P(k)가 참 \Rightarrow P(k+1)도 참이라면, 모든 자연수 n에 대해 P(n)이 참$$ (한글 혼입)
❌ $n=1$일 때 참이다 (한글 혼입)

핵심 규칙:
- 수식 구분자($, $$) 안에는 절대로 한글 포함 금지!
- 한글 설명은 반드시 수식 밖에 작성
- 각 수학 기호나 변수를 개별 인라인 수식($...$)으로 감싸기
K1 답변 구조
(즉답형 - 사실적 지식)
핵심 내용 정리 → 핵심 공식과 정리 → 실제 예시로 이해하기 → 더 넓게 알아보기
K2 답변 구조
(설명형 - 개념적 지식)
개념 정리하기 → 개념들 간의 연결고리 → 비슷한 개념과의 차이점 → 헷갈리기 쉬운 부분
K3 답변 구조
(적용형 - 절차적 지식)
단계별 문제 해결 과정 → 언제 이 방법을 쓸 수 있는지 → 실제 문제로 연습해보기 → 실수 방지하기
K4 답변 구조
(문제해결형 - 메타인지)
문제를 체계적으로 분석하기 → 다양한 접근 방법 생각하기 → 중간에 점검하기 → 다른 방법도 생각해보기




나. User Prompt 구조


요소
내용
이전 대화 컨텍스트
전체 학습 진행 상황, 지금까지 다룬 개념들 포함
현재 질문
- 질문: [학생의 질문]
- 질문 유형 (내부용): K1/K2/K3/K4
- 명료화 정보: 명료화 과정을 거쳐 구체화된 질문
출력 형식
- 마크다운 형식으로 체계적이고 교육적인 답변
- JSON이 아닌 일반 텍스트로 답변
- 제목은 ## 형식으로 섹션 구분
주의사항
답변에는 K1~K4 질문 유형 코드를 절대 언급하지 마세요!
분류 정보나 시스템 내부 처리 정보는 노출 금지
위 컨텍스트를 바탕으로 연속성 있고 자연스러운 답변을 생성


5. LO (observer_llm) 프롬프트: 학습 과정 요약

가. 가. System Prompt (핵심 발췌)


구분
내용
역할
당신은 학습 과정 요약 전문가입니다.
학생의 질문, 명료화 과정, 답변을 간결하고 명확하게 요약하고,
세션을 대표하는 깔끔한 제목도 함께 생성하여
백엔드 시스템에서 활용할 수 있도록 구조화된 정보를 제공합니다.
제목 생성
가이드라인
- 학생이 실제로 질문한 핵심 내용을 반영
- 15자 이내의 간결하고 명확한 제목
- "방법", "구하는", "알려주세요", "세션 XXX의 학습 요약:" 등 불필요한 단어 제거
- 구체적인 수학 개념과 주제를 명확히 표현
제목 예시
- "이차함수 꼭짓점" (← "이차함수의 꼭짓점을 구하는 방법을 알려주세요")
- "삼각함수 그래프" (← "삼각함수 그래프의 성질을 설명해주세요")
- "미분 기본개념" (← "미분의 기본 개념을 알려주세요")
- "로그함수 성질" (← "로그함수의 성질을 설명해주세요")
요약 기준
- 질문 요약: 최대 100자 (핵심 개념, 질문 의도, 구체적 내용)
- 학습 요약: 최대 200자 (주요 개념, 핵심 설명, 학습 포인트)
- 주요 개념 추출
- 학생 진척도 평가


나. 출력 형식 (JSON)

필드
설명
session_title
학생의 핵심 질문을 반영한 15자 이내 제목
learning_summary
전체 학습 내용 200자 이내 요약
key_concepts
주요 수학 개념 목록 (배열)
student_progress
학생의 이해도와 학습 성과


6. B.FT (freetalker_llm) 프롬프트: 즉시 답변 (대조군)
가. System Prompt (전문)

구분
내용
프롬프트 전문
필요할 때만 수학 수식을 LaTeX 형식($수식$)으로 작성해주세요.
특징
- 가장 단순한 프롬프트 (1문장)
- K1-K4 분류 없음
- 명료화 과정 없음
- Dewey 5단계 이론 없음
- 대조군 역할: Agent 모드와의 비교를 위한 최소 개입



7. 
[그림 B-1] Agent 모드: 명료화 과정 상세 플로우
B.7. Agent 모드 명료화 플로우


플로우 설명:
◯ QC (분류): 학생 질문 → K1-K4 유형 + answerable/needs_clarify/unanswerable 판정
◯ 명료화 루프: needs_clarify → Dewey 5단계 기반 질문 → 학생 응답 → QI 평가
◯ QI (평가): PASS → 최종 질문 생성 / NEED_MORE → 추가 명료화 (최대 3회)
◯ AG (답변): K1-K4 유형별 맞춤 교육적 답변 (마크다운)
◯ LO (요약): 세션 제목 (15자) + 학습 요약 (200자)


8. 실제 프롬프트 전문 (대표 사례)
가. QC (classifier_llm) - 질문 분류
실제 System Prompt 길이: 15,318자

핵심 구성 요소:
◯ 교육 맥락 (고등학교 2학년, 2015 개정 교육과정)
◯ K1-K4 유형 정의 (Anderson & Krathwohl, 2001)
◯ answerable/needs_clarify/unanswerable 품질 기준
◯ missing_fields 가이드라인 (유형별 상이)
◯ Dewey 5단계 기반 명료화 질문 전략
◯ 후속 질문 판단 로직 (맥락 참조)
◯ JSON 출력 형식 강제

나. AG (answer_generator_llm) - 답변 생성

실제 System Prompt 길이: 8,384자


핵심 구성 요소:
◯ 기본 역할 (고2 대상, 존댓말, 친근한 톤)
◯ 표준 용어 준수 (부등식 O / 불등식 X)
◯ LaTeX 수식 규칙 (한글과 수식 분리!)
◯ K1-K4 유형별 답변 구조
◯ 인지부하 관리 (한 번에 한 개념)
◯ 명료화 정보 활용 방법
◯ 마크다운 형식 규칙

다. LO (observer_llm) - 학습 요약

실제 System Prompt 길이: 5,875자

핵심 구성 요소:
◯ 세션 제목 생성 (15자 이내, 불필요한 단어 제거)
◯ 학습 요약 (200자 이내)
◯ 주요 수학 개념 추출
◯ 학생 진척도 평가
◯ JSON 출력 형식

라. FT (freetalker_llm) - 즉시 답변
실제 System Prompt 길이: 43자
전문: "필요할 때만 수학 수식을 LaTeX 형식($수식$)으로 작성해주세요."
대조군 설계: 최소 개입으로 Agent 모드와의 순수 비교 가능


9. 프롬프트 설계 원칙 요약


원칙
적용 방법
교육학적 근거
Dewey 반성적 사고
QC의 명료화 질문 5단계 전략
Dewey (1933)
Bloom 분류학
K1-K4 유형별 맞춤 교수법
Anderson & Krathwohl (2001)
인지부하 이론
AG의 점진적 심화, 한 번에 한 개념
Sweller (1988)
표준 용어 준수
2015 개정 교육과정 용어 강제
교육부 (2015)
학생 주도성
명료화를 통한 스스로 생각하기
Vygotsky ZPD




부록 B 요약: 본 부록에서 제시한 모든 프롬프트는 2025년 10-11월 실제 운영 시스템(PostgreSQL DB)에서 추출되었으며, Dewey의 반성적 사고와 Bloom의 분류학을 기반으로 설계되었다. 특히 QC의 명료화 질문 전략은 Dewey 5단계를 직접 구현하여 학생의 문제 정의 능력을 촉진한다. 연구의 재현을 위해서는 상기 프롬프트 구조와 교육학적 원칙을 준수해야 한다.
III. 부록 C: 학생 사후설문지
A. 기본 정보
A1. 학생 정보
학번: ____ (예: S001)
성별: □ 남 □ 여

B. AI랑 대화한 경험
지난 2주간 AI 도구 쓰면서 어땠는지 솔직하게 평가해주세요.
(1점: 전혀 그렇지 않다 ↔ 5점: 매우 그렇다)

번호
문항
1
2
3
4
5
1
AI와 하면서 내가 뭘 모르는지 정확히 알게 됐다⭐
○
○
○
○
○
2
AI와 대화하다 보니 문제를 혼자 힘으로 풀 수 있었다⭐
○
○
○
○
○
3
AI와 대화한 뒤 다음엔 뭘 물어봐야 할지 알게 됐다⭐
○
○
○
○
○
4
AI가 내가 묻고 싶은 걸 잘 이해해줬다
○
○
○
○
○
5
AI가 공부하는 데 충분히 도움을 줬다
○
○
○
○
○

⭐ = 핵심 문항 (RQ2 직접 검증)
이론적 근거: 질문 명료화 이론 (King, 1994), 학습자 중심 대화 이론 (Chi et al., 2001)

C. 내 질문 습관과 공부 방법
여러분이 평소에 어떻게 질문하고 공부하는지 생각하면서 답해주세요.
(1점: 전혀 그렇지 않다 ↔ 5점: 매우 그렇다)

번호
문항
1
2
3
4
5
6
질문할 때 내가 뭘 알고 싶은지 분명하게 말한다 ⭐
○
○
○
○
○
7
질문할 때 "나는 지금 이런 상황이야"라고 설명도 같이 한다 ⭐
○
○
○
○
○
8
공부할 때 내가 뭘 모르는지 정확히 안다 ⭐
○
○
○
○
○
9
어려운 내용이 나오면 스스로 질문을 만들어본다 ⭐
○
○
○
○
○

⭐ = 핵심 문항 (질문 능력 & 메타인지 측정)

이론적 근거: 효과적 질문의 특징 (King, 1994), 메타인지 이론 (Flavell, 1979), 듀이의 반성적 사고 (Dewey, 1910)

D. 수학적 귀납법 얼마나 이해했어요?
다음 내용을 지금 얼마나 이해하고 있는지 솔직하게 체크해주세요.
(1점: 전혀 이해 못함 ↔ 5점: 완전히 이해함)

번호
문항
1
2
3
4
5
10
수학적 귀납법을 이용한 증명이 어떻게 구성되는지 이해했다
○
○
○
○
○
11
귀납 가정을 어떻게 써서 증명하는지 안다
○
○
○
○
○
12
내가 쓴 증명이 맞는지 스스로 확인할 수 있다
○
○
○
○
○

이론적 근거: 수학적 귀납법 학습 특성 (Harel & Sowder, 1998), Polya의 문제해결 이론 (1945)

E. 이 AI 도구 어땠어요?
이번에 썼던 AI 도구에 대해 평가해주세요.

(1점: 전혀 그렇지 않다 ↔ 5점: 매우 그렇다)

번호
문항
1
2
3
4
5
13
이 AI 도구가 수학 공부에 도움이 됐다
○
○
○
○
○
14
이 도구 사용하기 쉬웠다
○
○
○
○
○
15
앞으로도 이 도구 계속 쓰고 싶다
○
○
○
○
○


이론적 근거: 기술수용모델 (TAM, Davis, 1989)


F. 여러분의 솔직한 이야기 (자유롭게 써주세요)
여러분이 직접 겪은 일을 자유롭게 써주세요. 좋았던 것도, 별로였던 것도, 느낀 점도 다 소중해요. 솔직하게 써주시면 됩니다!


Part 1: AI와 대화한 경험
AI랑 대화했던 경험을 자유롭게 써주세요.

이런 것들을 포함해서 써주면 좋아요:
AI랑 대화하면서 기억에 남는 순간이 있나요?
AI가 여러분한테 질문을 한 적 있나요? 어떤 질문이었나요?
그게 도움이 됐나요? 아니면 귀찮았나요?
처음 쓸 때랑 나중에 쓸 때 느낌이 달랐나요?

___________________________________________________________________

___________________________________________________________________

___________________________________________________________________

Part 2: 학습 방식이 달라졌나요?
이 도구를 쓰기 전이랑 후를 비교해서 달라진 게 있나요?

이런 것들을 생각하면서 써주세요:

수학 공부할 때 질문하는 방식이 달라졌나요?
내가 뭘 모르는지 파악하는 방법이 달라졌나요?
문제 푸는 방식이나 생각하는 방법이 달라졌나요?
구체적인 예를 들어주면 더 좋아요!
답변:
___________________________________________________________________

___________________________________________________________________

___________________________________________________________________

___________________________________________________________________
Part 3: 어떤 방식이 더 좋아요?
다음 두 가지 중에 어떤 게 더 좋나요?

A 방식: AI가 바로바로 답을 알려줌
B 방식: AI가 질문을 해서 생각하게 만든 다음 답을 줌
어떤 걸 선택했는지, 왜 그렇게 생각하는지 자세히 써주세요.

선택: □ A 방식 선호   □ B 방식 선호   

이유:
___________________________________________________________________

___________________________________________________________________

___________________________________________________________________

___________________________________________________________________
Part 4: 가장 기억에 남는 순간
AI 도구 쓰면서 가장 기억에 남는 순간이 뭐였어요? 에피소드 하나를 자세히 써주세요.

(예시: "귀납법 문제 풀다가 막혔는데...", "AI랑 대화하다가 갑자기 이해됐어요...", "처음엔 이해 안 됐는데 나중에..." 등)

답변:
__________________________________________________________________-

___________________________________________________________________

___________________________________________________________________


Part 5: 이렇게 바뀌면 좋겠어요
이 도구를 더 좋게 만들려면 뭘 바꾸면 좋을까요?

불편했던 거, 아쉬웠던 거, 이런 기능 있었으면 좋겠다 싶은 거 뭐든지 써주세요!

답변:
___________________________________________________________________

___________________________________________________________________

___________________________________________________________________

설문 완료
소중한 시간을 내어 설문에 응답해주셔서 감사합니다!

여러분의 응답은 더 나은 AI 학습 도구 개발에 큰 도움이 될 것입니다.
IV. 부록 D: LLM 배치 채점 프롬프트 및 평가 방법
본 연구에서 Gemini 2.5 Flash, Claude 4.5 Haiku, GPT-5 mini 3개 모델의 배치 채점에 사용된 프롬프트 전문입니다.
1. 평가 프롬프트 개요
목적: 학생-MAICE 간 수학 학습 세션의 질문 품질, 답변 품질, 대화 맥락을 체계적으로 평가

◯ 평가 대상:
● 학생이 세션 내에 남긴 모든 질문(유저 메시지)
● MAICE(assistant)가 남긴 모든 답변(assistant 메시지)
● 전체 대화 흐름(맥락)

◯ 평가 방식:
● QAC 체크리스트 v4.3 (C1 제외, 8개 항목, 32개 체크리스트 요소)
● 40점 만점 (A영역 15점 + B영역 15점 + C영역 10점)
● 각 체크리스트 요소는 0(미충족) 또는 1(충족)로 평가
● 각 항목: 4개 체크리스트 × 1.25점 = 5점
2. 실제 프롬프트 전문
아래는 학생(유저)과 MAICE(assistant) 간 전체 수학 학습 세션 대화입니다.

- 학생이 세션 내에 남긴 모든 질문(유저 메시지)
- MAICE(assistant)가 남긴 모든 답변(assistant 메시지)
- 전체 대화 흐름(맥락)

을 통합적으로 고려해서 아래 루브릭 기준에 따라 채점하세요.
# 수학 학습 세션 평가 루브릭 (C1 제외 - 공정한 평가)
## 평가 개요
- **평가 대상**: 학생 질문(15점), AI 응답(15점), 대화 맥락(10점)
- **전체 총점**: 40점 만점 (C1 명료화 항목 제외)
- **척도**: 각 세부 항목 5점 만점 (1점: 매우 미흡, 5점: 매우 우수)



## A. 학생 질문 평가 (15점 만점)
### A1. 수학적 전문성 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- A1-1. ☐ **concept_accuracy** (수학적 개념/원리의 정확성): 수학 용어를 정확하게 사용했는가?
- A1-2. ☐ **curriculum_hierarchy** (교과과정 내 위계성 파악): 학년 수준에 맞는 개념인가?
- A1-3. ☐ **terminology_appropriateness** (수학적 용어 사용의 적절성): 전문 용어를 적절히 사용했는가?
- A1-4. ☐ **problem_direction_specificity** (문제해결 방향의 구체성): 해결하려는 문제가 구체적인가?

### A2. 질문 구조화 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- A2-1. ☐ **question_singularity** (핵심 질문의 단일성): 한 번에 하나의 명확한 질문을 하는가?
- A2-2. ☐ **condition_completeness** (조건 제시의 완결성): 필요한 조건/정보를 모두 제시했는가?
- A2-3. ☐ **sentence_logic** (문장 구조의 논리성): 문장이 논리적으로 구성되었는가?
- A2-4. ☐ **intent_clarity** (질문 의도의 명시성): 무엇을 알고 싶은지 명확한가?

### A3. 학습 맥락 적용 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- A3-1. ☐ **current_stage_description** (현재 학습 단계 설명): 학년/단원/진도를 언급했는가?
- A3-2. ☐ **prior_learning_mention** (선수학습 내용 언급): 이전에 배운 내용을 언급했는가?
- A3-3. ☐ **difficulty_specification** (구체적 어려움 명시): 어디서 막혔는지 구체적으로 말했는가?
- A3-4. ☐ **learning_goal_presentation** (학습 목표 제시): 무엇을 배우고 싶은지 목표를 제시했는가?



## B. AI 응답 평가 (15점 만점)
### B1. 학습자 맞춤도 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- B1-1. ☐ **level_based_approach** (학습자 수준별 접근): 학생 수준에 맞게 설명했는가?
- B1-2. ☐ **prior_knowledge_connection** (선수지식 연계성): 이미 배운 내용과 연결했는가?
- B1-3. ☐ **difficulty_adjustment** (학습 난이도 조절): 너무 어렵거나 쉽지 않은가?
- B1-4. ☐ **personalized_feedback** (개인화된 피드백): 학생 상황을 고려한 피드백인가?

### B2. 설명의 체계성 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- B2-1. ☐ **concept_hierarchy** (개념 설명의 위계화): 쉬운 것부터 어려운 것으로 단계적으로 설명했는가?
- B2-2. ☐ **stepwise_logic** (단계별 논리 전개): 각 단계가 논리적으로 연결되는가?
- B2-3. ☐ **key_emphasis** (핵심 요소 강조): 중요한 내용을 명확히 강조했는가?
- B2-4. ☐ **example_appropriateness** (예시 활용의 적절성): 이해를 돕는 적절한 예시를 제공했는가?

### B3. 학습 내용 확장성 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- B3-1. ☐ **advanced_direction** (심화학습 방향 제시): 더 깊이 공부할 방향을 제시했는가?
- B3-2. ☐ **application_connection** (응용문제 연계성): 관련된 응용 문제를 연결했는가?
- B3-3. ☐ **misconception_correction** (오개념 교정 전략): 잘못된 이해를 바로잡았는가?
- B3-4. ☐ **self_directed_induction** (자기주도 학습 유도): 스스로 탐구하도록 유도했는가?

## C. 대화 맥락 평가 (10점 만점)
### C1. 대화 일관성 및 연속성 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- C1-1. ☐ **goal_centered_consistency** (학습 목표 중심 일관성): 학습 목표를 벗어나지 않고 진행되는가?
- C1-2. ☐ **context_reference** (누적 맥락 참조): 전체 대화 이력을 기억하고 참조하는가? (멀리 떨어진 이전 대화 내용도 기억)
- C1-3. ☐ **topic_continuity** (주제 연속성): 주제가 자연스럽게 연결되는가?
- C1-4. ☐ **previous_turn_connection** (직전 턴 연결성): 각 발화가 바로 이전 턴과 유기적으로 연결되는가? (턴바이턴 흐름)

### C2. 학습 과정 지원성 (5점)
다음 **4가지 요소**를 체크하세요 (0=미충족, 1=충족):
- C2-1. ☐ **thinking_process_induction** (사고 과정 유도): 학생의 사고 과정을 유도하는가?
- C2-2. ☐ **understanding_check** (이해도 확인): 학생의 이해도를 확인하는가?
- C2-3. ☐ **metacognitive_promotion** (메타인지 촉진): 학생이 자신의 학습 과정을 돌아보게 하는가?
- C2-4. ☐ **deep_thinking_guidance** (깊이 있는 사고 유도): 단순 암기를 넘어 깊이 있는 사고를 유도하는가?

## 평가 지침
- 각 세부 요소는 **0 (미충족)** 또는 **1 (충족)**로만 체크
- 세션 전체에서 **가장 우수한 질문** 기준으로 A 영역 평가
- 점수 합산은 시스템이 자동으로 처리

**🚨 중요: B영역 최저점 규칙 (교사 평가 기준)**
- AI 답변이 **수학적 내용으로 이어지지 못한 경우** B1, B2, B3 모든 항목 **0개 충족** 처리
- 단순 격려("잘하고 있어요", "화이팅!"), 일반적 대화만 반복하고 수학 설명이 없는 경우
- 이 경우 B영역 총점은 3점(각 항목 1점씩)이 됨


## 3. 응답 형식

반드시 아래 JSON 형식으로만 응답 (각 요소에 value와 evidence 포함):

```json
{
  "A1_math_expertise": {
    "concept_accuracy": {
      "value": 1, 
      "evidence": "메시지[2]에서 '이차함수 y=x^2의 꼭짓점과 축의 방정식'이라고 표현하며 수학 용어를 정확히 사용"
    },
    "curriculum_hierarchy": {
      "value": 0, 
      "evidence": "전체 대화에서 중2, 고1 등 학년 정보나 '함수 단원', '이차함수 파트' 등 교육과정 위계 언급이 전혀 없음"
    },
    "terminology_appropriateness": {
      "value": 1, 
      "evidence": "메시지[2,4]에서 '꼭짓점', '축의 방정식', '표준형' 등 교과서 수준의 적절한 수학 용어 사용"
    },
    "problem_direction_specificity": {
      "value": 1, 
      "evidence": "메시지[0]에서 '이차함수 그래프를 그리고 싶어요'라고 구체적인 문제 해결 목표를 명시"
    }
  },
  "A2_question_structure": {
    "question_singularity": {
      "value": 1, 
      "evidence": "각 학생 메시지마다 '그래프 그리기', '꼭짓점 구하기' 등 하나의 명확한 질문에만 집중"
    },
    "condition_completeness": {
      "value": 1, 
      "evidence": "메시지[2]에서 함수식 y=x^2-4x+3을 제시하며 문제 해결에 필요한 조건을 완전히 제공"
    },
    "sentence_logic": {
      "value": 0, 
      "evidence": "메시지[4]에서 '그래프 어떻게... 꼭짓점도...' 식으로 문장이 완결되지 않고 단편적"
    },
    "intent_clarity": {
      "value": 1, 
      "evidence": "메시지[0,2,4] 모두에서 '~하고 싶어요', '~구하는 방법은?' 등 질문 의도가 명확하게 표현됨"
    }
  },
  "A3_learning_context": {
    "current_stage_description": {
      "value": 0, 
      "evidence": "전체 대화에서 현재 진도, 단원, 학년 등 학습 단계 정보가 전혀 언급되지 않음"
    },
    "prior_learning_mention": {
      "value": 0, 
      "evidence": "일차함수, 좌표평면 등 이미 배운 선수학습 내용에 대한 언급이 없음"
    },
    "difficulty_specification": {
      "value": 1, 
      "evidence": "메시지[4]에서 '꼭짓점 구하는 공식은 알겠는데 그래프 그리는 게 어려워요'라고 구체적 어려움 명시"
    },
    "learning_goal_presentation": {
      "value": 0, 
      "evidence": "'이차함수 완전 정복', '시험 대비' 등 명확한 학습 목표 제시가 없음"
    }
  },
  "B1_learner_customization": {
    "level_based_approach": {
      "value": 1, 
      "evidence": "메시지[3]에서 MAICE가 '먼저 일차 계수를 보고...'라며 학생이 이해할 수 있는 쉬운 단계부터 설명"
    },
    "prior_knowledge_connection": {
      "value": 1, 
      "evidence": "메시지[5]에서 'y=ax^2 그래프 형태는 이미 배웠죠?'라며 선수지식과 연결"
    },
    "difficulty_adjustment": {
      "value": 1, 
      "evidence": "메시지[3,5,7]에서 단계별로 난이도를 조절하며 너무 어렵지도 쉽지도 않게 설명"
    },
    "personalized_feedback": {
      "value": 0, 
      "evidence": "학생 개별 상황이나 실수 패턴을 고려한 맞춤형 피드백 없이 일반적 설명만 제공"
    }
  },
  "B2_explanation_systematicity": {
    "concept_hierarchy": {
      "value": 1, 
      "evidence": "메시지[3]에서 '1단계: 표준형 변환 → 2단계: 꼭짓점 찾기 → 3단계: 그래프 그리기' 순으로 체계적 설명"
    },
    "stepwise_logic": {
      "value": 1, 
      "evidence": "메시지[5,7,9]가 논리적으로 연결되며 각 단계가 다음 단계의 기반이 됨"
    },
    "key_emphasis": {
      "value": 1, 
      "evidence": "메시지[3]에서 '가장 중요한 건 꼭짓점입니다'라고 핵심 개념을 명시적으로 강조"
    },
    "example_appropriateness": {
      "value": 1, 
      "evidence": "메시지[7]에서 y=(x-2)^2-1 구체적 예시를 들어 꼭짓점(2,-1) 찾기 과정을 상세히 설명"
    }
  },
  "B3_learning_expandability": {
    "advanced_direction": {
      "value": 0, 
      "evidence": "이차함수 최댓값/최솟값 문제, 이차방정식 연계 등 심화 방향 제시 없음"
    },
    "application_connection": {
      "value": 0, 
      "evidence": "포물선 운동, 최적화 문제 등 실생활 응용이나 다른 단원 연계 없음"
    },
    "misconception_correction": {
      "value": 1, 
      "evidence": "메시지[9]에서 '축이 y축이 아니라 x=2입니다'라며 학생의 오개념을 명확히 교정"
    },
    "self_directed_induction": {
      "value": 0, 
      "evidence": "'스스로 해보세요', '다른 예제도 풀어볼까요?' 등 자기주도 탐구 유도 없음"
    }
  },
  "C1_dialogue_coherence": {
    "goal_centered_consistency": {
      "value": 1, 
      "evidence": "메시지[0]의 '그래프 그리기' 목표가 메시지[11]까지 일관되게 유지되며 주제 이탈 없음"
    },
    "context_reference": {
      "value": 1, 
      "evidence": "메시지[7]에서 '아까 말한 y=x^2-4x+3을 다시 보면'이라며 멀리 떨어진 이전 대화[2] 내용을 기억하고 참조 (누적 맥락 유지)"
    },
    "topic_continuity": {
      "value": 1, 
      "evidence": "그래프→꼭짓점→표준형→축 순으로 주제가 자연스럽게 확장되며 맥락 유지"
    },
    "previous_turn_connection": {
      "value": 0, 
      "evidence": "메시지[7]에서 바로 직전 학생 발화[6]의 '그래프 그리는 게 어려워요'라는 구체적 어려움을 직접 언급하지 않고 일반적 설명으로 넘어감 (턴바이턴 흐름 단절)"
    }
  },
  "C2_learning_support": {
    "thinking_process_induction": {
      "value": 1, 
      "evidence": "메시지[5]에서 '왜 표준형으로 바꿔야 할까요? 생각해봅시다'라며 학생의 사고 과정 유도"
    },
    "understanding_check": {
      "value": 0, 
      "evidence": "'이해했나요?', '직접 풀어볼까요?' 등 학생 이해도 확인 질문이 전혀 없음"
    },
    "metacognitive_promotion": {
      "value": 0, 
      "evidence": "'어떤 부분이 어려웠나요?', '어떻게 접근했나요?' 등 학습 과정 성찰 유도 없음"
    },
    "deep_thinking_guidance": {
      "value": 1, 
      "evidence": "메시지[9]에서 '왜 a값에 따라 그래프 모양이 달라질까?'라며 본질적 이해를 위한 깊이 있는 질문 제시"
    }
  }
}
중요:

각 요소는 {"value": 0 또는 1, "evidence": "구체적인 근거 문장"} 형식
evidence는 실제 대화에서 인용한 구체적 근거를 포함 (30-100자 권장)
근거 작성 방법:
value=1인 경우: 실제 대화 내용을 인용하여 충족 근거 제시 (예: "메시지[3]에서 '이차함수의 꼭짓점을 구하는 방법' 질문")
value=0인 경우: 왜 충족하지 못했는지 구체적 이유 (예: "전체 대화에서 학년/단원 정보 언급 없음")
JSON 외 부가 텍스트 금지!
= 세션 대화 내용 =
[여기에 실제 세션 대화가 삽입됨]



## 4. 점수 산출 방식
각 항목(A1~A3, B1~B3, C1~C2)별로:

체크된 요소 개수 (0~4개)에 따라 점수 산출:

0개 충족 → 1점
1개 충족 → 2점
2개 충족 → 3점
3개 충족 → 4점
4개 충족 → 5점
공식: 점수 = (충족된 체크리스트 개수) + 1

**영역별 합산**:
- A영역 (15점): A1 (5점) + A2 (5점) + A3 (5점)
- B영역 (15점): B1 (5점) + B2 (5점) + B3 (5점)
- C영역 (10점): C1 (5점) + C2 (5점)
- **총점 (40점)**



## 5. 실제 적용 예시
**세션 예시**:
[0] 학생: 이차함수 그래프를 그리고 싶어요
[1] MAICE: 어떤 이차함수인가요?
[2] 학생: y=x^2-4x+3이요
[3] MAICE: 1단계로 표준형으로 바꿔봅시다. y=(x-2)^2-1입니다.
...

참고문헌
Anderson, L. W., & Krathwohl, D. R. (Eds.). (2001). A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives (Complete ed.). Addison Wesley Longman.

Bloom, B. S. (Ed.). (1956). Taxonomy of educational objectives: The classification of educational goals. Handbook I: Cognitive domain. New York: David McKay.

Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. Qualitative Research in Psychology, 3(2), 77-101. https://doi.org/10.1191/1478088706qp063oa

Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates.

Collins, A., Joseph, D., & Bielaczyc, K. (2004). Design research: Theoretical and methodological issues. Journal of the Learning Sciences, 13(1), 15-42. https://doi.org/10.1207/s15327809jls1301_2

Degen, B. (2025). Resurrecting Socrates in the Age of AI: A study protocol for evaluating a Socratic tutor to support research question development in higher education. [Manuscript in preparation]

Dewey, J. (1910). How we think. D.C. Heath & Co. https://doi.org/10.1037/10903-000

Graesser, A. C., & Person, N. K. (1994). Question asking during tutoring. American Educational Research Journal, 31(1), 104-137. https://doi.org/10.3102/00028312031001104
Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81-112. https://doi.org/10.3102/003465430298487

Hattie, J. (2009). Visible learning: A synthesis of over 800 meta-analyses relating to achievement. Routledge. https://doi.org/10.4324/9780203887332

King, A. (1994). Guiding knowledge construction in the classroom: Effects of teaching children how to question and how to explain. American Educational Research Journal, 31(2), 338-368. https://doi.org/10.3102/00028312031002338

Krathwohl, D. R. (2002). A revision of Bloom's taxonomy: An overview. Theory Into Practice, 41(4), 212-218. https://doi.org/10.1207/s15430421tip4104_2

Schoenfeld, A. H. (1985). Mathematical problem solving. Orlando, FL: Academic Press.

Wang, F., & Hannafin, M. J. (2005). Design-based research and technology-enhanced learning environments. Educational Technology Research and Development, 53(4), 5-23. https://doi.org/10.1007/BF02504682

Wooldridge, M., & Jennings, N. R. (1995). Intelligent agents: Theory and practice. The Knowledge Engineering Review, 10(2), 115-152. https://doi.org/10.1017/S0269888900008122

홍경선, 김동익. (2011). 공학교육에서 학생 생성 질문 교수학습방법을 적용한 수업 사례연구. 공학교육연구, 14(6), 24-30.

Design and Development of an AI Agent Supporting Question Clarification in Mathematics Learning: Focusing on Mathematical Induction for High School Grade 2

Kim Kyubong

Major in AI Convergence Education 
Graduate School of Education
 Pusan National University

Abstract


Despite the widespread adoption of generative AI in education, poor question quality hinders effective learning. A pilot study (n=385) found that 72.3% of student questions lacked learning context, and current immediate-answer approaches (termed "Freepass" mode) fail to support students' thinking processes. Question quality strongly correlated with answer quality (r=0.691, p<0.001), indicating that question clarification is a key mechanism for improving learning outcomes.

This study designed and developed MAICE (Mathematical AI Chatbot for Education), a multi-agent system based on Dewey's reflective thinking theory (1910) and Bloom's knowledge taxonomy (Anderson & Krathwohl, 2001). MAICE employs five independent AI agents (QuestionClassifier, QuestionImprover, AnswerGenerator, LearningObserver, FreeTalker) that collaborate to classify questions into Bloom's K1-K4 types (factual-conceptual-procedural-metacognitive knowledge), systematically clarify unclear questions following Dewey's five-stage reflective thinking process, and provide differentiated answers tailored to question types.

Methods: Fifty-eight grade 2 high school students were randomly assigned to Agent mode (clarification-included, n=28) or Freepass mode (immediate-answer only, n=30) in a three-week A/B test (October 20-November 8, 2024; 284 valid sessions). To mutually complement methodological limitations, we employed dual evaluation: LLM evaluation (N=284) for large-scale pattern detection and teacher preliminary evaluation (N=100) for educational validity verification. A QAC (Question-Answer-Context) checklist with 8 items (40 points) was evaluated by three independent AI models (Gemini-2.5-Flash, Claude-4.5-Haiku, GPT-5-mini) and two external mathematics teachers. Inter-rater reliability: LLM Cronbach's α=0.868, teachers r=0.644, LLM-teacher r=0.743 (p<0.001).

Results: Clarification effects were mutually verified through LLM-teacher dual evaluation.

(1) LLM evaluation (N=284, 3-model average): Agent mode showed significant superiority in C2 (learning support) (p=0.002, d=0.376) with very large effects for lower-achievers (Q1 C2: p=0.001, d=0.840; Q1 overall: +2.46 points, p=0.033, d=0.511).

(2) Teacher evaluation (N=100): Agent mode was significantly higher (+2.25 points, p=0.031, d=0.307) with very large effects for Q1 (+6.91 points, p=0.009, d=1.117). High LLM-teacher correlation (r=0.743) confirmed directional consistency. However, teacher evaluation is preliminary (2 evaluators, 100 samples); replication with 10+ evaluators and 300+ samples is needed.

(3) Qualitative analysis (1,589 dialogue logs): High-scoring sessions demonstrated high implementation of Bloom's higher-order thinking (100% reached Analyze, 75% reached Evaluate) and Dewey's reflective thinking (average 4.0/5 stages). Session 414 (Q1 lower-achiever) showed Bloom 4-stage progression and Dewey 5/5 complete implementation, demonstrating the qualitative mechanism of lower-achiever effects.

(4) Student self-assessment (N=40): 68.6% preferred clarification approach ("improves thinking" 42%, "deeper understanding" 25%), converging with objective evaluations. High scores in AI interaction quality (4.38/5.0), concept understanding (4.36/5.0), and questioning ability (4.13/5.0).

Conclusions: Question clarification processes enhance learning support, particularly for lower-achieving students (LLM d=0.840, teacher d=1.117). Four independent evidence sources (LLM objective evaluation, teacher expert evaluation, learner self-assessment, qualitative evidence) converged to strengthen validity. This study: (1) validated Dewey's "problem clarification" stage as an effective method through A/B testing (LLM p=0.002, teacher p=0.031), (2) demonstrated practical effects in supporting lower-achieving students by shifting AI educational tool design from immediate-answer centered to question-clarification centered, and (3) presented an LLM-teacher dual evaluation model combining large-scale objective assessment with expert validity verification, along with an extensible research platform enabling teacher-led prompting design. However, participants from a software development-specialized high school with extensive AI experience may limit generalization to students with less AI familiarity.

Keywords: question clarification, AI agent, reflective thinking, mathematical induction, multi-agent system, Dewey, educational gap reduction, teacher-led research, prompting design