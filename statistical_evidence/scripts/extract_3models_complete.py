#!/usr/bin/env python3
"""
3ê°œ LLM ëª¨ë¸ì˜ ì„¸ë¶€í•­ëª©(A1~C2) + ì´ì  ì™„ì „ ì¶”ì¶œ
JSONLì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ì§ì ‘ íŒŒì‹±

ì¶œë ¥: llm_3models_284sessions_COMPLETE.csv
- session_id
- gemini_A1, gemini_A2, ..., gemini_C2, gemini_total
- anthropic_A1, anthropic_A2, ..., anthropic_C2, anthropic_total
- openai_A1, openai_A2, ..., openai_C2, openai_total
- avg_A1, avg_A2, ..., avg_C2, avg_total
"""

import json
import re
import pandas as pd
import numpy as np
from pathlib import Path

print("=" * 80)
print("3ê°œ ëª¨ë¸ ì„¸ë¶€í•­ëª© + ì´ì  ì™„ì „ ì¶”ì¶œ")
print("=" * 80)

# íŒŒì¼ ê²½ë¡œ
BASE_DIR = Path(__file__).parent.parent.parent
DATA_DIR = BASE_DIR / 'analysis' / 'threemodel'

files = {
    'gemini': DATA_DIR / 'gemini_results_20251105_174045.jsonl',
    'anthropic': DATA_DIR / 'anthropic_haiku45_results_20251105.jsonl',
    'openai': DATA_DIR / 'openai_gpt5mini_results_20251105.jsonl'
}

def parse_jsonl_with_items(file_path, model_name):
    """JSONLì—ì„œ ì„¸ë¶€í•­ëª© + ì´ì  ì¶”ì¶œ (ìŠ¤í¬ë¦½íŠ¸ ë°©ì‹)"""
    results = []
    failed = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line)
                
                # ì„¸ì…˜ ID ì¶”ì¶œ (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
                if model_name == 'gemini':
                    custom_id = data.get('metadata', {}).get('key', '')
                else:
                    custom_id = data.get('custom_id', '')
                
                session_id = custom_id.replace('session_', '')
                
                if not session_id:
                    failed += 1
                    continue
                
                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
                text = None
                
                if model_name == 'gemini':
                    response = data.get('response', {})
                    candidates = response.get('candidates', [])
                    if isinstance(candidates, list) and len(candidates) > 0:
                        text = candidates[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                    else:
                        text = ''
                
                elif model_name == 'anthropic':
                    content = data.get('result', {}).get('message', {}).get('content', [])
                    if isinstance(content, list) and len(content) > 0:
                        text = content[0].get('text', '')
                    else:
                        text = str(content)
                
                elif model_name == 'openai':
                    body = data.get('response', {}).get('body', {})
                    if 'choices' in body:
                        content = body.get('choices', [])
                        if isinstance(content, list) and len(content) > 0:
                            text = content[0].get('message', {}).get('content', '')
                        else:
                            text = ''
                    else:
                        text = str(body)
                
                if not text:
                    failed += 1
                    continue
                
                # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
                text = re.sub(r'```json\s*', '', text)
                text = re.sub(r'```\s*$', '', text)
                text = text.strip()
                
                # 8ê°œ í•­ëª© ì ìˆ˜ ì¶”ì¶œ
                item_mapping = [
                    ('A1', 'A1_math_expertise'),
                    ('A2', 'A2_question_structure'),
                    ('A3', 'A3_learning_context'),
                    ('B1', 'B1_learner_customization'),
                    ('B2', 'B2_explanation_systematicity'),
                    ('B3', 'B3_learning_expandability'),
                    ('C1', 'C1_dialogue_coherence'),
                    ('C2', 'C2_learning_support')
                ]
                
                total_40 = 0
                item_scores = {'session_id': session_id}
                
                for item_key, full_name in item_mapping:
                    # í•­ëª© ì˜ì—­ ì°¾ê¸°
                    start_pattern = f'"{full_name}"'
                    start_pos = text.find(start_pattern)
                    
                    if start_pos == -1:
                        # í•­ëª© ëª» ì°¾ìœ¼ë©´ ìµœì†Œì ìˆ˜
                        item_scores[item_key] = 1
                        total_40 += 1
                        continue
                    
                    # ë‹¤ìŒ í•­ëª© ìœ„ì¹˜ ì°¾ê¸°
                    next_pos = len(text)
                    for _, next_name in item_mapping:
                        if next_name == full_name:
                            continue
                        pos = text.find(f'"{next_name}"', start_pos + 10)
                        if pos != -1 and pos < next_pos:
                            next_pos = pos
                    
                    # í•­ëª© í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    item_text = text[start_pos:next_pos]
                    
                    # value ê°’ ì¶”ì¶œ (ì •í™•íˆ 4ê°œ)
                    values = re.findall(r'"value"\s*:\s*(\d+)', item_text)
                    checked_count = sum(int(v) for v in values[:4])  # ìµœëŒ€ 4ê°œë§Œ
                    score = checked_count + 1  # 0ê°œ=1ì , 1ê°œ=2ì , ..., 4ê°œ=5ì 
                    
                    item_scores[item_key] = score
                    total_40 += score
                
                item_scores['total'] = total_40
                results.append(item_scores)
                
            except Exception as e:
                failed += 1
                continue
    
    df = pd.DataFrame(results)
    print(f"  {model_name:12}: {len(df):3}ê°œ íŒŒì‹± ì„±ê³µ, {failed:3}ê°œ ì‹¤íŒ¨")
    
    return df

# ê° ëª¨ë¸ íŒŒì‹±
print("\níŒŒì‹± ì¤‘...")
df_gemini = parse_jsonl_with_items(files['gemini'], 'gemini')
df_anthropic = parse_jsonl_with_items(files['anthropic'], 'anthropic')
df_openai = parse_jsonl_with_items(files['openai'], 'openai')

# ì»¬ëŸ¼ëª…ì— ëª¨ë¸ëª… prefix ì¶”ê°€
item_cols = ['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2', 'total']

df_gemini = df_gemini.rename(columns={col: f'gemini_{col}' for col in item_cols})
df_anthropic = df_anthropic.rename(columns={col: f'anthropic_{col}' for col in item_cols})
df_openai = df_openai.rename(columns={col: f'openai_{col}' for col in item_cols})

# ë³‘í•©
df_merged = df_gemini.merge(df_anthropic, on='session_id', how='outer')
df_merged = df_merged.merge(df_openai, on='session_id', how='outer')

# session_idë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
df_merged['session_id'] = pd.to_numeric(df_merged['session_id'])
df_merged = df_merged.sort_values('session_id').reset_index(drop=True)

print(f"\në³‘í•© ì™„ë£Œ: {len(df_merged)} ì„¸ì…˜")

# 3ê°œ ëª¨ë¸ í‰ê·  ê³„ì‚°
items = ['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2', 'total']

for item in items:
    cols = [f'gemini_{item}', f'anthropic_{item}', f'openai_{item}']
    df_merged[f'avg_{item}'] = df_merged[cols].mean(axis=1)

# ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
col_order = ['session_id']

# í•­ëª©ë³„ë¡œ gemini, anthropic, openai, avg ìˆœì„œ
for item in ['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2']:
    col_order.extend([f'gemini_{item}', f'anthropic_{item}', f'openai_{item}', f'avg_{item}'])

# ì´ì 
col_order.extend(['gemini_total', 'anthropic_total', 'openai_total', 'avg_total'])

df_final = df_merged[col_order]

# ì €ì¥
output_file = BASE_DIR / 'statistical_evidence' / 'data' / 'llm_evaluations' / 'llm_3models_284sessions_COMPLETE.csv'
df_final.to_csv(output_file, index=False, encoding='utf-8-sig')

print(f"\nâœ… ì €ì¥ ì™„ë£Œ!")
print(f"   íŒŒì¼: {output_file.name}")
print(f"   ê²½ë¡œ: {output_file.parent}")

print(f"\nğŸ“Š ë°ì´í„° êµ¬ì¡°:")
print(f"   ì„¸ì…˜ ìˆ˜: {len(df_final)}")
print(f"   ì»¬ëŸ¼ ìˆ˜: {len(df_final.columns)}")

print(f"\nì»¬ëŸ¼ êµ¬ì„±:")
print(f"   - session_id: 1ê°œ")
print(f"   - ì„¸ë¶€í•­ëª© (A1~C2): 8ê°œ Ã— 4 (gemini, anthropic, openai, avg) = 32ê°œ")
print(f"   - ì´ì  (total): 1ê°œ Ã— 4 = 4ê°œ")
print(f"   - í•©ê³„: 37ê°œ ì»¬ëŸ¼")

print(f"\nì²« 3ê°œ ì„¸ì…˜ (ì´ì ë§Œ):")
print(df_final[['session_id', 'gemini_total', 'anthropic_total', 'openai_total', 'avg_total']].head(3))

print(f"\níŒŒì¼ ê²½ë¡œ:")
print(f"  {output_file}")

EOF

