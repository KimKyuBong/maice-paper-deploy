---
tags: [논문/서론, 논문/1장]
alias: [서론, 1장]
parent: "수학 학습에서 질문 명료화를 지원하는 AI에이전트 설계 및 개발 고등학교 2학년 수학적 귀납법 단원 중심으로"
---

# I. 서론

## 1. 연구의 필요성

### 가. 수학 교육에서 생성형 AI 활용의 확산과 한계

대규모 언어 모델(Large Language Model, LLM)의 비약적 발전에 따라 ChatGPT, Claude 등 생성형 AI 도구가 교육 현장에 빠르게 도입되고 있다. 특히 수학 교육 영역에서는 학생들이 개념 이해, 문제 풀이, 추가 학습 등의 목적으로 LLM을 활용하는 빈도가 급증하고 있으며, 일부 교사들은 이를 보조 학습 도구로 활용하려는 시도를 하고 있다.

그러나 실제 교실 현장에서 LLM을 보조교사로 활용하려는 시도 과정에서, 교육적 효과성에 대한 근본적인 문제들이 지속적으로 관찰되었다. 현재 대부분의 LLM 기반 AI 도구(ChatGPT, Claude, Gemini 등)는 학생의 질문에 대해 **질문의 질이나 맥락과 무관하게 즉각적으로 답변을 제공**한다. 이러한 즉시 답변 제공 방식은 신속한 정보 제공이라는 장점이 있으나, 학습자가 스스로 질문을 구조화하고 사고를 확장하는 과정을 지원하지 못하여 깊이 있는 학습을 저해할 가능성이 제기되고 있다.

### 나. 학생 질문의 질적 문제: 실증 분석

본 연구에서는 MAICE 시스템 설계에 앞서, 현재 일반적인 LLM 사용 방식의 교육적 문제점을 실증적으로 파악하기 위해 2024년 5월 중순(학기 초)에 예비 조사를 실시하였다. 예비 조사는 다음의 순차적 과정으로 진행되었다:

**1단계: 데이터 수집** - 고등학교 수학 수업 환경에서 학생들에게 ChatGPT와 동일한 사용자 인터페이스를 제공하는 간단한 웹앱을 배포하였다. 수업 시간 동안 학생들이 자유롭게 질문하고 AI 답변을 받을 수 있도록 하여 총 385건의 실제 질문-답변 쌍을 수집하였다.

**2단계: 문제점 파악 및 루브릭 구성** - 수집된 데이터를 분석한 결과, 학생 질문과 AI 답변에서 반복적으로 나타나는 질적 문제 패턴들이 관찰되었다. 이러한 문제점들을 명료화하고 그 존재 여부를 객관적으로 판단하기 위해, 전통적 교육학 이론(Dewey의 반성적 사고, Bloom의 교육목표분류학 등)과 최신 AI 교육 평가 연구를 종합하여 체계적인 분석적 채점 기준(analytic rubric)을 개발하였다.

관찰된 구체적 문제 패턴에 대응하여 6개 평가 영역을 구성하였다:

**[표 1-1] 예비조사 루브릭 6개 평가 영역 구성**

| 구분 | 평가 영역 | 코드 | 평가 내용 |
|------|----------|:----:|----------|
| **질문<br>평가** | 수학적 전문성 | A1 | 수학 개념의 정확성, 교과과정 내 위치, 용어 사용의 적절성 |
| | 질문 구조화 | A2 | 질문 대상·범위·초점의 명확성, 구체적 어려움 제시 |
| | 학습 맥락 적용 | A3 | 학습자 수준, 선수지식, 학습 목적, 이해 상태 명시 |
| **AI 답변<br>평가** | 학습자 맞춤도 | B1 | 학습자 수준 파악, 선수지식 연계, 난이도 조절, 개인화된 피드백 |
| | 설명의 체계성 | B2 | 개념의 위계적 설명, 단계별 논리 전개, 핵심 요소 강조, 예시 활용 |
| | 학습 내용 확장성 | B3 | 심화학습 방향 제시, 응용문제 연계, 오개념 교정, 자기주도 학습 유도 |

각 영역은 **5점 리커트 척도(1=매우 부족 ~ 5=매우 우수)**로 평가되며, 교사가 6개 영역 각각에 대해 전체적인 판단을 바탕으로 1~5점을 직접 부여한다. 평가를 돕기 위해 각 영역별로 4개의 세부 평가 요소(총 24개)를 참고 가이드로 제시하였으나, 최종 점수는 교사의 종합적 판단에 따른다. 질문 영역 15점, 답변 영역 15점으로 총 30점 만점이다.

(루브릭 개발의 상세한 이론적 기반과 각 문항 구성 근거는 2장 이론적 배경 및 부록 A 참조)

**3단계: 동료교사 평가 실시** - 중등 수학교사 4명(교직 경력: 1년차 2명, 5년차 1명, 7년차 1명, 평균 3.5년)이 독립적으로 385건의 질문-답변을 6개 영역 각각에 대해 1~5점으로 평가하였다.

**4단계: 평가 결과 분석 및 주요 발견** - 교사 루브릭 평가 결과, 일반적인 LLM 사용 방식의 구체적 문제점들이 실증적으로 확인되었다:

**학생 질문의 문제점**:
- **수학적 전문성 (A1)**: 평균 2.277점, **45.5%가 최저점(1점)**
- **질문 구조화 (A2)**: 평균 2.049점, **45.8%가 최저점(1점)**
- **학습 맥락 적용 (A3)**: 평균 1.500점, **72.3%가 최저점(1점)** (가장 심각)

**AI 답변의 문제점**:
- **학습자 맞춤도 (B1)**: 평균 2.474점, 27.6%가 최저점(1점)
- **설명의 체계성 (B2)**: 평균 2.765점, 23.9%가 최저점(1점)
- **학습 내용 확장성 (B3)**: 평균 1.832점, **48.9%가 최저점(1점)** (AI 답변 중 가장 심각)

**핵심 발견 1**: 질문 품질과 답변 품질 간 **강한 상관관계 r=0.691 (p<0.001)** 확인. 이는 **질문 개선이 학습 효과 향상의 핵심 메커니즘**임을 시사하며, MAICE의 질문 명료화 기반 시스템 설계의 실증적 근거가 되었다.

**핵심 발견 2**: 교사 간 평가 일치도가 낮음 (평균 r=0.28, ICC(2,k)=0.29). 특히 수학적 전문성과 학습확장성 영역에서 평가 차이가 컸다(r=0.01~0.26). 이러한 낮은 일치도는 교사의 종합적 판단에 의한 점수 부여 방식의 주관성에서 비롯되었다. 이는 본 실험을 위한 더 객관적이고 신뢰성 높은 평가 도구 개발의 필요성을 시사하였다.

교사 루브릭 평가 결과, 학생 질문의 질적 문제가 심각한 수준임이 확인되었다. 다음 표는 각 평가 영역별 결과를 보여준다:

**[표 1-2] 예비조사 루브릭 평가 결과 (질문 및 AI 답변)**

| 평가 영역 | 평균 점수 (5점 만점) | 최저점(1점) 비율 | 중앙값 |
|----------|---------------------|-----------------|--------|
| **질문 평가** | | | |
| 수학적 전문성 | 2.277 | 45.5% | 2.0 |
| 질문 구조화 | 2.049 | 45.8% | 2.0 |
| 학습 맥락 적용 | **1.500** | **72.3%** | **1.0** |
| **AI 답변 평가** | | | |
| 학습자 맞춤도 | 2.474 | 27.6% | 3.0 |
| 설명의 체계성 | 2.765 | 23.9% | 3.0 |
| 학습 내용 확장성 | **1.832** | **48.9%** | **2.0** |

**[표 1-3] 학생 질문 문제 유형별 분석**

| 문제 유형 | 해당 영역 | 발생 비율 | 사례 수 | 주요 문제점 |
|----------|----------|:--------:|:------:|------------|
| 맥락 없는 질문 | 학습 맥락 | 72.3% | 184건 | 학습자 수준·목적 미제시 |
| 모호한 질문 | 질문 구조화 | 45.8% | 25건 | 질문 대상·범위 불명확 |
| 수학 무관 질문 | 수학적 전문성 | 45.5% | 26건 | 학습 집중도 저하 |

[표 1-2]와 [표 1-3]에서 확인할 수 있듯이, 학생 질문에서 **학습 맥락 부재**가 가장 심각한 문제였다. 대표적 사례는 다음과 같다:

**맥락 부재 사례 (ID 294):** "근데 어떻게 증명한거야?"
- 어떤 증명인지, 어느 부분이 어려운지, 학습자 수준이 무엇인지 전혀 파악 불가
- AI도 맥락을 알 수 없어 추가 정보 요청만 가능

**질문 모호 사례 (ID 142):** "내 증명에대해서 피드백해줘"
- 어떤 증명을, 어느 부분을 평가받고 싶은지 불명확

**수학 무관 사례 (ID 315):** "토푸리아 마카체프 5분 5라운드..."
- 수학 학습과 완전 무관, 학습 집중도 저하

**대조: 우수 질문 사례 (ID 358):** "나는 소프트웨어마이스터고에 다니는 고등학교 2학년이야 현재 수학을 공부하고 있는데 0!이 1인 이유를 알기 쉽게 알려줘."
- 학습자 신분·수준·현재 상황·구체적 질문·설명 방식 요청이 모두 포함 (교사 평가: 10.0/15점 vs 불량 질문 3.0점)

### 다. AI 답변의 질적 문제: 교사 루브릭 평가 결과

학생 질문에 대한 LLM의 답변을 동일한 교사 패널이 개발한 루브릭으로 평가한 결과, 심각한 교육적 한계가 확인되었다.

**[표 1-4] AI 답변 문제 유형별 분석**

| 문제 유형 | 해당 영역 | 평균 점수 | 최저점 비율 | 주요 문제점 |
|----------|----------|:--------:|:----------:|------------|
| 학습 확장 부족 | 학습 확장성 | 1.832 | 48.9% | 심화·응용·자기주도 유도 실패 |
| 맞춤화 실패 | 학습자 맞춤도 | 2.474 | 27.6% | 수준 파악·난이도 조절 실패 |
| 체계성 부족 | 설명 체계성 | 2.765 | 23.9% | 교육과정 용어·위계 무시 |

[표 1-4]에서 확인할 수 있듯이, AI 답변에서 **학습 확장성 결여(평균 1.832점, 48.9% 최저점)**가 가장 심각한 문제였으며, 맞춤화 실패와 체계성 부족도 두드러졌다. 대표적 사례는 다음과 같다:

**맥락 오해 사례 (ID 130):** "지수의 확장에 대해 설명해줘"
- AI가 수학 "지수(exponent)"를 비즈니스 "지점 확장"으로 오해하여 완전히 무관한 답변 제공

**수준 불일치 사례 (ID 158):** 고1 학생이 "로지스틱 회귀" 질문
- AI가 대학 통계학 수준의 "odds ratio", "최대우도추정" 개념 사용

**교육과정 용어 부적절 (ID 202):** "수열이 뭐야?"
- AI가 한국 표준 용어 "일반항" 대신 "명시적 공식(Explicit Formula)" 사용
- 교사-학생 간 의사소통 혼란 초래

**확장성 부족 사례 (ID 145):** "수1의 핵심 부분이 뭐야?"
- 단순 목차 나열만 제공, 학생이 왜 질문했는지 파악 없음
- 심화 방향 제시나 자기주도 학습 유도 없음

### 라. 프리패스 방식의 구조적 한계와 MAICE 시스템 설계의 필요성

**[표 1-5] 예비조사 종합 결과 및 질문-답변 품질 상관관계**

| 구분 | 질문 품질 | AI 답변 품질 | 상관관계 |
|------|:--------:|:----------:|:--------:|
| 평균 점수 (15점) | 5.826 (SD 2.925) | 7.071 (SD 2.800) | r=0.691*** |
| 중앙값 | 6.0 | 7.0 | p<0.001 |
| 백분율 | 38.8% | 47.1% | - |
| **최저 영역** | **학습맥락 1.500** | **학습확장성 1.832** | r=0.521 |

주: ***p<0.001. 질문이 명료할수록 AI 답변 품질 향상.

예비 조사의 교사 루브릭 평가 결과를 종합하면, 학생 질문의 평균 점수는 15점 만점에 5.826점으로 매우 낮았으며, AI 답변의 평균 점수는 7.071점으로 여전히 중간 수준(50%)에 미치지 못하였다. 

추가 분석 결과, 질문 품질과 답변 품질 간에는 강한 양의 상관관계(r=0.691, p<0.001)가 확인되어, 질문이 명료하고 맥락이 풍부할수록 AI 답변의 질도 향상되는 것으로 나타났다.[^correlation_method] 

[^correlation_method]: 예비 조사 385건 질문-답변 쌍에서 질문 총점(A1+A2+A3, 15점 만점)과 답변 총점(B1+B2+B3, 15점 만점) 간 Pearson 상관계수를 계산한 결과이다. 특히 질문의 학습맥락 점수와 답변의 학습확장성 점수 간 상관계수는 0.521로, 학생이 자신의 학습 상황을 명확히 제시할수록 AI가 학습을 확장하는 답변을 제공할 가능성이 높아짐을 보여준다. **이는 질문 명료화 프로세스를 핵심 전략으로 하는 MAICE 시스템 개발의 실증적 근거가 되었다.**

이러한 문제점들은 단순히 개별 사례의 오류가 아니라, 현재 LLM이 채택하고 있는 즉시 답변 제공 방식의 구조적 한계에서 비롯된다. 이 방식은 질문의 질이나 맥락과 무관하게 즉각적인 답변을 제공함으로써, 학습자가 스스로 질문을 명료화하고 구조화하는 과정을 경험할 기회를 제공하지 않는다. 예비 조사에서 관찰된 바와 같이, 이는 학생들의 LLM 활용 만족도를 저하시킬 뿐만 아니라, 실질적인 학습 효과를 얻지 못하는 결과를 초래한다.

특히 수학적 귀납법과 같이 논리적 사고의 구조화와 단계적 추론이 핵심인 단원에서는, 즉시 답변 제공 방식이 학생의 사고 과정을 충분히 유도하지 못하고 피상적 이해에 머무르게 하는 한계가 있다. 수학적 귀납법은 "n=1일 때 성립함을 보이고, n=k일 때 성립한다고 가정하면 n=k+1일 때도 성립함을 보인다"는 명확한 논리 구조를 요구하는데, 단순한 결론 제시만으로는 학생의 개념 이해와 적용 능력을 효과적으로 향상시키기 어렵다.

따라서 예비 조사에서 확인된 즉시 답변 제공 방식의 구조적 한계를 극복하고, 질문 명료화 과정을 통해 학습자의 사고 과정을 깊이 있게 지원하며, 교사 루브릭 평가에서 드러난 문제점들(학습맥락 부재 72.3%, 학습확장성 결여 48.9%)을 개선할 수 있는 **질문 명료화 기반 AI agent 시스템 MAICE의 개발이 필요하다**.

### 마. 예비 조사 결과가 시사하는 두 가지 설계 방향

예비 조사 결과는 MAICE 시스템 설계에 두 가지 중요한 방향을 제시하였다:

**1. 시스템 설계 방향**: 질문 명료화 프로세스의 필요성

질문 품질과 답변 품질 간 강한 상관관계(r=0.691)는 **질문 개선이 학습 효과 향상의 핵심**임을 보여준다. 특히 학습 맥락 부재(72.3%)와 학습 확장성 결여(48.9%)가 가장 심각한 문제로 확인되어, MAICE는 이 두 영역을 집중적으로 개선하는 질문 명료화 프로세스를 핵심 기능으로 설계하였다.

**2. 평가 도구 개선 방향**: 객관적 측정 도구의 필요성

예비 조사 루브릭의 낮은 교사 간 일치도(r=0.28, ICC=0.29)는 5점 척도 종합 판단 방식의 주관성 문제를 드러냈다. 이는 본 실험에서 더 객관적이고 신뢰성 높은 평가 도구가 필요함을 시사하였다. 

이러한 필요성에 따라 본 연구는 **QAC 체크리스트(Question-Answer-Context Checklist)**를 개발하였다(6.4절 상세). QAC는 예비 조사 루브릭의 6개 영역에 **대화 맥락 영역(C)**을 추가하고, 각 항목을 4개의 명확한 체크리스트 요소로 분해하여 이진 판단(0/1)으로 평가하는 개선된 도구이다(8개 항목, 32개 체크리스트, 40점 만점). 

본 실험에서는 평가의 일관성과 대규모 평가 가능성을 위해 **3개 독립 AI 모델(Gemini, Claude, GPT-5)을 평가자로 활용**하였으며, 이들 AI 평가자 간 신뢰도는 ICC(2,1)=0.595, Cronbach's α=0.840으로 확인되었다. 추가로 현직 교사 4명(수학 3명, 정보 1명)이 100개 세션을 독립적으로 평가하여(총 172개 평가 레코드) AI 평가의 타당성을 검증하였으며, 교사 간 신뢰도는 Pearson 상관계수 평균 r=0.85로 매우 높게 나타났다(7.9절).

## 2. 연구 목적

예비 조사에서 확인된 핵심 발견은 질문 품질과 답변 품질 간 강한 상관관계(r=0.691)로, 이는 **질문 개선이 학습 효과 향상의 핵심 메커니즘**임을 의미한다. 따라서 본 연구는 고등학교 2학년 수학적 귀납법 단원을 중심으로 **질문 명료화 프로세스를 핵심으로 하는 AI agent 시스템 MAICE(Mathematical AI Chatbot for Education)를 설계·개발**한다. 

MAICE의 교육적 효과를 검증하기 위해, 본 연구는 무작위 배정 A/B 테스트를 설계하였다. 학생들을 두 그룹으로 나누어 한 그룹은 **MAICE의 질문 명료화 프로세스를 제공받는 방식(본 연구에서 "Agent 모드"로 명명)**을 사용하고, 다른 그룹은 **일반적인 LLM처럼 즉시 답변만 제공하는 방식(본 연구에서 "Freepass 모드"로 명명)**을 사용하도록 하여, 질문 명료화의 순수한 효과를 비교 분석한다. 

### 가. 학술적 연구 목적

1. **질문 명료화 기반 멀티 에이전트 시스템 개발**: 예비 조사에서 확인된 질문의 질적 문제(학습맥락 부재 72.3%, 질문구조 불명확 45.8%, 수학적전문성 결여 45.5%)를 개선하는 질문 명료화 프로세스를 핵심으로 하는 멀티 에이전트 시스템 개발

2. **질문 품질 개선 효과 검증**: MAICE의 질문 명료화 프로세스가 학생의 질문 품질(학습맥락, 질문구조, 수학적전문성)을 일반적인 즉시 답변 방식 대비 실질적으로 개선함을 입증

3. **학습 경험 향상 증명**: 질문 명료화 프로세스를 통한 구조화된 학습 경험이 즉시 답변 방식보다 학습 만족도와 메타인지를 향상시킴을 입증

<!-- 4. **학습 효과 개선 검증**: 질문 품질 개선이 실제 학습 성과로 이어지는지 확인하기 위해, 수학적 귀납법 학습 효과(이해도, 문제해결 능력)에서 일반적인 즉시 답변 방식 대비 우수함을 검증 -->

4. **수학적 귀납법 단원 적용 검증**: MAICE 에이전트 시스템이 수학적 귀납법 단원(논리 구조 이해, 단계별 증명 필요)에서 효과적으로 작동하는지 검증

### 나. 교육적 실천 목적

본 연구는 학술적 검증을 넘어, 교육 현장에서의 지속 가능한 활용과 발전을 다음과 같이 목표한다:

1. **학생의 반성적 사고 능력 탐구**: 학생들이 AI와의 대화를 통해 스스로의 사고 과정을 명료화하고, 메타인지 능력을 발달시키는 과정을 관찰하고 검증함으로써, Dewey의 반성적 사고 이론이 현대 AI 교육 환경에서도 유효함을 실증

2. **교사 주도 AI 교육 연구 플랫폼 제공**: MAICE 시스템을 교사가 자율적으로 커스터마이징할 수 있는 연구 도구로 설계하여, 교사가 각자의 교육 맥락에서 AI 활용에 필요한 핵심 요소(명료화 전략, 피드백 설계, 학습자 맞춤)를 직접 탐구하고 개선할 수 있는 기반 마련

3. **프롬프팅 설계 연구의 지속가능성**: 각 에이전트의 프롬프트를 공개하고 수정 가능하도록 설계하여, 교사가 자신의 수업 목표와 학생 특성에 맞춰 프롬프팅을 설계하고 그 효과를 체계적으로 연구할 수 있는 실험 환경 제공

본 시스템은 "완성된 솔루션"이 아닌 **"교사 주도 연구를 위한 확장 가능한 플랫폼"**으로 설계되었다. 이는 현재 에듀테크 산업에서 지배적인 "상업적으로 주어진 완성된 도구를 소비만 하면 좋은 수업이 나온다"는 접근에 대한 근본적 비판에서 출발한다. 

**AI 도구의 교육적 가치는 결국 그것을 설계하고 사용하는 교사의 전문성을 넘을 수 없다.** 마치 훌륭한 소설의 캐릭터가 작가의 능력을 넘지 못하듯, AI 교육 도구의 효과성은 그것을 설계한 교육적 철학과 교사의 맥락적 이해를 초과할 수 없다. 따라서 본 연구는 교육 현장의 교사들이 AI 교육 도구의 수동적 소비자가 아닌, **능동적 설계자이자 연구자**로서 자신의 교육 맥락에 최적화된 AI 시스템을 직접 개발하고 개선하는 역량을 갖추도록 지원하는 것을 궁극적 목표로 한다.

## 3. 연구 문제

예비 조사를 통해 확인된 일반적인 즉시 답변 제공 방식의 핵심 문제는 학생들의 질문 품질 부족(학습맥락 72.3% 최저점)이며, 질문 품질과 답변 품질 간 강한 상관관계(r=0.691)는 질문 개선이 학습 효과 향상의 핵심 메커니즘임을 시사한다. 

앞서 1.2절에서 설명한 바와 같이, 본 연구는 MAICE 시스템의 두 가지 작동 방식—질문 명료화를 제공하는 **Agent 모드**와 일반적인 LLM처럼 즉시 답변만 제공하는 **Freepass 모드**—를 무작위 배정 A/B 테스트로 비교한다. MAICE 시스템의 효과성을 검증하기 위한 연구 문제를 다음과 같이 설정한다:

**RQ1. 질문 품질 개선 검증 (핵심 메커니즘)**: MAICE의 질문 명료화 프로세스(Agent 모드)가 학생의 질문 품질을 실질적으로 개선하는가? 특히 예비 조사에서 가장 심각한 문제로 확인된 학습 맥락 적용, 질문 구조화, 수학적 전문성 영역에서 일반적인 즉시 답변 방식(Freepass 모드) 대비 유의미한 개선이 나타나는가?

**RQ2. 학습 경험 향상 검증 (교육적 가치)**: 질문 명료화 프로세스를 통한 구조화된 학습 경험(Agent 모드)이 즉시 답변 방식(Freepass 모드)보다 학생의 학습 만족도와 메타인지를 향상시키는가?

<!-- **RQ3. 학습 효과 향상 검증 (실제 성취도)**: 질문 품질 개선과 학습 경험 향상이 실제 학습 성과로 이어지는가? MAICE의 Agent 모드가 Freepass 모드보다 수학적 귀납법 학습 효과(수행평가 점수, 문제해결 능력)에서 우수한가? [수행평가 미실시로 향후 연구 과제로 유보] -->

## 4. 용어의 정의

- **예비 조사(Pilot Study)**: 본 연구의 MAICE 시스템 설계에 앞서, 2024년 5월 중순 실제 고등학교 수학 수업 환경에서 프리패스 방식 LLM의 교육적 문제점을 실증적으로 파악하기 위해 실시한 질문-답변 수집 및 교사 루브릭 평가 연구 (385건 질문, 1,012건 교사 평가)

- **MAICE (Mathematical AI Chatbot for Education)**: 예비 조사에서 확인된 일반적인 LLM 사용 방식의 한계를 극복하기 위해 개발된 질문 명료화 기반 AI agent 시스템. 본 연구에서는 MAICE를 두 가지 모드로 운영하여 비교 실험을 수행한다.

- **Agent 모드**: MAICE 시스템의 질문 명료화 프로세스를 제공하는 실험군 조건. QuestionImprover 에이전트가 학생의 초기 질문을 받아 명료화 질문을 통해 문제를 구체화한 후, 명료화된 질문에 대해 맞춤형 답변을 제공한다.

- **Freepass 모드**: MAICE 시스템의 대조군 조건으로, 일반적인 LLM(ChatGPT, Claude 등)처럼 질문 명료화 과정 없이 학생의 질문에 즉시 답변만 제공하는 방식. "Freepass"는 "명료화 과정을 거치지 않고(free) 바로 통과(pass)하여 답변"이라는 의미로 본 연구에서 명명하였다. 

- **질문 명료화**: 모호하거나 불완전한 질문을 구체적이고 명확한 질문으로 개선하는 과정으로, 학습자가 스스로 질문을 구조화하고 사고 과정을 명료화하도록 지원하는 교육적 개입.

- **AI 에이전트(AI Agent)**: 특정 목표를 달성하기 위해 환경을 인식하고, 자율적으로 의사결정을 내려 행동하는 소프트웨어 시스템. 본 연구에서는 LLM을 기반으로 특정 교육적 역할(질문 분류, 명료화 지원, 답변 생성 등)을 수행하도록 설계된 독립적 구성 요소를 의미함.

- **멀티 에이전트 시스템**: 질문 분류, 명료화 지원, 답변 생성, 학습 관찰 등 각기 다른 역할을 수행하는 여러 개의 독립적인 AI agent가 협업하여 학습 과정을 지원하는 시스템

- **학습 효과**: 수학적 이해도, 문제해결 능력, 질문 품질 개선, 메타인지 향상 등을 포함하는 학습 성과와 학습 경험의 종합적 측정

---

**이전**: 없음 | **다음**: [[chapters/02-theoretical-background]]

**[그림 1-1] 프리패스 방식의 문제점 예시**

_(그림 위치: mermaid 다이어그램 또는 이미지 추가 필요)_


**[그림 1-2] 연구 진행 절차**

_(그림 위치: mermaid 다이어그램 또는 이미지 추가 필요)_

