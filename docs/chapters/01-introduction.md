---
tags: [논문/서론, 논문/1장]
alias: [서론, 1장]
parent: "수학 학습에서 질문 명료화를 지원하는 AI에이전트 설계 및 개발 고등학교 2학년 수학적 귀납법 단원 중심으로"
---

# 1. 서론

## 1.1 연구의 필요성

### 1.1.1 수학 교육에서 생성형 AI 활용의 확산과 한계

대규모 언어 모델(Large Language Model, LLM)의 비약적 발전에 따라 ChatGPT, Claude 등 생성형 AI 도구가 교육 현장에 빠르게 도입되고 있다. 특히 수학 교육 영역에서는 학생들이 개념 이해, 문제 풀이, 추가 학습 등의 목적으로 LLM을 활용하는 빈도가 급증하고 있으며, 일부 교사들은 이를 보조 학습 도구로 활용하려는 시도를 하고 있다.

그러나 실제 교실 현장에서 LLM을 보조교사로 활용하려는 시도 과정에서, 교육적 효과성에 대한 근본적인 문제들이 지속적으로 관찰되었다. 현재 대부분의 LLM 기반 AI 도구(ChatGPT, Claude, Gemini 등)는 학생의 질문에 대해 **질문의 질이나 맥락과 무관하게 즉각적으로 답변을 제공**한다. 이러한 즉시 답변 제공 방식은 신속한 정보 제공이라는 장점이 있으나, 학습자가 스스로 질문을 구조화하고 사고를 확장하는 과정을 지원하지 못하여 깊이 있는 학습을 저해할 가능성이 제기되고 있다.

### 1.1.2 학생 질문의 질적 문제: 실증 분석

본 연구에서는 MAICE 시스템 설계에 앞서, 현재 일반적인 LLM 사용 방식의 교육적 문제점을 실증적으로 파악하기 위해 2024년 5월 중순(학기 초)에 예비 조사를 실시하였다. 예비 조사는 다음의 순차적 과정으로 진행되었다:

**1단계: 데이터 수집** - 고등학교 수학 수업 환경에서 학생들에게 ChatGPT와 동일한 사용자 인터페이스를 제공하는 간단한 웹앱을 배포하였다. 수업 시간 동안 학생들이 자유롭게 질문하고 AI 답변을 받을 수 있도록 하여 총 385건의 실제 질문-답변 쌍을 수집하였다.

**2단계: 문제점 파악 및 루브릭 구성** - 수집된 데이터를 분석한 결과, 학생 질문과 AI 답변에서 반복적으로 나타나는 질적 문제 패턴들이 관찰되었다. 이러한 문제점들을 명료화하고 그 존재 여부를 객관적으로 판단하기 위해, 전통적 교육학 이론(Dewey의 반성적 사고, Vygotsky의 ZPD, Bloom의 교육목표분류학 등)과 최신 AI 교육 평가 연구(MRBench, 소크라테스 루브릭 등)를 종합하여 체계적인 분석적 채점 기준(analytic rubric)을 개발하였다.

관찰된 구체적 문제 패턴에 대응하여 6개 평가 영역을 구성하였다:

**질문 평가 영역 (학생의 질문 품질)**:
- **수학적 전문성 (A1)**: 수학 개념의 정확성, 교과과정 내 위치, 용어 사용의 적절성 평가
- **질문 구조화 (A2)**: 질문 대상·범위·초점의 명확성, 구체적 어려움 제시 평가
- **학습 맥락 적용 (A3)**: 학습자 수준, 선수지식, 학습 목적, 이해 상태 명시 평가

**AI 답변 평가 영역 (AI의 교육적 응답 품질)**:
- **학습자 맞춤도 (B1)**: 학습자 수준 파악, 선수지식 연계, 난이도 조절, 개인화된 피드백 평가
- **설명의 체계성 (B2)**: 개념의 위계적 설명, 단계별 논리 전개, 핵심 요소 강조, 예시 활용 평가
- **학습 내용 확장성 (B3)**: 심화학습 방향 제시, 응용문제 연계, 오개념 교정, 자기주도 학습 유도 평가

각 영역은 **5점 리커트 척도(1=매우 부족 ~ 5=매우 우수)**로 평가되며, 교사가 6개 영역 각각에 대해 전체적인 판단을 바탕으로 1~5점을 직접 부여한다. 평가를 돕기 위해 각 영역별로 4개의 세부 평가 요소(총 24개)를 참고 가이드로 제시하였으나, 최종 점수는 교사의 종합적 판단에 따른다. 질문 영역 15점, 답변 영역 15점으로 총 30점 만점이다.

(루브릭 개발의 상세한 이론적 기반과 각 문항 구성 근거는 2장 이론적 배경 및 부록 A 참조)

**3단계: 동료교사 평가 실시** - 중등 수학교사 4명(교직 경력: 1년차 2명, 5년차 1명, 7년차 1명, 평균 3.5년)이 독립적으로 385건의 질문-답변을 6개 영역 각각에 대해 1~5점으로 평가하였다.

**4단계: 평가 결과 분석 및 주요 발견** - 교사 루브릭 평가 결과, 일반적인 LLM 사용 방식의 구체적 문제점들이 실증적으로 확인되었다:

**학생 질문의 문제점**:
- **수학적 전문성 (A1)**: 평균 2.277점, **45.5%가 최저점(1점)**
- **질문 구조화 (A2)**: 평균 2.049점, **45.8%가 최저점(1점)**
- **학습 맥락 적용 (A3)**: 평균 1.500점, **72.3%가 최저점(1점)** (가장 심각)

**AI 답변의 문제점**:
- **학습자 맞춤도 (B1)**: 평균 2.474점, 27.6%가 최저점(1점)
- **설명의 체계성 (B2)**: 평균 2.765점, 23.9%가 최저점(1점)
- **학습 내용 확장성 (B3)**: 평균 1.832점, **48.9%가 최저점(1점)** (AI 답변 중 가장 심각)

**핵심 발견 1**: 질문 품질과 답변 품질 간 **강한 상관관계 r=0.691 (p<0.001)** 확인. 이는 **질문 개선이 학습 효과 향상의 핵심 메커니즘**임을 시사하며, MAICE의 질문 명료화 기반 시스템 설계의 실증적 근거가 되었다.

**핵심 발견 2**: 교사 간 평가 일치도가 낮음 (평균 r=0.28, ICC(2,k)=0.29). 특히 수학적 전문성과 학습확장성 영역에서 평가 차이가 컸다(r=0.01~0.26). 이러한 낮은 일치도는 교사의 종합적 판단에 의한 점수 부여 방식의 주관성에서 비롯되었다. 이는 본 실험을 위한 더 객관적이고 신뢰성 높은 평가 도구 개발의 필요성을 시사하였다.

교사 루브릭 평가 결과, 학생 질문의 질적 문제가 심각한 수준임이 확인되었다. 다음 표는 각 평가 영역별 결과를 보여준다:

| 평가 영역 | 평균 점수 (5점 만점) | 최저점(1점) 비율 | 중앙값 |
|----------|---------------------|-----------------|--------|
| **질문 평가** | | | |
| 수학적 전문성 | 2.277 | 45.5% | 2.0 |
| 질문 구조화 | 2.049 | 45.8% | 2.0 |
| 학습 맥락 적용 | **1.500** | **72.3%** | **1.0** |
| **AI 답변 평가** | | | |
| 학습자 맞춤도 | 2.474 | 27.6% | 3.0 |
| 설명의 체계성 | 2.765 | 23.9% | 3.0 |
| 학습 내용 확장성 | **1.832** | **48.9%** | **2.0** |

질문의 **수학적 전문성** 영역에서 45.5%가 최저점(1점)을 받았으며, **질문 구조화** 영역에서는 45.8%가 최저점을 기록하였다. 가장 심각한 문제는 **학습 맥락 적용** 영역으로, 전체의 72.3%가 최저점을 받아 대다수 학생들이 학습자의 현재 수준, 선수 지식, 학습 목적 등의 맥락 정보를 제공하지 못하는 것으로 나타났다.

학생 질문을 유형별로 분석한 결과, 다음과 같은 문제 패턴이 확인되었다:

#### 1.1.2.1 맥락 없는 개념 설명 요청 (학습맥락 1점: 72.3%, 184건)

학생들은 "지수의 확장을 알려줘", "log 실생활 활용 예시", "수1의 핵심 부분이 뭐야?"와 같이 학습자의 현재 수준, 선수 지식, 학습 목적 등의 맥락 정보가 결여된 질문을 빈번하게 제시하였다. 분석 결과 전체 질문 중 72.3%가 학습맥락 평가에서 최저점(1점)을 받았으며, 교사 평가 기준으로 학습맥락이 모든 교사로부터 1점을 받은 질문이 184건 발견되었다.

**실제 사례 (ID 294):** "근데 어떻게 증명한거야?"  
**교사 평가:** 수학적전문성 1점, 질문구조 1점, 학습맥락 1점 (질문 총점 3.0/15점)  
**AI 답변:** "어떤 것을 증명했는지 구체적으로 말씀해 주시면 더 자세히 답변드리겠습니다. 일반적으로 수학적 또는 과학적 증명은 논리적 추론과 증거를 사용하여..."  
**문제점:** 어떤 증명을 지칭하는지, 어떤 부분이 이해되지 않는지, 학습자가 어느 수준인지 전혀 파악할 수 없어 AI도 추가 정보를 요청할 수밖에 없는 구조. 이러한 질문에서는 AI가 적절한 난이도와 설명 방식을 선택할 수 있는 근거가 부족하여, 학습자 맞춤형 답변 생성이 구조적으로 불가능하다.

#### 1.1.2.2 모호하고 불명확한 질문 (질문구조 1점: 45.8%, 25건)

전체 질문 중 45.8%가 질문구조 평가에서 최저점(1점)을 받았으며, 특히 질문구조가 모든 교사로부터 1점을 받은 질문이 25건 발견되었다. 이러한 질문 구조에서는 AI가 학습자의 실제 어려움이나 구체적인 궁금증을 파악하기 어려워 적절한 답변을 생성할 수 없다.

**실제 사례들 (모든 교사 질문구조 1점):**
- (ID 142) "내 증명에대해서 피드백해줘" - 어떤 증명을, 어느 부분을 평가받고 싶은지 불명확
- (ID 47) "a+b = c+d" - 단순 수식 제시로 질문 의도 파악 불가, AI는 영어로 수식 설명만 제공
- (ID 315) "토푸리아 마카체프 5분 5라운드 라이트급에서 챔피언 벨트를 두고 싸우면 누가 이길 것 같아? 승자를 말해줘" - 수학과 무관하며 질문 형태도 불명확

**문제점:** 질문의 대상, 범위, 초점이 불명확하여 AI가 학생이 실제로 무엇을 알고 싶어하는지 파악할 수 없음

#### 1.1.2.3 수학과 무관한 질문 (수학적전문성 1점: 45.5%, 26건)

전체 질문 중 45.5%가 수학적전문성 평가에서 최저점(1점)을 받았으며, 모든 교사로부터 수학적전문성 1점을 받은 질문이 26건 발견되었다. 이는 학습 집중도를 저하시키고 AI의 교육적 역할 수행을 방해하는 요인으로 작용한다.

**실제 사례들 (모든 교사 수학적전문성 1점):**
- (ID 128) "반갑다" - 단순 인사, AI도 "어떻게 도와드릴까요?" 응답만 제공 (평균 질문 총점 3.0/15점)
- (ID 119) "발해의 건국과 발전을 정리해줘" - 역사 질문으로 수학과 완전 무관
- (ID 344) "수학으로 된 노래가 있어?" - 수학 교육과 관련 없는 호기심 질문

**문제점:** 수학 학습 목적에서 완전히 이탈하여 수업 집중도를 저하시키며, 제한된 수업 시간에서 AI 교육 도구의 효과성을 훼손

#### 1.1.2.4 질문 품질의 극명한 편차

흥미롭게도, 매우 불량한 질문들과 대조적으로 일부 학생들은 매우 구조화되고 맥락이 풍부한 질문을 제시하기도 하였다. 학습맥락 평균 4점 이상을 받은 질문은 단 5건에 불과했으나, 이들 질문의 평균 총점은 10.0/15점으로 불량 질문(3.0/15점)과 극명한 대조를 보였다.

**우수 질문 실제 사례 (ID 358):** "나는 소프트웨어마이스터고에 다니는 고등학교 2학년이야 현재 수학을 공부하고 있는데 0!이 1인 이유를 알기 쉽게 알려줘."  
**교사 평가:** 수학적전문성 4점, 질문구조 3점, 학습맥락 5점 (질문 총점 10.0/15점)  
**우수한 점:** ① 학습자의 신분과 수준 명시 (소프트웨어마이스터고 2학년), ② 현재 학습 상황 설명 (수학 공부 중), ③ 구체적 질문 (0!이 1인 이유), ④ 설명 방식 요청 (알기 쉽게)

이러한 질문 품질의 큰 편차(최저 3.0점 vs 최고 10.0점)는 학생들이 AI에게 효과적으로 질문하는 방법에 대한 명시적 교육이 필요함을 시사한다.

### 1.1.3 AI 답변의 질적 문제: 교사 루브릭 평가 결과

학생 질문에 대한 LLM의 답변을 동일한 교사 패널이 개발한 루브릭으로 평가한 결과, 심각한 교육적 한계가 확인되었다. AI 답변 평가 결과, **학습 내용 확장성** 영역(심화학습 방향 제시, 응용문제 연계성, 오개념 교정 전략, 자기주도 학습 유도)에서 48.9%가 최저점(1점)을 받아 가장 심각한 문제를 보였다. **학습자 맞춤도** 영역(학습자 수준별 접근, 선수지식 연계성, 학습 난이도 조절, 개인화된 피드백)에서는 평균 2.474점으로 27.6%가 최저점을 받았으며, **설명의 체계성** 영역(개념 설명 위계화, 단계별 논리 전개, 핵심 요소 강조, 예시 활용의 적절성)은 평균 2.765점으로 상대적으로 양호한 결과를 보였으나 여전히 23.9%가 최저점을 받았다.

각 루브릭 영역별로 확인된 구체적인 AI 답변의 문제점은 다음과 같다:

#### 1.1.3.1 학습자 맞춤도 문제: 학습자 수준별 접근 및 맥락 파악 실패 (평균 2.474점, 최저점 27.6%)

루브릭의 '학습자 맞춤도' 영역은 AI 답변이 학습자 수준별 접근, 선수지식 연계성, 학습 난이도 조절, 개인화된 피드백을 제공하는지 평가한다. 그러나 프리패스 방식 AI는 질문에 학습 맥락이 부족할 때 학습자 수준을 파악하지 못하거나, 심지어 질문의 의미 자체를 잘못 해석하는 문제를 보였다.

**실제 사례 (ID 130):** "지수의 확장에 대해 설명해줘"  
**AI 답변:** "지수의 확장에 대해 설명하자면, 이는 **지수가 영역을 확장하고 더 많은 지역이나 새로운 시장으로 진출하는 과정**을 의미합니다. 이러한 확장은 일반적으로 비즈니스의 성장 및 발전 전략의 일환으로 이루어지며... 1. **지리적 확장**: 새로운 도시, 지역, 국가로 사업을 확장하여..."  
**교사 평가:** 4명 모두 학습자적합성 1점 (최저점)  
**루브릭 위반 항목:** ① 학습자 수준별 접근 실패 (고등학교 수학인지 비즈니스인지 파악 못함), ② 선수지식 연계성 부재 (수학 개념과 무관), ③ 학습 난이도 조절 불가능 (맥락 오해로 잘못된 답변)  
**문제점:** 수학의 '지수(exponent)'와 비즈니스 '지점 확장'을 혼동하여 완전히 무관한 답변 제공

#### 1.1.3.2 학습자 맞춤도 문제: 학습 난이도 조절 실패

**실제 사례:** "로지스틱 회귀가 뭐야?" (고등학교 1학년)  
**AI 답변:** "로지스틱 회귀는... 고등학교 1학년 수준에서 이해할 수 있도록 조금 더 쉽게 설명하면... odds ratio... 최대우도추정..."  
**루브릭 위반 항목:** ① 학습 난이도 조절 실패 (대학 수준 통계 개념 사용), ② 개인화된 피드백 부재 (고1 수준 고려 없음)  
**문제점:** "고등학교 1학년 수준"이라고 언급하면서도 실제 내용은 대학 수준의 통계학 개념 포함

#### 1.1.3.3 설명의 체계성 문제: 교육과정 용어 사용의 부적절성 (평균 2.765점, 최저점 23.9%)

루브릭의 '설명의 체계성' 영역은 개념 설명의 위계화, 단계별 논리 전개, 핵심 요소 강조, 예시 활용의 적절성을 평가한다. 한국 교육과정에 맞지 않는 용어 사용은 학생의 개념 형성에 혼란을 초래한다.

**실제 사례:** "수열이 뭐야?"  
**AI 답변:** "수열은... **명시적 공식(Explicit Formula)** \(a_n\)을 \(n\)에 대한 함수로 표현합니다..."  
**루브릭 위반 항목:** ① 핵심 요소 강조 부적절 (교육과정 비표준 용어), ② 개념 설명의 위계화 실패 (한국 교육과정 체계 무시)  
**문제점:** 대한민국 수학 교육과정 표준 용어 '일반항' 대신 '명시적 공식(Explicit Formula)' 사용으로 교사-학생 간 의사소통 혼란 초래

#### 1.1.3.4 학습 내용 확장성 문제: 심화학습 방향 제시 및 자기주도 학습 유도 실패 (평균 1.832점, 최저점 48.9%)

루브릭의 '학습 내용 확장성' 영역은 심화학습 방향 제시, 응용문제 연계성, 오개념 교정 전략, 자기주도 학습 유도를 평가한다. 이 영역에서 전체 답변 중 48.9%가 최저점(1점)을 받았으며, 모든 교사로부터 학습확장성 1점을 받은 답변이 82건 발견되어 가장 심각한 문제로 확인되었다.

**실제 사례 (ID 145):** "수1의 핵심 부분이 뭐야?"  
**AI 답변:** "수1의 핵심 부분은 고등학교 수학 1에서 다루는 주요 개념들을 의미합니다. 주요 내용은 다음과 같습니다: 1. **다항식**: 다항식의 사칙연산, 인수분해, 나머지 정리... 2. **방정식과 부등식**: 일차방정식, 이차방정식..."  
**교사 평가:** 학습자적합성 3점, 설명체계성 2점, **학습확장성 1점**  
**루브릭 위반 항목:** ① 심화학습 방향 제시 부재 (단순 나열만), ② 응용문제 연계성 없음, ③ 자기주도 학습 유도 실패 (학생이 스스로 탐구할 방향 제시 없음)  
**문제점:** 학생이 왜 이 질문을 했는지 파악 없이 관련 개념만 나열. 과도한 정보 제시로 인지 부담 가중

**실제 사례 (ID 128):** "반갑다"  
**AI 답변:** "안녕하세요! 만나서 반갑습니다. 어떻게 도와드릴까요?"  
**교사 평가:** 학습자적합성 1점, 설명체계성 1점, **학습확장성 1점** (평균 답변 총점 3.0/15점)  
**루브릭 위반 항목:** ① 심화학습 방향 제시 전무, ② 응용문제 연계 없음, ③ 오개념 교정 기회 상실, ④ 자기주도 학습 유도 실패  
**문제점:** 단순 인사를 수학 학습 시작의 기회로 활용하지 못함. 학습자의 현재 상태나 관심사를 파악하려는 시도 없이 형식적 응답만 제공

이러한 문제들은 프리패스 방식 AI가 학습자의 현재 이해 수준과 필요성을 고려하지 않고, 심화학습을 유도하거나 자기주도적 학습을 촉진하는 교육적 역할을 수행하지 못함을 보여준다.

### 1.1.4 프리패스 방식의 구조적 한계와 MAICE 시스템 설계의 필요성

예비 조사의 교사 루브릭 평가 결과를 종합하면, 학생 질문의 평균 점수는 15점 만점에 5.826점(표준편차 2.925, 중앙값 6.0점)으로 매우 낮았으며, AI 답변의 평균 점수는 7.071점(표준편차 2.800, 중앙값 7.0점)으로 질문보다는 다소 높았으나 여전히 중간 수준(50%)에 미치지 못하였다. 특히 주목할 점은 학생 질문의 학습맥락 영역(평균 1.500/5.0)과 AI 답변의 학습확장성 영역(평균 1.832/5.0)이 가장 낮은 점수를 받았다는 것이다.

추가 분석 결과, 질문 품질과 답변 품질 간에는 강한 양의 상관관계(r=0.691, p<0.001)가 확인되어, 질문이 명료하고 맥락이 풍부할수록 AI 답변의 질도 향상되는 것으로 나타났다.[^correlation_method] 

[^correlation_method]: 예비 조사 385건 질문-답변 쌍에서 질문 총점(A1+A2+A3, 15점 만점)과 답변 총점(B1+B2+B3, 15점 만점) 간 Pearson 상관계수를 계산한 결과이다. 특히 질문의 학습맥락 점수와 답변의 학습확장성 점수 간 상관계수는 0.521로, 학생이 자신의 학습 상황을 명확히 제시할수록 AI가 학습을 확장하는 답변을 제공할 가능성이 높아짐을 보여준다. **이는 질문 명료화 프로세스를 핵심 전략으로 하는 MAICE 시스템 개발의 실증적 근거가 되었다.**

이러한 문제점들은 단순히 개별 사례의 오류가 아니라, 현재 LLM이 채택하고 있는 즉시 답변 제공 방식의 구조적 한계에서 비롯된다. 이 방식은 질문의 질이나 맥락과 무관하게 즉각적인 답변을 제공함으로써, 학습자가 스스로 질문을 명료화하고 구조화하는 과정을 경험할 기회를 제공하지 않는다. 예비 조사에서 관찰된 바와 같이, 이는 학생들의 LLM 활용 만족도를 저하시킬 뿐만 아니라, 실질적인 학습 효과를 얻지 못하는 결과를 초래한다.

특히 수학적 귀납법과 같이 논리적 사고의 구조화와 단계적 추론이 핵심인 단원에서는, 즉시 답변 제공 방식이 학생의 사고 과정을 충분히 유도하지 못하고 피상적 이해에 머무르게 하는 한계가 있다. 수학적 귀납법은 "n=1일 때 성립함을 보이고, n=k일 때 성립한다고 가정하면 n=k+1일 때도 성립함을 보인다"는 명확한 논리 구조를 요구하는데, 단순한 결론 제시만으로는 학생의 개념 이해와 적용 능력을 효과적으로 향상시키기 어렵다.

따라서 예비 조사에서 확인된 즉시 답변 제공 방식의 구조적 한계를 극복하고, 질문 명료화 과정을 통해 학습자의 사고 과정을 깊이 있게 지원하며, 교사 루브릭 평가에서 드러난 문제점들(학습맥락 부재 72.3%, 학습확장성 결여 48.9%)을 개선할 수 있는 **질문 명료화 기반 AI agent 시스템 MAICE의 개발이 필요하다**.

### 1.1.5 예비 조사 결과가 시사하는 두 가지 설계 방향

예비 조사 결과는 MAICE 시스템 설계에 두 가지 중요한 방향을 제시하였다:

**1. 시스템 설계 방향**: 질문 명료화 프로세스의 필요성

질문 품질과 답변 품질 간 강한 상관관계(r=0.691)는 **질문 개선이 학습 효과 향상의 핵심**임을 보여준다. 특히 학습 맥락 부재(72.3%)와 학습 확장성 결여(48.9%)가 가장 심각한 문제로 확인되어, MAICE는 이 두 영역을 집중적으로 개선하는 질문 명료화 프로세스를 핵심 기능으로 설계하였다.

**2. 평가 도구 개선 방향**: 객관적 측정 도구의 필요성

예비 조사 루브릭의 낮은 교사 간 일치도(r=0.28, ICC=0.29)는 5점 척도 종합 판단 방식의 주관성 문제를 드러냈다. 이는 본 실험에서 더 객관적이고 신뢰성 높은 평가 도구가 필요함을 시사하였다. 

이러한 필요성에 따라 본 연구는 **QAC 체크리스트(Question-Answer-Context Checklist)**를 개발하였다(6.4절 상세). QAC는 예비 조사 루브릭의 6개 영역에 **대화 맥락 영역(C)**을 추가하고, 각 항목을 4개의 명확한 체크리스트 요소로 분해하여 이진 판단(0/1)으로 평가하는 개선된 도구이다(8개 항목, 32개 체크리스트, 40점 만점). 

본 실험에서는 평가의 일관성과 대규모 평가 가능성을 위해 **3개 독립 AI 모델(Gemini, Claude, GPT-5)을 평가자로 활용**하였으며, 이들 AI 평가자 간 신뢰도는 ICC(2,1)=0.595, Cronbach's α=0.840으로 확인되었다. 추가로 현직 교사 2명이 54개 세션을 평가하여 AI 평가의 타당성을 검증하였다(7.6절).

## 1.2 연구 목적

예비 조사에서 확인된 핵심 발견은 질문 품질과 답변 품질 간 강한 상관관계(r=0.691)로, 이는 **질문 개선이 학습 효과 향상의 핵심 메커니즘**임을 의미한다. 따라서 본 연구는 고등학교 2학년 수학적 귀납법 단원을 중심으로 **질문 명료화 프로세스를 핵심으로 하는 AI agent 시스템 MAICE(Mathematical AI Chatbot for Education)를 설계·개발**한다. 

MAICE의 교육적 효과를 검증하기 위해, 본 연구는 무작위 배정 A/B 테스트를 설계하였다. 학생들을 두 그룹으로 나누어 한 그룹은 **MAICE의 질문 명료화 프로세스를 제공받는 방식(본 연구에서 "Agent 모드"로 명명)**을 사용하고, 다른 그룹은 **일반적인 LLM처럼 즉시 답변만 제공하는 방식(본 연구에서 "Freepass 모드"로 명명)**을 사용하도록 하여, 질문 명료화의 순수한 효과를 비교 분석한다. 

### 1.2.1 학술적 연구 목적

1. **질문 명료화 기반 멀티 에이전트 시스템 개발**: 예비 조사에서 확인된 질문의 질적 문제(학습맥락 부재 72.3%, 질문구조 불명확 45.8%, 수학적전문성 결여 45.5%)를 개선하는 질문 명료화 프로세스를 핵심으로 하는 멀티 에이전트 시스템 개발

2. **질문 품질 개선 효과 검증**: MAICE의 질문 명료화 프로세스가 학생의 질문 품질(학습맥락, 질문구조, 수학적전문성)을 일반적인 즉시 답변 방식 대비 실질적으로 개선함을 입증

3. **학습 경험 향상 증명**: 질문 명료화 프로세스를 통한 구조화된 학습 경험이 즉시 답변 방식보다 학습 만족도와 메타인지를 향상시킴을 입증

<!-- 4. **학습 효과 개선 검증**: 질문 품질 개선이 실제 학습 성과로 이어지는지 확인하기 위해, 수학적 귀납법 학습 효과(이해도, 문제해결 능력)에서 일반적인 즉시 답변 방식 대비 우수함을 검증 -->

4. **수학적 귀납법 단원 적용 검증**: MAICE 에이전트 시스템이 수학적 귀납법 단원(논리 구조 이해, 단계별 증명 필요)에서 효과적으로 작동하는지 검증

### 1.2.2 교육적 실천 목적

본 연구는 학술적 검증을 넘어, 교육 현장에서의 지속 가능한 활용과 발전을 다음과 같이 목표한다:

1. **학생의 반성적 사고 능력 탐구**: 학생들이 AI와의 대화를 통해 스스로의 사고 과정을 명료화하고, 메타인지 능력을 발달시키는 과정을 관찰하고 검증함으로써, Dewey의 반성적 사고 이론이 현대 AI 교육 환경에서도 유효함을 실증

2. **교사 주도 AI 교육 연구 플랫폼 제공**: MAICE 시스템을 교사가 자율적으로 커스터마이징할 수 있는 연구 도구로 설계하여, 교사가 각자의 교육 맥락에서 AI 활용에 필요한 핵심 요소(명료화 전략, 피드백 설계, 학습자 맞춤)를 직접 탐구하고 개선할 수 있는 기반 마련

3. **프롬프팅 설계 연구의 지속가능성**: 각 에이전트의 프롬프트를 공개하고 수정 가능하도록 설계하여, 교사가 자신의 수업 목표와 학생 특성에 맞춰 프롬프팅을 설계하고 그 효과를 체계적으로 연구할 수 있는 실험 환경 제공

본 시스템은 "완성된 솔루션"이 아닌 **"교사 주도 연구를 위한 확장 가능한 플랫폼"**으로 설계되었다. 이는 현재 에듀테크 산업에서 지배적인 "상업적으로 주어진 완성된 도구를 소비만 하면 좋은 수업이 나온다"는 접근에 대한 근본적 비판에서 출발한다. 

**AI 도구의 교육적 가치는 결국 그것을 설계하고 사용하는 교사의 전문성을 넘을 수 없다.** 마치 훌륭한 소설의 캐릭터가 작가의 능력을 넘지 못하듯, AI 교육 도구의 효과성은 그것을 설계한 교육적 철학과 교사의 맥락적 이해를 초과할 수 없다. 따라서 본 연구는 교육 현장의 교사들이 AI 교육 도구의 수동적 소비자가 아닌, **능동적 설계자이자 연구자**로서 자신의 교육 맥락에 최적화된 AI 시스템을 직접 개발하고 개선하는 역량을 갖추도록 지원하는 것을 궁극적 목표로 한다.

## 1.3 연구 문제

예비 조사를 통해 확인된 일반적인 즉시 답변 제공 방식의 핵심 문제는 학생들의 질문 품질 부족(학습맥락 72.3% 최저점)이며, 질문 품질과 답변 품질 간 강한 상관관계(r=0.691)는 질문 개선이 학습 효과 향상의 핵심 메커니즘임을 시사한다. 

앞서 1.2절에서 설명한 바와 같이, 본 연구는 MAICE 시스템의 두 가지 작동 방식—질문 명료화를 제공하는 **Agent 모드**와 일반적인 LLM처럼 즉시 답변만 제공하는 **Freepass 모드**—를 무작위 배정 A/B 테스트로 비교한다. MAICE 시스템의 효과성을 검증하기 위한 연구 문제를 다음과 같이 설정한다:

**RQ1. 질문 품질 개선 검증 (핵심 메커니즘)**: MAICE의 질문 명료화 프로세스(Agent 모드)가 학생의 질문 품질을 실질적으로 개선하는가? 특히 예비 조사에서 가장 심각한 문제로 확인된 학습 맥락 적용, 질문 구조화, 수학적 전문성 영역에서 일반적인 즉시 답변 방식(Freepass 모드) 대비 유의미한 개선이 나타나는가?

**RQ2. 학습 경험 향상 검증 (교육적 가치)**: 질문 명료화 프로세스를 통한 구조화된 학습 경험(Agent 모드)이 즉시 답변 방식(Freepass 모드)보다 학생의 학습 만족도와 메타인지를 향상시키는가?

<!-- **RQ3. 학습 효과 향상 검증 (실제 성취도)**: 질문 품질 개선과 학습 경험 향상이 실제 학습 성과로 이어지는가? MAICE의 Agent 모드가 Freepass 모드보다 수학적 귀납법 학습 효과(수행평가 점수, 문제해결 능력)에서 우수한가? [수행평가 미실시로 향후 연구 과제로 유보] -->

## 1.4 용어의 정의

- **예비 조사(Pilot Study)**: 본 연구의 MAICE 시스템 설계에 앞서, 2024년 5월 중순 실제 고등학교 수학 수업 환경에서 프리패스 방식 LLM의 교육적 문제점을 실증적으로 파악하기 위해 실시한 질문-답변 수집 및 교사 루브릭 평가 연구 (385건 질문, 1,012건 교사 평가)

- **MAICE (Mathematical AI Chatbot for Education)**: 예비 조사에서 확인된 일반적인 LLM 사용 방식의 한계를 극복하기 위해 개발된 질문 명료화 기반 AI agent 시스템. 본 연구에서는 MAICE를 두 가지 모드로 운영하여 비교 실험을 수행한다.

- **Agent 모드**: MAICE 시스템의 질문 명료화 프로세스를 제공하는 실험군 조건. QuestionImprover 에이전트가 학생의 초기 질문을 받아 명료화 질문을 통해 문제를 구체화한 후, 명료화된 질문에 대해 맞춤형 답변을 제공한다.

- **Freepass 모드**: MAICE 시스템의 대조군 조건으로, 일반적인 LLM(ChatGPT, Claude 등)처럼 질문 명료화 과정 없이 학생의 질문에 즉시 답변만 제공하는 방식. "Freepass"는 "명료화 과정을 거치지 않고(free) 바로 통과(pass)하여 답변"이라는 의미로 본 연구에서 명명하였다. 

- **질문 명료화**: 모호하거나 불완전한 질문을 구체적이고 명확한 질문으로 개선하는 과정으로, 학습자가 스스로 질문을 구조화하고 사고 과정을 명료화하도록 지원하는 교육적 개입.

- **AI 에이전트(AI Agent)**: 특정 목표를 달성하기 위해 환경을 인식하고, 자율적으로 의사결정을 내려 행동하는 소프트웨어 시스템. 본 연구에서는 LLM을 기반으로 특정 교육적 역할(질문 분류, 명료화 지원, 답변 생성 등)을 수행하도록 설계된 독립적 구성 요소를 의미함.

- **멀티 에이전트 시스템**: 질문 분류, 명료화 지원, 답변 생성, 학습 관찰 등 각기 다른 역할을 수행하는 여러 개의 독립적인 AI agent가 협업하여 학습 과정을 지원하는 시스템

- **학습 효과**: 수학적 이해도, 문제해결 능력, 질문 품질 개선, 메타인지 향상 등을 포함하는 학습 성과와 학습 경험의 종합적 측정

---

**이전**: 없음 | **다음**: [[chapters/02-theoretical-background]]
