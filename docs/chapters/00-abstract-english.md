# Abstract (English)

## Design and Development of an AI Agent Supporting Question Clarification in Mathematics Learning: Focusing on Mathematical Induction for High School Grade 2

By Hwang Si-hyun  
Major in AI Convergence Education  
Graduate School of Education, Pusan National University  
Supervised by Professor [Name]

---

## Abstract

Despite the widespread adoption of generative AI in education, poor question quality hinders effective learning. A pilot study (n=385) found that 72.3% of student questions lacked learning context, and current immediate-answer approaches fail to support students' thinking processes.

This study designed and developed MAICE, an AI agent system based on Dewey's reflective thinking theory that prioritizes question clarification. We validated its learning effectiveness through application to mathematical induction for high school grade 2 students. The teacher repeatedly emphasized key concepts—domino model, inductive connection process, and equation/inequality strategies—in every class, establishing a shared language that served as the foundation for AI-assisted learning.

**Methods**: Fifty-eight grade 2 students were randomly assigned to clarification-first (n=28) or immediate-answer modes (n=30) in a two-week A/B test (280 valid sessions). To mutually complement methodological limitations, we employed dual evaluation: LLM evaluation (N=280) for large-scale pattern detection and teacher preliminary evaluation (N=100) for educational validity verification. A dialogue quality checklist was developed and evaluated by three independent AI models (GPT-5-mini, Claude-4.5-Haiku, Gemini-2.5-Flash) and two external mathematics teachers. Inter-rater reliability: LLM α=0.840, teachers r=0.644, LLM-teacher r=0.771 (Claude-4.5-Haiku, p<0.001).

**Results**: Through LLM-teacher dual evaluation, clarification effects were mutually verified. (1) LLM evaluation (N=284, 3-model average): Clarification mode showed significant superiority in learning support (C2: +1.55 points, p=0.002, d=0.376) with notable effects for lower-achievers (Q1 overall: +2.46, p=0.033, d=0.511; Q1 C2: +3.36, p=0.001, d=0.840). (2) Teacher preliminary evaluation (N=100): Clarification mode was significantly higher (+2.25 points, p=0.031, d=0.307) with large effects for Q1 (+6.91 points, p=0.009, d=1.117). High LLM-teacher correlation (r=0.743, 3-model average) confirmed directional consistency. (3) Student self-assessment (N=40, Appendix H): 20-item survey analysis showed high scores in AI interaction quality (4.38/5.0), concept understanding (4.36/5.0), and questioning ability (4.13/5.0), with 68.6% preferring question-guided approach (reasons: "improves thinking" 42%, "deeper understanding" 25%). Objective evaluations (LLM·teacher) converged with subjective perceptions (student survey). Teacher evaluation is preliminary; replication with 10+ evaluators and 300+ samples is needed.

**Conclusions**: Question clarification processes enhance learning support, particularly contributing to closing educational gaps for lower-achieving students. Four independent evidence sources (objective evaluation, expert evaluation, learner evaluation, qualitative evidence) converged to strengthen validity. This study implemented Dewey's theory in AI educational tools and presented an LLM-teacher dual evaluation model combining large-scale objective assessment with expert validity verification. An extensible research platform enabling teacher-led prompting design was developed. However, participants from a software development-specialized high school with extensive AI experience may limit generalization to students with less AI familiarity.

---

**Keywords**: question clarification, AI agent, reflective thinking, mathematical induction, multi-agent system, Dewey, educational gap reduction, teacher-led research, prompting design

