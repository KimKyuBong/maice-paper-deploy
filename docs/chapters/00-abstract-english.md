# Abstract (English)

## Design and Development of an AI Agent Supporting Question Clarification in Mathematics Learning: Focusing on Mathematical Induction for High School Grade 2

By Hwang Si-hyun  
Major in AI Convergence Education  
Graduate School of Education, Pusan National University  
Supervised by Professor [Name]

---

## Abstract

Despite the widespread adoption of generative AI in education, poor question quality hinders effective learning. A pilot study (n=385) found that 72.3% of student questions lacked learning context, and current immediate-answer approaches fail to support students' thinking processes.

This study designed and developed MAICE, an AI agent system based on Dewey's reflective thinking theory that prioritizes question clarification. We validated its learning effectiveness through application to mathematical induction for high school grade 2 students. The teacher repeatedly emphasized key concepts—domino model, inductive connection process, and equation/inequality strategies—in every class, establishing a shared language that served as the foundation for AI-assisted learning.

**Methods**: Fifty-eight grade 2 students were randomly assigned to clarification-first (n=28) or immediate-answer modes (n=30) in a two-week A/B test (280 valid sessions). To mutually complement methodological limitations, we employed dual evaluation: LLM evaluation (N=280) for large-scale pattern detection and teacher preliminary evaluation (N=100) for educational validity verification. A dialogue quality checklist was developed and evaluated by three independent AI models (GPT-5-mini, Claude-4.5-Haiku, Gemini-2.5-Flash) and two external mathematics teachers. Inter-rater reliability: LLM α=0.840, teachers r=0.644, LLM-teacher r=0.771 (Claude-4.5-Haiku, p<0.001).

**Results**: Through LLM-teacher dual evaluation, clarification effects were mutually verified. LLM evaluation (N=280) showed clarification mode was superior in learning support (p=0.034, d=0.275), with particularly large effects for lower-achievers (+8.00 points, p=0.029, d=1.323). Teacher preliminary evaluation (N=100) observed consistent patterns (+2.25 points, p=0.031, d=0.307) with large effects for Q1 (+6.91 points, p=0.009, d=1.117). **Mutual verification succeeded** through high LLM-teacher correlation (r=0.771, Claude-4.5-Haiku) and nearly identical Q1 effects (+7.01 vs +6.91). Student survey (n=40) showed 60% preferred clarification. LLM evaluation provided sufficient sample for pattern detection, while teacher evaluation (preliminary with 2 evaluators) confirmed educational validity. Future research requires expanded teacher evaluation (10+ evaluators, 300+ samples).

**Conclusions**: Question clarification processes enhance learning support, particularly contributing to closing educational gaps for lower-achieving students. This study implemented Dewey's theory in AI educational tools and presented an LLM-teacher mutual verification model that combines large-scale objective evaluation with expert validity verification. An extensible research platform enabling teacher-led prompting design was developed.

---

**Keywords**: question clarification, AI agent, reflective thinking, mathematical induction, multi-agent system, Dewey, educational gap reduction, teacher-led research, prompting design

